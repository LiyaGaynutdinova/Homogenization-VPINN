{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from dataset import *\n",
    "from save_load import *\n",
    "from NN_library.PINN.PINN import *\n",
    "from NN_library.PINN.train_PINN import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset(2**12, [0, 2*np.pi], [0, 2*np.pi])\n",
    "loaders = get_loaders(data, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1d39f769710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = 2*np.pi\n",
    "N = 64 # number of nodes in each direction including the border\n",
    "H1 = torch.tensor([1, 0], device=dev).view(1, 2) # macrogradient\n",
    "H2 = torch.tensor([0, 1], device=dev).view(1, 2) # macrogradient\n",
    "x = np.linspace(0, L, N, endpoint=True)\n",
    "y = np.linspace(0, L, N, endpoint=True)\n",
    "\n",
    "XY = np.meshgrid(x, y)\n",
    "grid_data = torch.tensor(np.vstack((XY[0].flatten(), XY[1].flatten())).T, dtype=torch.float, device=dev)\n",
    "def a_function(x,y):\n",
    "    a = 2+np.sin(x)*np.sin(2*y)\n",
    "    return a\n",
    "def A(x):\n",
    "    a = (2+torch.sin(x[:,0])*torch.sin(2*x[:,1])).view(-1,1,1)\n",
    "    I = torch.eye(2, device=dev).repeat(x.shape[0], 1, 1)\n",
    "    A = a * I\n",
    "    return A\n",
    "Z = a_function(XY[0].flatten(),XY[1].flatten())\n",
    "plt.pcolormesh(XY[0], XY[1], Z.reshape(N, N))\n",
    "plt.colorbar()\n",
    "plt.scatter(data[:,0], data[:,1], s = 1, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 5411\n"
     ]
    }
   ],
   "source": [
    "args = {'lr' : 0.00001, 'epochs' : 2000, 'dev' : dev, 'name' : 'NN_library/PINN/PINN_H1'}\n",
    "net_H1 = PINN(n_periodic=10, n_hidden=30, n_layers=5, period_len=L)\n",
    "total_params = sum(p.numel() for p in net_H1.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_H1 = load_network(net_H1, args['name']+'', args)\n",
    "net_H1 = net_H1.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train, losses_val = train(net_H1, loaders, args, A, H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train)\n",
    "plt.plot(losses_val)\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 5411\n"
     ]
    }
   ],
   "source": [
    "args = {'lr' : 0.00001, 'epochs' : 10000, 'dev' : dev, 'name' : 'NN_library/PINN/PINN_H2'}\n",
    "net_H2 = PINN(n_periodic=10, n_hidden=30, n_layers=5, period_len=L)\n",
    "total_params = sum(p.numel() for p in net_H2.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_H2 = load_network(net_H2, args['name']+'', args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 mean train loss:  1.58386431e+00, mean val. rec. loss:  1.47427696e+00\n",
      "Epoch: 1 mean train loss:  1.55549607e+00, mean val. rec. loss:  1.44759998e+00\n",
      "Epoch: 2 mean train loss:  1.52882973e+00, mean val. rec. loss:  1.42224330e+00\n",
      "Epoch: 3 mean train loss:  1.50464187e+00, mean val. rec. loss:  1.39770210e+00\n",
      "Epoch: 4 mean train loss:  1.48053888e+00, mean val. rec. loss:  1.37479427e+00\n",
      "Epoch: 5 mean train loss:  1.45829201e+00, mean val. rec. loss:  1.35325794e+00\n",
      "Epoch: 6 mean train loss:  1.43714695e+00, mean val. rec. loss:  1.33304190e+00\n",
      "Epoch: 7 mean train loss:  1.41708715e+00, mean val. rec. loss:  1.31407754e+00\n",
      "Epoch: 8 mean train loss:  1.39859094e+00, mean val. rec. loss:  1.29600190e+00\n",
      "Epoch: 9 mean train loss:  1.38102044e+00, mean val. rec. loss:  1.27881991e+00\n",
      "Epoch: 10 mean train loss:  1.36428960e+00, mean val. rec. loss:  1.26254853e+00\n",
      "Epoch: 11 mean train loss:  1.34824113e+00, mean val. rec. loss:  1.24729659e+00\n",
      "Epoch: 12 mean train loss:  1.33302303e+00, mean val. rec. loss:  1.23281176e+00\n",
      "Epoch: 13 mean train loss:  1.31884516e+00, mean val. rec. loss:  1.21901975e+00\n",
      "Epoch: 14 mean train loss:  1.30542486e+00, mean val. rec. loss:  1.20573879e+00\n",
      "Epoch: 15 mean train loss:  1.29203760e+00, mean val. rec. loss:  1.19334270e+00\n",
      "Epoch: 16 mean train loss:  1.27995068e+00, mean val. rec. loss:  1.18129027e+00\n",
      "Epoch: 17 mean train loss:  1.26799229e+00, mean val. rec. loss:  1.16987111e+00\n",
      "Epoch: 18 mean train loss:  1.25679081e+00, mean val. rec. loss:  1.15890816e+00\n",
      "Epoch: 19 mean train loss:  1.24588124e+00, mean val. rec. loss:  1.14847784e+00\n",
      "Epoch: 20 mean train loss:  1.23533828e+00, mean val. rec. loss:  1.13863801e+00\n",
      "Epoch: 21 mean train loss:  1.22548427e+00, mean val. rec. loss:  1.12907343e+00\n",
      "Epoch: 22 mean train loss:  1.21587636e+00, mean val. rec. loss:  1.11994227e+00\n",
      "Epoch: 23 mean train loss:  1.20677488e+00, mean val. rec. loss:  1.11113311e+00\n",
      "Epoch: 24 mean train loss:  1.19800626e+00, mean val. rec. loss:  1.10260985e+00\n",
      "Epoch: 25 mean train loss:  1.18951488e+00, mean val. rec. loss:  1.09443851e+00\n",
      "Epoch: 26 mean train loss:  1.18140460e+00, mean val. rec. loss:  1.08653140e+00\n",
      "Epoch: 27 mean train loss:  1.17331272e+00, mean val. rec. loss:  1.07905668e+00\n",
      "Epoch: 28 mean train loss:  1.16584070e+00, mean val. rec. loss:  1.07176380e+00\n",
      "Epoch: 29 mean train loss:  1.15852060e+00, mean val. rec. loss:  1.06467404e+00\n",
      "Epoch: 30 mean train loss:  1.15141333e+00, mean val. rec. loss:  1.05784257e+00\n",
      "Epoch: 31 mean train loss:  1.14447664e+00, mean val. rec. loss:  1.05132163e+00\n",
      "Epoch: 32 mean train loss:  1.13801522e+00, mean val. rec. loss:  1.04490684e+00\n",
      "Epoch: 33 mean train loss:  1.13143194e+00, mean val. rec. loss:  1.03877482e+00\n",
      "Epoch: 34 mean train loss:  1.12536177e+00, mean val. rec. loss:  1.03275146e+00\n",
      "Epoch: 35 mean train loss:  1.11919322e+00, mean val. rec. loss:  1.02701490e+00\n",
      "Epoch: 36 mean train loss:  1.11335567e+00, mean val. rec. loss:  1.02139297e+00\n",
      "Epoch: 37 mean train loss:  1.10778523e+00, mean val. rec. loss:  1.01592437e+00\n",
      "Epoch: 38 mean train loss:  1.10216718e+00, mean val. rec. loss:  1.01064855e+00\n",
      "Epoch: 39 mean train loss:  1.09677259e+00, mean val. rec. loss:  1.00553857e+00\n",
      "Epoch: 40 mean train loss:  1.09171491e+00, mean val. rec. loss:  1.00040588e+00\n",
      "Epoch: 41 mean train loss:  1.08651217e+00, mean val. rec. loss:  9.95513023e-01\n",
      "Epoch: 42 mean train loss:  1.08153866e+00, mean val. rec. loss:  9.90801109e-01\n",
      "Epoch: 43 mean train loss:  1.07681209e+00, mean val. rec. loss:  9.86104975e-01\n",
      "Epoch: 44 mean train loss:  1.07208715e+00, mean val. rec. loss:  9.81575570e-01\n",
      "Epoch: 45 mean train loss:  1.06747538e+00, mean val. rec. loss:  9.77143823e-01\n",
      "Epoch: 46 mean train loss:  1.06300676e+00, mean val. rec. loss:  9.72865779e-01\n",
      "Epoch: 47 mean train loss:  1.05860254e+00, mean val. rec. loss:  9.68672813e-01\n",
      "Epoch: 48 mean train loss:  1.05439727e+00, mean val. rec. loss:  9.64549888e-01\n",
      "Epoch: 49 mean train loss:  1.05022771e+00, mean val. rec. loss:  9.60530797e-01\n",
      "Epoch: 50 mean train loss:  1.04614877e+00, mean val. rec. loss:  9.56538056e-01\n",
      "Epoch: 51 mean train loss:  1.04214742e+00, mean val. rec. loss:  9.52690013e-01\n",
      "Epoch: 52 mean train loss:  1.03816643e+00, mean val. rec. loss:  9.48967017e-01\n",
      "Epoch: 53 mean train loss:  1.03437188e+00, mean val. rec. loss:  9.45305354e-01\n",
      "Epoch: 54 mean train loss:  1.03066818e+00, mean val. rec. loss:  9.41641012e-01\n",
      "Epoch: 55 mean train loss:  1.02697498e+00, mean val. rec. loss:  9.38088022e-01\n",
      "Epoch: 56 mean train loss:  1.02334866e+00, mean val. rec. loss:  9.34632241e-01\n",
      "Epoch: 57 mean train loss:  1.01991608e+00, mean val. rec. loss:  9.31170803e-01\n",
      "Epoch: 58 mean train loss:  1.01635530e+00, mean val. rec. loss:  9.27853915e-01\n",
      "Epoch: 59 mean train loss:  1.01302258e+00, mean val. rec. loss:  9.24535165e-01\n",
      "Epoch: 60 mean train loss:  1.00965532e+00, mean val. rec. loss:  9.21321813e-01\n",
      "Epoch: 61 mean train loss:  1.00633257e+00, mean val. rec. loss:  9.18203959e-01\n",
      "Epoch: 62 mean train loss:  1.00318009e+00, mean val. rec. loss:  9.15069878e-01\n",
      "Epoch: 63 mean train loss:  9.99953623e-01, mean val. rec. loss:  9.12002787e-01\n",
      "Epoch: 64 mean train loss:  9.96912722e-01, mean val. rec. loss:  9.08991372e-01\n",
      "Epoch: 65 mean train loss:  9.93775420e-01, mean val. rec. loss:  9.06029827e-01\n",
      "Epoch: 66 mean train loss:  9.90716466e-01, mean val. rec. loss:  9.03164822e-01\n",
      "Epoch: 67 mean train loss:  9.87817636e-01, mean val. rec. loss:  9.00275030e-01\n",
      "Epoch: 68 mean train loss:  9.84869783e-01, mean val. rec. loss:  8.97462724e-01\n",
      "Epoch: 69 mean train loss:  9.82007363e-01, mean val. rec. loss:  8.94697236e-01\n",
      "Epoch: 70 mean train loss:  9.79164832e-01, mean val. rec. loss:  8.91910311e-01\n",
      "Epoch: 71 mean train loss:  9.76381938e-01, mean val. rec. loss:  8.89174447e-01\n",
      "Epoch: 72 mean train loss:  9.73579863e-01, mean val. rec. loss:  8.86518599e-01\n",
      "Epoch: 73 mean train loss:  9.70868252e-01, mean val. rec. loss:  8.83925796e-01\n",
      "Epoch: 74 mean train loss:  9.68198532e-01, mean val. rec. loss:  8.81296893e-01\n",
      "Epoch: 75 mean train loss:  9.65495579e-01, mean val. rec. loss:  8.78757979e-01\n",
      "Epoch: 76 mean train loss:  9.62873252e-01, mean val. rec. loss:  8.76250774e-01\n",
      "Epoch: 77 mean train loss:  9.60309151e-01, mean val. rec. loss:  8.73739401e-01\n",
      "Epoch: 78 mean train loss:  9.57744761e-01, mean val. rec. loss:  8.71256312e-01\n",
      "Epoch: 79 mean train loss:  9.55216721e-01, mean val. rec. loss:  8.68815055e-01\n",
      "Epoch: 80 mean train loss:  9.52724098e-01, mean val. rec. loss:  8.66410865e-01\n",
      "Epoch: 81 mean train loss:  9.50210981e-01, mean val. rec. loss:  8.64077387e-01\n",
      "Epoch: 82 mean train loss:  9.47770870e-01, mean val. rec. loss:  8.61715177e-01\n",
      "Epoch: 83 mean train loss:  9.45340977e-01, mean val. rec. loss:  8.59402391e-01\n",
      "Epoch: 84 mean train loss:  9.42954155e-01, mean val. rec. loss:  8.57110596e-01\n",
      "Epoch: 85 mean train loss:  9.40574544e-01, mean val. rec. loss:  8.54820438e-01\n",
      "Epoch: 86 mean train loss:  9.38219063e-01, mean val. rec. loss:  8.52568985e-01\n",
      "Epoch: 87 mean train loss:  9.35854226e-01, mean val. rec. loss:  8.50345519e-01\n",
      "Epoch: 88 mean train loss:  9.33571626e-01, mean val. rec. loss:  8.48148402e-01\n",
      "Epoch: 89 mean train loss:  9.31222462e-01, mean val. rec. loss:  8.45960664e-01\n",
      "Epoch: 90 mean train loss:  9.28997352e-01, mean val. rec. loss:  8.43767417e-01\n",
      "Epoch: 91 mean train loss:  9.26716987e-01, mean val. rec. loss:  8.41650539e-01\n",
      "Epoch: 92 mean train loss:  9.24496107e-01, mean val. rec. loss:  8.39485876e-01\n",
      "Epoch: 93 mean train loss:  9.22267723e-01, mean val. rec. loss:  8.37384554e-01\n",
      "Epoch: 94 mean train loss:  9.20038044e-01, mean val. rec. loss:  8.35244602e-01\n",
      "Epoch: 95 mean train loss:  9.17822399e-01, mean val. rec. loss:  8.33179604e-01\n",
      "Epoch: 96 mean train loss:  9.15657905e-01, mean val. rec. loss:  8.31099720e-01\n",
      "Epoch: 97 mean train loss:  9.13501008e-01, mean val. rec. loss:  8.29024673e-01\n",
      "Epoch: 98 mean train loss:  9.11347744e-01, mean val. rec. loss:  8.26975306e-01\n",
      "Epoch: 99 mean train loss:  9.09187373e-01, mean val. rec. loss:  8.24980647e-01\n",
      "Epoch: 100 mean train loss:  9.07030392e-01, mean val. rec. loss:  8.22976164e-01\n",
      "Epoch: 101 mean train loss:  9.04957586e-01, mean val. rec. loss:  8.20979644e-01\n",
      "Epoch: 102 mean train loss:  9.02842353e-01, mean val. rec. loss:  8.19021234e-01\n",
      "Epoch: 103 mean train loss:  9.00695549e-01, mean val. rec. loss:  8.16994346e-01\n",
      "Epoch: 104 mean train loss:  8.98630778e-01, mean val. rec. loss:  8.15063030e-01\n",
      "Epoch: 105 mean train loss:  8.96548433e-01, mean val. rec. loss:  8.13082067e-01\n",
      "Epoch: 106 mean train loss:  8.94446493e-01, mean val. rec. loss:  8.11128793e-01\n",
      "Epoch: 107 mean train loss:  8.92407395e-01, mean val. rec. loss:  8.09206558e-01\n",
      "Epoch: 108 mean train loss:  8.90341301e-01, mean val. rec. loss:  8.07269808e-01\n",
      "Epoch: 109 mean train loss:  8.88265016e-01, mean val. rec. loss:  8.05321521e-01\n",
      "Epoch: 110 mean train loss:  8.86196383e-01, mean val. rec. loss:  8.03402189e-01\n",
      "Epoch: 111 mean train loss:  8.84158301e-01, mean val. rec. loss:  8.01483154e-01\n",
      "Epoch: 112 mean train loss:  8.82140582e-01, mean val. rec. loss:  7.99601783e-01\n",
      "Epoch: 113 mean train loss:  8.80111559e-01, mean val. rec. loss:  7.97718178e-01\n",
      "Epoch: 114 mean train loss:  8.78059914e-01, mean val. rec. loss:  7.95828173e-01\n",
      "Epoch: 115 mean train loss:  8.76071795e-01, mean val. rec. loss:  7.93936083e-01\n",
      "Epoch: 116 mean train loss:  8.74013749e-01, mean val. rec. loss:  7.92066769e-01\n",
      "Epoch: 117 mean train loss:  8.72000600e-01, mean val. rec. loss:  7.90155178e-01\n",
      "Epoch: 118 mean train loss:  8.70007800e-01, mean val. rec. loss:  7.88275295e-01\n",
      "Epoch: 119 mean train loss:  8.67968693e-01, mean val. rec. loss:  7.86394222e-01\n",
      "Epoch: 120 mean train loss:  8.65937071e-01, mean val. rec. loss:  7.84544410e-01\n",
      "Epoch: 121 mean train loss:  8.63950513e-01, mean val. rec. loss:  7.82670928e-01\n",
      "Epoch: 122 mean train loss:  8.61922119e-01, mean val. rec. loss:  7.80814566e-01\n",
      "Epoch: 123 mean train loss:  8.59897600e-01, mean val. rec. loss:  7.79000333e-01\n",
      "Epoch: 124 mean train loss:  8.57903621e-01, mean val. rec. loss:  7.77134742e-01\n",
      "Epoch: 125 mean train loss:  8.55906495e-01, mean val. rec. loss:  7.75266917e-01\n",
      "Epoch: 126 mean train loss:  8.53875678e-01, mean val. rec. loss:  7.73418147e-01\n",
      "Epoch: 127 mean train loss:  8.51877532e-01, mean val. rec. loss:  7.71566326e-01\n",
      "Epoch: 128 mean train loss:  8.49860586e-01, mean val. rec. loss:  7.69677213e-01\n",
      "Epoch: 129 mean train loss:  8.47856808e-01, mean val. rec. loss:  7.67768525e-01\n",
      "Epoch: 130 mean train loss:  8.45818754e-01, mean val. rec. loss:  7.65938438e-01\n",
      "Epoch: 131 mean train loss:  8.43829862e-01, mean val. rec. loss:  7.64052154e-01\n",
      "Epoch: 132 mean train loss:  8.41797322e-01, mean val. rec. loss:  7.62229808e-01\n",
      "Epoch: 133 mean train loss:  8.39793418e-01, mean val. rec. loss:  7.60389002e-01\n",
      "Epoch: 134 mean train loss:  8.37750306e-01, mean val. rec. loss:  7.58487385e-01\n",
      "Epoch: 135 mean train loss:  8.35729280e-01, mean val. rec. loss:  7.56594328e-01\n",
      "Epoch: 136 mean train loss:  8.33689321e-01, mean val. rec. loss:  7.54769674e-01\n",
      "Epoch: 137 mean train loss:  8.31660987e-01, mean val. rec. loss:  7.52907358e-01\n",
      "Epoch: 138 mean train loss:  8.29616006e-01, mean val. rec. loss:  7.51086724e-01\n",
      "Epoch: 139 mean train loss:  8.27602829e-01, mean val. rec. loss:  7.49157566e-01\n",
      "Epoch: 140 mean train loss:  8.25525771e-01, mean val. rec. loss:  7.47309541e-01\n",
      "Epoch: 141 mean train loss:  8.23511443e-01, mean val. rec. loss:  7.45401075e-01\n",
      "Epoch: 142 mean train loss:  8.21448059e-01, mean val. rec. loss:  7.43533921e-01\n",
      "Epoch: 143 mean train loss:  8.19386451e-01, mean val. rec. loss:  7.41597766e-01\n",
      "Epoch: 144 mean train loss:  8.17325187e-01, mean val. rec. loss:  7.39715502e-01\n",
      "Epoch: 145 mean train loss:  8.15234463e-01, mean val. rec. loss:  7.37795351e-01\n",
      "Epoch: 146 mean train loss:  8.13182379e-01, mean val. rec. loss:  7.35882047e-01\n",
      "Epoch: 147 mean train loss:  8.11087146e-01, mean val. rec. loss:  7.33984896e-01\n",
      "Epoch: 148 mean train loss:  8.08996412e-01, mean val. rec. loss:  7.32073081e-01\n",
      "Epoch: 149 mean train loss:  8.06884677e-01, mean val. rec. loss:  7.30197218e-01\n",
      "Epoch: 150 mean train loss:  8.04823041e-01, mean val. rec. loss:  7.28257342e-01\n",
      "Epoch: 151 mean train loss:  8.02720737e-01, mean val. rec. loss:  7.26343369e-01\n",
      "Epoch: 152 mean train loss:  8.00596528e-01, mean val. rec. loss:  7.24384289e-01\n",
      "Epoch: 153 mean train loss:  7.98502790e-01, mean val. rec. loss:  7.22473070e-01\n",
      "Epoch: 154 mean train loss:  7.96372708e-01, mean val. rec. loss:  7.20509450e-01\n",
      "Epoch: 155 mean train loss:  7.94221117e-01, mean val. rec. loss:  7.18563173e-01\n",
      "Epoch: 156 mean train loss:  7.92094034e-01, mean val. rec. loss:  7.16577297e-01\n",
      "Epoch: 157 mean train loss:  7.89925423e-01, mean val. rec. loss:  7.14625066e-01\n",
      "Epoch: 158 mean train loss:  7.87812565e-01, mean val. rec. loss:  7.12666284e-01\n",
      "Epoch: 159 mean train loss:  7.85639762e-01, mean val. rec. loss:  7.10695444e-01\n",
      "Epoch: 160 mean train loss:  7.83463326e-01, mean val. rec. loss:  7.08723710e-01\n",
      "Epoch: 161 mean train loss:  7.81314172e-01, mean val. rec. loss:  7.06760462e-01\n",
      "Epoch: 162 mean train loss:  7.79098744e-01, mean val. rec. loss:  7.04733649e-01\n",
      "Epoch: 163 mean train loss:  7.76945456e-01, mean val. rec. loss:  7.02687482e-01\n",
      "Epoch: 164 mean train loss:  7.74750082e-01, mean val. rec. loss:  7.00696917e-01\n",
      "Epoch: 165 mean train loss:  7.72570744e-01, mean val. rec. loss:  6.98644126e-01\n",
      "Epoch: 166 mean train loss:  7.70290952e-01, mean val. rec. loss:  6.96641057e-01\n",
      "Epoch: 167 mean train loss:  7.68081125e-01, mean val. rec. loss:  6.94607767e-01\n",
      "Epoch: 168 mean train loss:  7.65902262e-01, mean val. rec. loss:  6.92551627e-01\n",
      "Epoch: 169 mean train loss:  7.63651232e-01, mean val. rec. loss:  6.90515137e-01\n",
      "Epoch: 170 mean train loss:  7.61383951e-01, mean val. rec. loss:  6.88470087e-01\n",
      "Epoch: 171 mean train loss:  7.59160658e-01, mean val. rec. loss:  6.86392286e-01\n",
      "Epoch: 172 mean train loss:  7.56863893e-01, mean val. rec. loss:  6.84310913e-01\n",
      "Epoch: 173 mean train loss:  7.54641443e-01, mean val. rec. loss:  6.82227009e-01\n",
      "Epoch: 174 mean train loss:  7.52336546e-01, mean val. rec. loss:  6.80162309e-01\n",
      "Epoch: 175 mean train loss:  7.50053480e-01, mean val. rec. loss:  6.78034340e-01\n",
      "Epoch: 176 mean train loss:  7.47779949e-01, mean val. rec. loss:  6.75954902e-01\n",
      "Epoch: 177 mean train loss:  7.45449332e-01, mean val. rec. loss:  6.73854400e-01\n",
      "Epoch: 178 mean train loss:  7.43167477e-01, mean val. rec. loss:  6.71732535e-01\n",
      "Epoch: 179 mean train loss:  7.40838168e-01, mean val. rec. loss:  6.69588340e-01\n",
      "Epoch: 180 mean train loss:  7.38538954e-01, mean val. rec. loss:  6.67458139e-01\n",
      "Epoch: 181 mean train loss:  7.36192640e-01, mean val. rec. loss:  6.65277100e-01\n",
      "Epoch: 182 mean train loss:  7.33839284e-01, mean val. rec. loss:  6.63179872e-01\n",
      "Epoch: 183 mean train loss:  7.31498690e-01, mean val. rec. loss:  6.61011040e-01\n",
      "Epoch: 184 mean train loss:  7.29125963e-01, mean val. rec. loss:  6.58849800e-01\n",
      "Epoch: 185 mean train loss:  7.26756057e-01, mean val. rec. loss:  6.56687071e-01\n",
      "Epoch: 186 mean train loss:  7.24402333e-01, mean val. rec. loss:  6.54510945e-01\n",
      "Epoch: 187 mean train loss:  7.21998436e-01, mean val. rec. loss:  6.52346727e-01\n",
      "Epoch: 188 mean train loss:  7.19660837e-01, mean val. rec. loss:  6.50163381e-01\n",
      "Epoch: 189 mean train loss:  7.17228970e-01, mean val. rec. loss:  6.47933290e-01\n",
      "Epoch: 190 mean train loss:  7.14815701e-01, mean val. rec. loss:  6.45753591e-01\n",
      "Epoch: 191 mean train loss:  7.12427188e-01, mean val. rec. loss:  6.43568234e-01\n",
      "Epoch: 192 mean train loss:  7.09987466e-01, mean val. rec. loss:  6.41331891e-01\n",
      "Epoch: 193 mean train loss:  7.07589314e-01, mean val. rec. loss:  6.39111700e-01\n",
      "Epoch: 194 mean train loss:  7.05155257e-01, mean val. rec. loss:  6.36849306e-01\n",
      "Epoch: 195 mean train loss:  7.02678587e-01, mean val. rec. loss:  6.34605370e-01\n",
      "Epoch: 196 mean train loss:  7.00231485e-01, mean val. rec. loss:  6.32401183e-01\n",
      "Epoch: 197 mean train loss:  6.97801210e-01, mean val. rec. loss:  6.30160150e-01\n",
      "Epoch: 198 mean train loss:  6.95326622e-01, mean val. rec. loss:  6.27870885e-01\n",
      "Epoch: 199 mean train loss:  6.92872668e-01, mean val. rec. loss:  6.25627099e-01\n",
      "Epoch: 200 mean train loss:  6.90391708e-01, mean val. rec. loss:  6.23380559e-01\n",
      "Epoch: 201 mean train loss:  6.87878074e-01, mean val. rec. loss:  6.21094271e-01\n",
      "Epoch: 202 mean train loss:  6.85458215e-01, mean val. rec. loss:  6.18778694e-01\n",
      "Epoch: 203 mean train loss:  6.82937543e-01, mean val. rec. loss:  6.16495049e-01\n",
      "Epoch: 204 mean train loss:  6.80412442e-01, mean val. rec. loss:  6.14241065e-01\n",
      "Epoch: 205 mean train loss:  6.77934723e-01, mean val. rec. loss:  6.11953512e-01\n",
      "Epoch: 206 mean train loss:  6.75417119e-01, mean val. rec. loss:  6.09650403e-01\n",
      "Epoch: 207 mean train loss:  6.72882173e-01, mean val. rec. loss:  6.07359835e-01\n",
      "Epoch: 208 mean train loss:  6.70365498e-01, mean val. rec. loss:  6.05079688e-01\n",
      "Epoch: 209 mean train loss:  6.67848981e-01, mean val. rec. loss:  6.02757152e-01\n",
      "Epoch: 210 mean train loss:  6.65323632e-01, mean val. rec. loss:  6.00421515e-01\n",
      "Epoch: 211 mean train loss:  6.62818098e-01, mean val. rec. loss:  5.98098383e-01\n",
      "Epoch: 212 mean train loss:  6.60239959e-01, mean val. rec. loss:  5.95754037e-01\n",
      "Epoch: 213 mean train loss:  6.57672845e-01, mean val. rec. loss:  5.93467452e-01\n",
      "Epoch: 214 mean train loss:  6.55165818e-01, mean val. rec. loss:  5.91175284e-01\n",
      "Epoch: 215 mean train loss:  6.52595108e-01, mean val. rec. loss:  5.88865550e-01\n",
      "Epoch: 216 mean train loss:  6.50031666e-01, mean val. rec. loss:  5.86517148e-01\n",
      "Epoch: 217 mean train loss:  6.47505303e-01, mean val. rec. loss:  5.84140312e-01\n",
      "Epoch: 218 mean train loss:  6.44949302e-01, mean val. rec. loss:  5.81797083e-01\n",
      "Epoch: 219 mean train loss:  6.42388121e-01, mean val. rec. loss:  5.79484186e-01\n",
      "Epoch: 220 mean train loss:  6.39765002e-01, mean val. rec. loss:  5.77145646e-01\n",
      "Epoch: 221 mean train loss:  6.37248178e-01, mean val. rec. loss:  5.74749235e-01\n",
      "Epoch: 222 mean train loss:  6.34633000e-01, mean val. rec. loss:  5.72397930e-01\n",
      "Epoch: 223 mean train loss:  6.32092221e-01, mean val. rec. loss:  5.70072453e-01\n",
      "Epoch: 224 mean train loss:  6.29500942e-01, mean val. rec. loss:  5.67725056e-01\n",
      "Epoch: 225 mean train loss:  6.26921580e-01, mean val. rec. loss:  5.65370364e-01\n",
      "Epoch: 226 mean train loss:  6.24349368e-01, mean val. rec. loss:  5.63041352e-01\n",
      "Epoch: 227 mean train loss:  6.21775873e-01, mean val. rec. loss:  5.60685916e-01\n",
      "Epoch: 228 mean train loss:  6.19183066e-01, mean val. rec. loss:  5.58339933e-01\n",
      "Epoch: 229 mean train loss:  6.16613963e-01, mean val. rec. loss:  5.55994471e-01\n",
      "Epoch: 230 mean train loss:  6.14009564e-01, mean val. rec. loss:  5.53661067e-01\n",
      "Epoch: 231 mean train loss:  6.11442189e-01, mean val. rec. loss:  5.51277570e-01\n",
      "Epoch: 232 mean train loss:  6.08867480e-01, mean val. rec. loss:  5.48883913e-01\n",
      "Epoch: 233 mean train loss:  6.06298496e-01, mean val. rec. loss:  5.46537148e-01\n",
      "Epoch: 234 mean train loss:  6.03720524e-01, mean val. rec. loss:  5.44213495e-01\n",
      "Epoch: 235 mean train loss:  6.01130810e-01, mean val. rec. loss:  5.41855231e-01\n",
      "Epoch: 236 mean train loss:  5.98551842e-01, mean val. rec. loss:  5.39530833e-01\n",
      "Epoch: 237 mean train loss:  5.95971789e-01, mean val. rec. loss:  5.37184701e-01\n",
      "Epoch: 238 mean train loss:  5.93399087e-01, mean val. rec. loss:  5.34824725e-01\n",
      "Epoch: 239 mean train loss:  5.90806973e-01, mean val. rec. loss:  5.32518080e-01\n",
      "Epoch: 240 mean train loss:  5.88295542e-01, mean val. rec. loss:  5.30235253e-01\n",
      "Epoch: 241 mean train loss:  5.85692013e-01, mean val. rec. loss:  5.27902966e-01\n",
      "Epoch: 242 mean train loss:  5.83141709e-01, mean val. rec. loss:  5.25536774e-01\n",
      "Epoch: 243 mean train loss:  5.80590743e-01, mean val. rec. loss:  5.23182976e-01\n",
      "Epoch: 244 mean train loss:  5.78009381e-01, mean val. rec. loss:  5.20897172e-01\n",
      "Epoch: 245 mean train loss:  5.75473767e-01, mean val. rec. loss:  5.18567341e-01\n",
      "Epoch: 246 mean train loss:  5.72899433e-01, mean val. rec. loss:  5.16248452e-01\n",
      "Epoch: 247 mean train loss:  5.70313618e-01, mean val. rec. loss:  5.13962797e-01\n",
      "Epoch: 248 mean train loss:  5.67815410e-01, mean val. rec. loss:  5.11655705e-01\n",
      "Epoch: 249 mean train loss:  5.65285805e-01, mean val. rec. loss:  5.09357210e-01\n",
      "Epoch: 250 mean train loss:  5.62744336e-01, mean val. rec. loss:  5.07034972e-01\n",
      "Epoch: 251 mean train loss:  5.60194914e-01, mean val. rec. loss:  5.04767143e-01\n",
      "Epoch: 252 mean train loss:  5.57652067e-01, mean val. rec. loss:  5.02505642e-01\n",
      "Epoch: 253 mean train loss:  5.55168441e-01, mean val. rec. loss:  5.00206254e-01\n",
      "Epoch: 254 mean train loss:  5.52623297e-01, mean val. rec. loss:  4.97920860e-01\n",
      "Epoch: 255 mean train loss:  5.50124833e-01, mean val. rec. loss:  4.95646742e-01\n",
      "Epoch: 256 mean train loss:  5.47630606e-01, mean val. rec. loss:  4.93362762e-01\n",
      "Epoch: 257 mean train loss:  5.45110083e-01, mean val. rec. loss:  4.91113170e-01\n",
      "Epoch: 258 mean train loss:  5.42634561e-01, mean val. rec. loss:  4.88873254e-01\n",
      "Epoch: 259 mean train loss:  5.40126214e-01, mean val. rec. loss:  4.86613725e-01\n",
      "Epoch: 260 mean train loss:  5.37667793e-01, mean val. rec. loss:  4.84377010e-01\n",
      "Epoch: 261 mean train loss:  5.35174662e-01, mean val. rec. loss:  4.82145579e-01\n",
      "Epoch: 262 mean train loss:  5.32693225e-01, mean val. rec. loss:  4.79919099e-01\n",
      "Epoch: 263 mean train loss:  5.30250992e-01, mean val. rec. loss:  4.77707840e-01\n",
      "Epoch: 264 mean train loss:  5.27780871e-01, mean val. rec. loss:  4.75465133e-01\n",
      "Epoch: 265 mean train loss:  5.25332180e-01, mean val. rec. loss:  4.73265113e-01\n",
      "Epoch: 266 mean train loss:  5.22903547e-01, mean val. rec. loss:  4.71052067e-01\n",
      "Epoch: 267 mean train loss:  5.20458287e-01, mean val. rec. loss:  4.68847917e-01\n",
      "Epoch: 268 mean train loss:  5.18033821e-01, mean val. rec. loss:  4.66664570e-01\n",
      "Epoch: 269 mean train loss:  5.15629360e-01, mean val. rec. loss:  4.64470766e-01\n",
      "Epoch: 270 mean train loss:  5.13185017e-01, mean val. rec. loss:  4.62328581e-01\n",
      "Epoch: 271 mean train loss:  5.10831512e-01, mean val. rec. loss:  4.60158409e-01\n",
      "Epoch: 272 mean train loss:  5.08379264e-01, mean val. rec. loss:  4.57992368e-01\n",
      "Epoch: 273 mean train loss:  5.06023761e-01, mean val. rec. loss:  4.55829639e-01\n",
      "Epoch: 274 mean train loss:  5.03635271e-01, mean val. rec. loss:  4.53685816e-01\n",
      "Epoch: 275 mean train loss:  5.01268839e-01, mean val. rec. loss:  4.51549511e-01\n",
      "Epoch: 276 mean train loss:  4.98917634e-01, mean val. rec. loss:  4.49428577e-01\n",
      "Epoch: 277 mean train loss:  4.96542573e-01, mean val. rec. loss:  4.47313746e-01\n",
      "Epoch: 278 mean train loss:  4.94206746e-01, mean val. rec. loss:  4.45188420e-01\n",
      "Epoch: 279 mean train loss:  4.91884842e-01, mean val. rec. loss:  4.43068937e-01\n",
      "Epoch: 280 mean train loss:  4.89550205e-01, mean val. rec. loss:  4.40953027e-01\n",
      "Epoch: 281 mean train loss:  4.87222271e-01, mean val. rec. loss:  4.38883563e-01\n",
      "Epoch: 282 mean train loss:  4.84925849e-01, mean val. rec. loss:  4.36807139e-01\n",
      "Epoch: 283 mean train loss:  4.82607235e-01, mean val. rec. loss:  4.34761531e-01\n",
      "Epoch: 284 mean train loss:  4.80328240e-01, mean val. rec. loss:  4.32715290e-01\n",
      "Epoch: 285 mean train loss:  4.78061923e-01, mean val. rec. loss:  4.30642700e-01\n",
      "Epoch: 286 mean train loss:  4.75782194e-01, mean val. rec. loss:  4.28604721e-01\n",
      "Epoch: 287 mean train loss:  4.73525440e-01, mean val. rec. loss:  4.26541212e-01\n",
      "Epoch: 288 mean train loss:  4.71251752e-01, mean val. rec. loss:  4.24516557e-01\n",
      "Epoch: 289 mean train loss:  4.69006707e-01, mean val. rec. loss:  4.22472847e-01\n",
      "Epoch: 290 mean train loss:  4.66786945e-01, mean val. rec. loss:  4.20461515e-01\n",
      "Epoch: 291 mean train loss:  4.64561807e-01, mean val. rec. loss:  4.18435073e-01\n",
      "Epoch: 292 mean train loss:  4.62332382e-01, mean val. rec. loss:  4.16431260e-01\n",
      "Epoch: 293 mean train loss:  4.60137841e-01, mean val. rec. loss:  4.14456251e-01\n",
      "Epoch: 294 mean train loss:  4.57910969e-01, mean val. rec. loss:  4.12477484e-01\n",
      "Epoch: 295 mean train loss:  4.55751652e-01, mean val. rec. loss:  4.10495665e-01\n",
      "Epoch: 296 mean train loss:  4.53572237e-01, mean val. rec. loss:  4.08529998e-01\n",
      "Epoch: 297 mean train loss:  4.51403405e-01, mean val. rec. loss:  4.06575719e-01\n",
      "Epoch: 298 mean train loss:  4.49226845e-01, mean val. rec. loss:  4.04620585e-01\n",
      "Epoch: 299 mean train loss:  4.47102483e-01, mean val. rec. loss:  4.02663217e-01\n",
      "Epoch: 300 mean train loss:  4.44932232e-01, mean val. rec. loss:  4.00711804e-01\n",
      "Epoch: 301 mean train loss:  4.42806442e-01, mean val. rec. loss:  3.98815025e-01\n",
      "Epoch: 302 mean train loss:  4.40694108e-01, mean val. rec. loss:  3.96904178e-01\n",
      "Epoch: 303 mean train loss:  4.38593165e-01, mean val. rec. loss:  3.95012832e-01\n",
      "Epoch: 304 mean train loss:  4.36492834e-01, mean val. rec. loss:  3.93114006e-01\n",
      "Epoch: 305 mean train loss:  4.34378865e-01, mean val. rec. loss:  3.91234272e-01\n",
      "Epoch: 306 mean train loss:  4.32299748e-01, mean val. rec. loss:  3.89368495e-01\n",
      "Epoch: 307 mean train loss:  4.30258265e-01, mean val. rec. loss:  3.87483550e-01\n",
      "Epoch: 308 mean train loss:  4.28208348e-01, mean val. rec. loss:  3.85595815e-01\n",
      "Epoch: 309 mean train loss:  4.26118862e-01, mean val. rec. loss:  3.83744998e-01\n",
      "Epoch: 310 mean train loss:  4.24082734e-01, mean val. rec. loss:  3.81897122e-01\n",
      "Epoch: 311 mean train loss:  4.22032600e-01, mean val. rec. loss:  3.80066476e-01\n",
      "Epoch: 312 mean train loss:  4.20015216e-01, mean val. rec. loss:  3.78226378e-01\n",
      "Epoch: 313 mean train loss:  4.18034457e-01, mean val. rec. loss:  3.76381590e-01\n",
      "Epoch: 314 mean train loss:  4.16020544e-01, mean val. rec. loss:  3.74561887e-01\n",
      "Epoch: 315 mean train loss:  4.14020475e-01, mean val. rec. loss:  3.72750110e-01\n",
      "Epoch: 316 mean train loss:  4.12042841e-01, mean val. rec. loss:  3.70964534e-01\n",
      "Epoch: 317 mean train loss:  4.10073838e-01, mean val. rec. loss:  3.69183275e-01\n",
      "Epoch: 318 mean train loss:  4.08092005e-01, mean val. rec. loss:  3.67394387e-01\n",
      "Epoch: 319 mean train loss:  4.06143528e-01, mean val. rec. loss:  3.65631625e-01\n",
      "Epoch: 320 mean train loss:  4.04189509e-01, mean val. rec. loss:  3.63907381e-01\n",
      "Epoch: 321 mean train loss:  4.02282407e-01, mean val. rec. loss:  3.62119014e-01\n",
      "Epoch: 322 mean train loss:  4.00346192e-01, mean val. rec. loss:  3.60352009e-01\n",
      "Epoch: 323 mean train loss:  3.98445574e-01, mean val. rec. loss:  3.58608637e-01\n",
      "Epoch: 324 mean train loss:  3.96545918e-01, mean val. rec. loss:  3.56895633e-01\n",
      "Epoch: 325 mean train loss:  3.94664292e-01, mean val. rec. loss:  3.55145859e-01\n",
      "Epoch: 326 mean train loss:  3.92731288e-01, mean val. rec. loss:  3.53456376e-01\n",
      "Epoch: 327 mean train loss:  3.90893030e-01, mean val. rec. loss:  3.51731947e-01\n",
      "Epoch: 328 mean train loss:  3.88990991e-01, mean val. rec. loss:  3.50033086e-01\n",
      "Epoch: 329 mean train loss:  3.87153497e-01, mean val. rec. loss:  3.48339025e-01\n",
      "Epoch: 330 mean train loss:  3.85316430e-01, mean val. rec. loss:  3.46668374e-01\n",
      "Epoch: 331 mean train loss:  3.83480305e-01, mean val. rec. loss:  3.44981384e-01\n",
      "Epoch: 332 mean train loss:  3.81644658e-01, mean val. rec. loss:  3.43323312e-01\n",
      "Epoch: 333 mean train loss:  3.79841289e-01, mean val. rec. loss:  3.41664496e-01\n",
      "Epoch: 334 mean train loss:  3.78021464e-01, mean val. rec. loss:  3.40015393e-01\n",
      "Epoch: 335 mean train loss:  3.76226143e-01, mean val. rec. loss:  3.38385903e-01\n",
      "Epoch: 336 mean train loss:  3.74483365e-01, mean val. rec. loss:  3.36737656e-01\n",
      "Epoch: 337 mean train loss:  3.72658200e-01, mean val. rec. loss:  3.35107161e-01\n",
      "Epoch: 338 mean train loss:  3.70904368e-01, mean val. rec. loss:  3.33478490e-01\n",
      "Epoch: 339 mean train loss:  3.69167607e-01, mean val. rec. loss:  3.31878216e-01\n",
      "Epoch: 340 mean train loss:  3.67391030e-01, mean val. rec. loss:  3.30270535e-01\n",
      "Epoch: 341 mean train loss:  3.65664360e-01, mean val. rec. loss:  3.28665943e-01\n",
      "Epoch: 342 mean train loss:  3.63935268e-01, mean val. rec. loss:  3.27084425e-01\n",
      "Epoch: 343 mean train loss:  3.62216148e-01, mean val. rec. loss:  3.25506666e-01\n",
      "Epoch: 344 mean train loss:  3.60513126e-01, mean val. rec. loss:  3.23926451e-01\n",
      "Epoch: 345 mean train loss:  3.58802071e-01, mean val. rec. loss:  3.22380699e-01\n",
      "Epoch: 346 mean train loss:  3.57112044e-01, mean val. rec. loss:  3.20829959e-01\n",
      "Epoch: 347 mean train loss:  3.55447515e-01, mean val. rec. loss:  3.19280373e-01\n",
      "Epoch: 348 mean train loss:  3.53765865e-01, mean val. rec. loss:  3.17767706e-01\n",
      "Epoch: 349 mean train loss:  3.52122243e-01, mean val. rec. loss:  3.16210007e-01\n",
      "Epoch: 350 mean train loss:  3.50441710e-01, mean val. rec. loss:  3.14692130e-01\n",
      "Epoch: 351 mean train loss:  3.48854163e-01, mean val. rec. loss:  3.13150248e-01\n",
      "Epoch: 352 mean train loss:  3.47183288e-01, mean val. rec. loss:  3.11666479e-01\n",
      "Epoch: 353 mean train loss:  3.45582180e-01, mean val. rec. loss:  3.10161758e-01\n",
      "Epoch: 354 mean train loss:  3.43970763e-01, mean val. rec. loss:  3.08646523e-01\n",
      "Epoch: 355 mean train loss:  3.42345381e-01, mean val. rec. loss:  3.07174534e-01\n",
      "Epoch: 356 mean train loss:  3.40764187e-01, mean val. rec. loss:  3.05691603e-01\n",
      "Epoch: 357 mean train loss:  3.39173421e-01, mean val. rec. loss:  3.04222293e-01\n",
      "Epoch: 358 mean train loss:  3.37595778e-01, mean val. rec. loss:  3.02750601e-01\n",
      "Epoch: 359 mean train loss:  3.36034130e-01, mean val. rec. loss:  3.01310879e-01\n",
      "Epoch: 360 mean train loss:  3.34499941e-01, mean val. rec. loss:  2.99865592e-01\n",
      "Epoch: 361 mean train loss:  3.32950788e-01, mean val. rec. loss:  2.98411188e-01\n",
      "Epoch: 362 mean train loss:  3.31404717e-01, mean val. rec. loss:  2.96987357e-01\n",
      "Epoch: 363 mean train loss:  3.29872909e-01, mean val. rec. loss:  2.95599514e-01\n",
      "Epoch: 364 mean train loss:  3.28373742e-01, mean val. rec. loss:  2.94168482e-01\n",
      "Epoch: 365 mean train loss:  3.26852500e-01, mean val. rec. loss:  2.92786147e-01\n",
      "Epoch: 366 mean train loss:  3.25359144e-01, mean val. rec. loss:  2.91371694e-01\n",
      "Epoch: 367 mean train loss:  3.23894682e-01, mean val. rec. loss:  2.89944756e-01\n",
      "Epoch: 368 mean train loss:  3.22394039e-01, mean val. rec. loss:  2.88554196e-01\n",
      "Epoch: 369 mean train loss:  3.20906282e-01, mean val. rec. loss:  2.87201225e-01\n",
      "Epoch: 370 mean train loss:  3.19460681e-01, mean val. rec. loss:  2.85803520e-01\n",
      "Epoch: 371 mean train loss:  3.17998404e-01, mean val. rec. loss:  2.84438789e-01\n",
      "Epoch: 372 mean train loss:  3.16563503e-01, mean val. rec. loss:  2.83121007e-01\n",
      "Epoch: 373 mean train loss:  3.15143876e-01, mean val. rec. loss:  2.81768389e-01\n",
      "Epoch: 374 mean train loss:  3.13681167e-01, mean val. rec. loss:  2.80427272e-01\n",
      "Epoch: 375 mean train loss:  3.12277738e-01, mean val. rec. loss:  2.79095180e-01\n",
      "Epoch: 376 mean train loss:  3.10875950e-01, mean val. rec. loss:  2.77755588e-01\n",
      "Epoch: 377 mean train loss:  3.09491569e-01, mean val. rec. loss:  2.76472008e-01\n",
      "Epoch: 378 mean train loss:  3.08088605e-01, mean val. rec. loss:  2.75144382e-01\n",
      "Epoch: 379 mean train loss:  3.06709418e-01, mean val. rec. loss:  2.73823436e-01\n",
      "Epoch: 380 mean train loss:  3.05351828e-01, mean val. rec. loss:  2.72518344e-01\n",
      "Epoch: 381 mean train loss:  3.03957456e-01, mean val. rec. loss:  2.71251176e-01\n",
      "Epoch: 382 mean train loss:  3.02621685e-01, mean val. rec. loss:  2.69966051e-01\n",
      "Epoch: 383 mean train loss:  3.01258711e-01, mean val. rec. loss:  2.68696669e-01\n",
      "Epoch: 384 mean train loss:  2.99943486e-01, mean val. rec. loss:  2.67430226e-01\n",
      "Epoch: 385 mean train loss:  2.98598745e-01, mean val. rec. loss:  2.66145622e-01\n",
      "Epoch: 386 mean train loss:  2.97281533e-01, mean val. rec. loss:  2.64902329e-01\n",
      "Epoch: 387 mean train loss:  2.95973145e-01, mean val. rec. loss:  2.63642976e-01\n",
      "Epoch: 388 mean train loss:  2.94667111e-01, mean val. rec. loss:  2.62391718e-01\n",
      "Epoch: 389 mean train loss:  2.93357793e-01, mean val. rec. loss:  2.61175537e-01\n",
      "Epoch: 390 mean train loss:  2.92091766e-01, mean val. rec. loss:  2.59928522e-01\n",
      "Epoch: 391 mean train loss:  2.90774756e-01, mean val. rec. loss:  2.58752814e-01\n",
      "Epoch: 392 mean train loss:  2.89525329e-01, mean val. rec. loss:  2.57550049e-01\n",
      "Epoch: 393 mean train loss:  2.88259675e-01, mean val. rec. loss:  2.56328694e-01\n",
      "Epoch: 394 mean train loss:  2.87002264e-01, mean val. rec. loss:  2.55155498e-01\n",
      "Epoch: 395 mean train loss:  2.85784226e-01, mean val. rec. loss:  2.53974487e-01\n",
      "Epoch: 396 mean train loss:  2.84508172e-01, mean val. rec. loss:  2.52775871e-01\n",
      "Epoch: 397 mean train loss:  2.83280979e-01, mean val. rec. loss:  2.51587621e-01\n",
      "Epoch: 398 mean train loss:  2.82058985e-01, mean val. rec. loss:  2.50419542e-01\n",
      "Epoch: 399 mean train loss:  2.80862855e-01, mean val. rec. loss:  2.49239275e-01\n",
      "Epoch: 400 mean train loss:  2.79641212e-01, mean val. rec. loss:  2.48116861e-01\n",
      "Epoch: 401 mean train loss:  2.78448840e-01, mean val. rec. loss:  2.46958998e-01\n",
      "Epoch: 402 mean train loss:  2.77231889e-01, mean val. rec. loss:  2.45781019e-01\n",
      "Epoch: 403 mean train loss:  2.76051167e-01, mean val. rec. loss:  2.44647421e-01\n",
      "Epoch: 404 mean train loss:  2.74867606e-01, mean val. rec. loss:  2.43513935e-01\n",
      "Epoch: 405 mean train loss:  2.73698399e-01, mean val. rec. loss:  2.42407543e-01\n",
      "Epoch: 406 mean train loss:  2.72541765e-01, mean val. rec. loss:  2.41294489e-01\n",
      "Epoch: 407 mean train loss:  2.71396975e-01, mean val. rec. loss:  2.40166585e-01\n",
      "Epoch: 408 mean train loss:  2.70233918e-01, mean val. rec. loss:  2.39066166e-01\n",
      "Epoch: 409 mean train loss:  2.69104945e-01, mean val. rec. loss:  2.37998534e-01\n",
      "Epoch: 410 mean train loss:  2.67959793e-01, mean val. rec. loss:  2.36887341e-01\n",
      "Epoch: 411 mean train loss:  2.66828829e-01, mean val. rec. loss:  2.35809382e-01\n",
      "Epoch: 412 mean train loss:  2.65695202e-01, mean val. rec. loss:  2.34731088e-01\n",
      "Epoch: 413 mean train loss:  2.64584928e-01, mean val. rec. loss:  2.33654525e-01\n",
      "Epoch: 414 mean train loss:  2.63471119e-01, mean val. rec. loss:  2.32563968e-01\n",
      "Epoch: 415 mean train loss:  2.62366053e-01, mean val. rec. loss:  2.31514889e-01\n",
      "Epoch: 416 mean train loss:  2.61309264e-01, mean val. rec. loss:  2.30425374e-01\n",
      "Epoch: 417 mean train loss:  2.60195284e-01, mean val. rec. loss:  2.29400672e-01\n",
      "Epoch: 418 mean train loss:  2.59110961e-01, mean val. rec. loss:  2.28376844e-01\n",
      "Epoch: 419 mean train loss:  2.58046897e-01, mean val. rec. loss:  2.27320787e-01\n",
      "Epoch: 420 mean train loss:  2.56969988e-01, mean val. rec. loss:  2.26273774e-01\n",
      "Epoch: 421 mean train loss:  2.55889898e-01, mean val. rec. loss:  2.25242764e-01\n",
      "Epoch: 422 mean train loss:  2.54855766e-01, mean val. rec. loss:  2.24223142e-01\n",
      "Epoch: 423 mean train loss:  2.53790847e-01, mean val. rec. loss:  2.23224900e-01\n",
      "Epoch: 424 mean train loss:  2.52764265e-01, mean val. rec. loss:  2.22247314e-01\n",
      "Epoch: 425 mean train loss:  2.51739644e-01, mean val. rec. loss:  2.21236736e-01\n",
      "Epoch: 426 mean train loss:  2.50694076e-01, mean val. rec. loss:  2.20232503e-01\n",
      "Epoch: 427 mean train loss:  2.49654803e-01, mean val. rec. loss:  2.19241147e-01\n",
      "Epoch: 428 mean train loss:  2.48630156e-01, mean val. rec. loss:  2.18257923e-01\n",
      "Epoch: 429 mean train loss:  2.47625831e-01, mean val. rec. loss:  2.17288152e-01\n",
      "Epoch: 430 mean train loss:  2.46620712e-01, mean val. rec. loss:  2.16305430e-01\n",
      "Epoch: 431 mean train loss:  2.45621696e-01, mean val. rec. loss:  2.15321387e-01\n",
      "Epoch: 432 mean train loss:  2.44634664e-01, mean val. rec. loss:  2.14353385e-01\n",
      "Epoch: 433 mean train loss:  2.43615860e-01, mean val. rec. loss:  2.13403413e-01\n",
      "Epoch: 434 mean train loss:  2.42657281e-01, mean val. rec. loss:  2.12453368e-01\n",
      "Epoch: 435 mean train loss:  2.41666764e-01, mean val. rec. loss:  2.11501443e-01\n",
      "Epoch: 436 mean train loss:  2.40691382e-01, mean val. rec. loss:  2.10547061e-01\n",
      "Epoch: 437 mean train loss:  2.39715231e-01, mean val. rec. loss:  2.09614712e-01\n",
      "Epoch: 438 mean train loss:  2.38754193e-01, mean val. rec. loss:  2.08666620e-01\n",
      "Epoch: 439 mean train loss:  2.37811418e-01, mean val. rec. loss:  2.07725339e-01\n",
      "Epoch: 440 mean train loss:  2.36839863e-01, mean val. rec. loss:  2.06823582e-01\n",
      "Epoch: 441 mean train loss:  2.35889933e-01, mean val. rec. loss:  2.05918866e-01\n",
      "Epoch: 442 mean train loss:  2.34958642e-01, mean val. rec. loss:  2.05016680e-01\n",
      "Epoch: 443 mean train loss:  2.34004149e-01, mean val. rec. loss:  2.04098250e-01\n",
      "Epoch: 444 mean train loss:  2.33084852e-01, mean val. rec. loss:  2.03210859e-01\n",
      "Epoch: 445 mean train loss:  2.32158405e-01, mean val. rec. loss:  2.02301044e-01\n",
      "Epoch: 446 mean train loss:  2.31242683e-01, mean val. rec. loss:  2.01377013e-01\n",
      "Epoch: 447 mean train loss:  2.30314516e-01, mean val. rec. loss:  2.00493343e-01\n",
      "Epoch: 448 mean train loss:  2.29396615e-01, mean val. rec. loss:  1.99613339e-01\n",
      "Epoch: 449 mean train loss:  2.28494067e-01, mean val. rec. loss:  1.98743495e-01\n",
      "Epoch: 450 mean train loss:  2.27589768e-01, mean val. rec. loss:  1.97859899e-01\n",
      "Epoch: 451 mean train loss:  2.26673146e-01, mean val. rec. loss:  1.97001834e-01\n",
      "Epoch: 452 mean train loss:  2.25796081e-01, mean val. rec. loss:  1.96138689e-01\n",
      "Epoch: 453 mean train loss:  2.24917138e-01, mean val. rec. loss:  1.95298432e-01\n",
      "Epoch: 454 mean train loss:  2.24025882e-01, mean val. rec. loss:  1.94427472e-01\n",
      "Epoch: 455 mean train loss:  2.23126722e-01, mean val. rec. loss:  1.93578618e-01\n",
      "Epoch: 456 mean train loss:  2.22259504e-01, mean val. rec. loss:  1.92738026e-01\n",
      "Epoch: 457 mean train loss:  2.21373440e-01, mean val. rec. loss:  1.91887274e-01\n",
      "Epoch: 458 mean train loss:  2.20519646e-01, mean val. rec. loss:  1.91057494e-01\n",
      "Epoch: 459 mean train loss:  2.19648726e-01, mean val. rec. loss:  1.90233631e-01\n",
      "Epoch: 460 mean train loss:  2.18801522e-01, mean val. rec. loss:  1.89406009e-01\n",
      "Epoch: 461 mean train loss:  2.17930415e-01, mean val. rec. loss:  1.88569623e-01\n",
      "Epoch: 462 mean train loss:  2.17092610e-01, mean val. rec. loss:  1.87737107e-01\n",
      "Epoch: 463 mean train loss:  2.16231239e-01, mean val. rec. loss:  1.86921171e-01\n",
      "Epoch: 464 mean train loss:  2.15386760e-01, mean val. rec. loss:  1.86107245e-01\n",
      "Epoch: 465 mean train loss:  2.14556795e-01, mean val. rec. loss:  1.85337718e-01\n",
      "Epoch: 466 mean train loss:  2.13717463e-01, mean val. rec. loss:  1.84532240e-01\n",
      "Epoch: 467 mean train loss:  2.12886757e-01, mean val. rec. loss:  1.83732270e-01\n",
      "Epoch: 468 mean train loss:  2.12056364e-01, mean val. rec. loss:  1.82917358e-01\n",
      "Epoch: 469 mean train loss:  2.11240743e-01, mean val. rec. loss:  1.82145319e-01\n",
      "Epoch: 470 mean train loss:  2.10405656e-01, mean val. rec. loss:  1.81349182e-01\n",
      "Epoch: 471 mean train loss:  2.09608886e-01, mean val. rec. loss:  1.80565029e-01\n",
      "Epoch: 472 mean train loss:  2.08771038e-01, mean val. rec. loss:  1.79797661e-01\n",
      "Epoch: 473 mean train loss:  2.07981374e-01, mean val. rec. loss:  1.79018532e-01\n",
      "Epoch: 474 mean train loss:  2.07176358e-01, mean val. rec. loss:  1.78236184e-01\n",
      "Epoch: 475 mean train loss:  2.06359418e-01, mean val. rec. loss:  1.77469914e-01\n",
      "Epoch: 476 mean train loss:  2.05586630e-01, mean val. rec. loss:  1.76726606e-01\n",
      "Epoch: 477 mean train loss:  2.04762327e-01, mean val. rec. loss:  1.75969119e-01\n",
      "Epoch: 478 mean train loss:  2.03984415e-01, mean val. rec. loss:  1.75205082e-01\n",
      "Epoch: 479 mean train loss:  2.03201937e-01, mean val. rec. loss:  1.74459653e-01\n",
      "Epoch: 480 mean train loss:  2.02418124e-01, mean val. rec. loss:  1.73708939e-01\n",
      "Epoch: 481 mean train loss:  2.01623912e-01, mean val. rec. loss:  1.72962989e-01\n",
      "Epoch: 482 mean train loss:  2.00851394e-01, mean val. rec. loss:  1.72215811e-01\n",
      "Epoch: 483 mean train loss:  2.00069437e-01, mean val. rec. loss:  1.71463217e-01\n",
      "Epoch: 484 mean train loss:  1.99303297e-01, mean val. rec. loss:  1.70759713e-01\n",
      "Epoch: 485 mean train loss:  1.98532871e-01, mean val. rec. loss:  1.70030622e-01\n",
      "Epoch: 486 mean train loss:  1.97766573e-01, mean val. rec. loss:  1.69324307e-01\n",
      "Epoch: 487 mean train loss:  1.97015469e-01, mean val. rec. loss:  1.68605711e-01\n",
      "Epoch: 488 mean train loss:  1.96244031e-01, mean val. rec. loss:  1.67870461e-01\n",
      "Epoch: 489 mean train loss:  1.95505963e-01, mean val. rec. loss:  1.67173358e-01\n",
      "Epoch: 490 mean train loss:  1.94742501e-01, mean val. rec. loss:  1.66456195e-01\n",
      "Epoch: 491 mean train loss:  1.93995354e-01, mean val. rec. loss:  1.65747815e-01\n",
      "Epoch: 492 mean train loss:  1.93242263e-01, mean val. rec. loss:  1.65034336e-01\n",
      "Epoch: 493 mean train loss:  1.92514063e-01, mean val. rec. loss:  1.64317136e-01\n",
      "Epoch: 494 mean train loss:  1.91760345e-01, mean val. rec. loss:  1.63628183e-01\n",
      "Epoch: 495 mean train loss:  1.91019869e-01, mean val. rec. loss:  1.62956759e-01\n",
      "Epoch: 496 mean train loss:  1.90303950e-01, mean val. rec. loss:  1.62262112e-01\n",
      "Epoch: 497 mean train loss:  1.89564493e-01, mean val. rec. loss:  1.61591246e-01\n",
      "Epoch: 498 mean train loss:  1.88845081e-01, mean val. rec. loss:  1.60911746e-01\n",
      "Epoch: 499 mean train loss:  1.88094725e-01, mean val. rec. loss:  1.60228394e-01\n",
      "Epoch: 500 mean train loss:  1.87389493e-01, mean val. rec. loss:  1.59557510e-01\n",
      "Epoch: 501 mean train loss:  1.86657954e-01, mean val. rec. loss:  1.58884076e-01\n",
      "Epoch: 502 mean train loss:  1.85951150e-01, mean val. rec. loss:  1.58203962e-01\n",
      "Epoch: 503 mean train loss:  1.85226167e-01, mean val. rec. loss:  1.57537898e-01\n",
      "Epoch: 504 mean train loss:  1.84517944e-01, mean val. rec. loss:  1.56881304e-01\n",
      "Epoch: 505 mean train loss:  1.83813097e-01, mean val. rec. loss:  1.56210067e-01\n",
      "Epoch: 506 mean train loss:  1.83101883e-01, mean val. rec. loss:  1.55562024e-01\n",
      "Epoch: 507 mean train loss:  1.82402617e-01, mean val. rec. loss:  1.54897048e-01\n",
      "Epoch: 508 mean train loss:  1.81705503e-01, mean val. rec. loss:  1.54264590e-01\n",
      "Epoch: 509 mean train loss:  1.81005955e-01, mean val. rec. loss:  1.53623702e-01\n",
      "Epoch: 510 mean train loss:  1.80308203e-01, mean val. rec. loss:  1.52974636e-01\n",
      "Epoch: 511 mean train loss:  1.79616422e-01, mean val. rec. loss:  1.52331580e-01\n",
      "Epoch: 512 mean train loss:  1.78925742e-01, mean val. rec. loss:  1.51687883e-01\n",
      "Epoch: 513 mean train loss:  1.78235522e-01, mean val. rec. loss:  1.51038081e-01\n",
      "Epoch: 514 mean train loss:  1.77543248e-01, mean val. rec. loss:  1.50416528e-01\n",
      "Epoch: 515 mean train loss:  1.76858883e-01, mean val. rec. loss:  1.49783558e-01\n",
      "Epoch: 516 mean train loss:  1.76183770e-01, mean val. rec. loss:  1.49176472e-01\n",
      "Epoch: 517 mean train loss:  1.75500652e-01, mean val. rec. loss:  1.48558286e-01\n",
      "Epoch: 518 mean train loss:  1.74840317e-01, mean val. rec. loss:  1.47923995e-01\n",
      "Epoch: 519 mean train loss:  1.74168551e-01, mean val. rec. loss:  1.47315616e-01\n",
      "Epoch: 520 mean train loss:  1.73482003e-01, mean val. rec. loss:  1.46700054e-01\n",
      "Epoch: 521 mean train loss:  1.72831523e-01, mean val. rec. loss:  1.46105082e-01\n",
      "Epoch: 522 mean train loss:  1.72150562e-01, mean val. rec. loss:  1.45482701e-01\n",
      "Epoch: 523 mean train loss:  1.71490138e-01, mean val. rec. loss:  1.44875298e-01\n",
      "Epoch: 524 mean train loss:  1.70842225e-01, mean val. rec. loss:  1.44299028e-01\n",
      "Epoch: 525 mean train loss:  1.70164765e-01, mean val. rec. loss:  1.43702921e-01\n",
      "Epoch: 526 mean train loss:  1.69516625e-01, mean val. rec. loss:  1.43106619e-01\n",
      "Epoch: 527 mean train loss:  1.68859886e-01, mean val. rec. loss:  1.42489540e-01\n",
      "Epoch: 528 mean train loss:  1.68204320e-01, mean val. rec. loss:  1.41901370e-01\n",
      "Epoch: 529 mean train loss:  1.67554465e-01, mean val. rec. loss:  1.41302434e-01\n",
      "Epoch: 530 mean train loss:  1.66913936e-01, mean val. rec. loss:  1.40730007e-01\n",
      "Epoch: 531 mean train loss:  1.66266441e-01, mean val. rec. loss:  1.40145614e-01\n",
      "Epoch: 532 mean train loss:  1.65625173e-01, mean val. rec. loss:  1.39569892e-01\n",
      "Epoch: 533 mean train loss:  1.64980640e-01, mean val. rec. loss:  1.38992291e-01\n",
      "Epoch: 534 mean train loss:  1.64336329e-01, mean val. rec. loss:  1.38398771e-01\n",
      "Epoch: 535 mean train loss:  1.63705275e-01, mean val. rec. loss:  1.37842206e-01\n",
      "Epoch: 536 mean train loss:  1.63075021e-01, mean val. rec. loss:  1.37268969e-01\n",
      "Epoch: 537 mean train loss:  1.62432692e-01, mean val. rec. loss:  1.36728129e-01\n",
      "Epoch: 538 mean train loss:  1.61802951e-01, mean val. rec. loss:  1.36164577e-01\n",
      "Epoch: 539 mean train loss:  1.61177105e-01, mean val. rec. loss:  1.35608580e-01\n",
      "Epoch: 540 mean train loss:  1.60557175e-01, mean val. rec. loss:  1.35040944e-01\n",
      "Epoch: 541 mean train loss:  1.59937165e-01, mean val. rec. loss:  1.34470665e-01\n",
      "Epoch: 542 mean train loss:  1.59306800e-01, mean val. rec. loss:  1.33891352e-01\n",
      "Epoch: 543 mean train loss:  1.58693216e-01, mean val. rec. loss:  1.33361835e-01\n",
      "Epoch: 544 mean train loss:  1.58074895e-01, mean val. rec. loss:  1.32815440e-01\n",
      "Epoch: 545 mean train loss:  1.57463613e-01, mean val. rec. loss:  1.32278591e-01\n",
      "Epoch: 546 mean train loss:  1.56857396e-01, mean val. rec. loss:  1.31733480e-01\n",
      "Epoch: 547 mean train loss:  1.56240367e-01, mean val. rec. loss:  1.31223967e-01\n",
      "Epoch: 548 mean train loss:  1.55626930e-01, mean val. rec. loss:  1.30672706e-01\n",
      "Epoch: 549 mean train loss:  1.55015580e-01, mean val. rec. loss:  1.30132294e-01\n",
      "Epoch: 550 mean train loss:  1.54414328e-01, mean val. rec. loss:  1.29596636e-01\n",
      "Epoch: 551 mean train loss:  1.53809961e-01, mean val. rec. loss:  1.29081894e-01\n",
      "Epoch: 552 mean train loss:  1.53222473e-01, mean val. rec. loss:  1.28538513e-01\n",
      "Epoch: 553 mean train loss:  1.52619241e-01, mean val. rec. loss:  1.28013201e-01\n",
      "Epoch: 554 mean train loss:  1.52004235e-01, mean val. rec. loss:  1.27492114e-01\n",
      "Epoch: 555 mean train loss:  1.51425067e-01, mean val. rec. loss:  1.26968067e-01\n",
      "Epoch: 556 mean train loss:  1.50832945e-01, mean val. rec. loss:  1.26446729e-01\n",
      "Epoch: 557 mean train loss:  1.50231380e-01, mean val. rec. loss:  1.25934545e-01\n",
      "Epoch: 558 mean train loss:  1.49644443e-01, mean val. rec. loss:  1.25422371e-01\n",
      "Epoch: 559 mean train loss:  1.49062836e-01, mean val. rec. loss:  1.24916058e-01\n",
      "Epoch: 560 mean train loss:  1.48479461e-01, mean val. rec. loss:  1.24404377e-01\n",
      "Epoch: 561 mean train loss:  1.47885675e-01, mean val. rec. loss:  1.23910020e-01\n",
      "Epoch: 562 mean train loss:  1.47317686e-01, mean val. rec. loss:  1.23402135e-01\n",
      "Epoch: 563 mean train loss:  1.46739575e-01, mean val. rec. loss:  1.22919632e-01\n",
      "Epoch: 564 mean train loss:  1.46147729e-01, mean val. rec. loss:  1.22422753e-01\n",
      "Epoch: 565 mean train loss:  1.45596810e-01, mean val. rec. loss:  1.21921818e-01\n",
      "Epoch: 566 mean train loss:  1.45001555e-01, mean val. rec. loss:  1.21423424e-01\n",
      "Epoch: 567 mean train loss:  1.44445414e-01, mean val. rec. loss:  1.20912273e-01\n",
      "Epoch: 568 mean train loss:  1.43864190e-01, mean val. rec. loss:  1.20417972e-01\n",
      "Epoch: 569 mean train loss:  1.43293486e-01, mean val. rec. loss:  1.19939088e-01\n",
      "Epoch: 570 mean train loss:  1.42724732e-01, mean val. rec. loss:  1.19455347e-01\n",
      "Epoch: 571 mean train loss:  1.42168925e-01, mean val. rec. loss:  1.18985181e-01\n",
      "Epoch: 572 mean train loss:  1.41609365e-01, mean val. rec. loss:  1.18493429e-01\n",
      "Epoch: 573 mean train loss:  1.41045558e-01, mean val. rec. loss:  1.18018695e-01\n",
      "Epoch: 574 mean train loss:  1.40491147e-01, mean val. rec. loss:  1.17549794e-01\n",
      "Epoch: 575 mean train loss:  1.39932841e-01, mean val. rec. loss:  1.17053930e-01\n",
      "Epoch: 576 mean train loss:  1.39374734e-01, mean val. rec. loss:  1.16582982e-01\n",
      "Epoch: 577 mean train loss:  1.38826204e-01, mean val. rec. loss:  1.16120557e-01\n",
      "Epoch: 578 mean train loss:  1.38279062e-01, mean val. rec. loss:  1.15657313e-01\n",
      "Epoch: 579 mean train loss:  1.37719535e-01, mean val. rec. loss:  1.15193883e-01\n",
      "Epoch: 580 mean train loss:  1.37180586e-01, mean val. rec. loss:  1.14728714e-01\n",
      "Epoch: 581 mean train loss:  1.36626280e-01, mean val. rec. loss:  1.14272439e-01\n",
      "Epoch: 582 mean train loss:  1.36081319e-01, mean val. rec. loss:  1.13809865e-01\n",
      "Epoch: 583 mean train loss:  1.35557376e-01, mean val. rec. loss:  1.13359144e-01\n",
      "Epoch: 584 mean train loss:  1.35004400e-01, mean val. rec. loss:  1.12911057e-01\n",
      "Epoch: 585 mean train loss:  1.34457288e-01, mean val. rec. loss:  1.12462746e-01\n",
      "Epoch: 586 mean train loss:  1.33923375e-01, mean val. rec. loss:  1.12000758e-01\n",
      "Epoch: 587 mean train loss:  1.33396231e-01, mean val. rec. loss:  1.11550196e-01\n",
      "Epoch: 588 mean train loss:  1.32863515e-01, mean val. rec. loss:  1.11086971e-01\n",
      "Epoch: 589 mean train loss:  1.32327392e-01, mean val. rec. loss:  1.10644531e-01\n",
      "Epoch: 590 mean train loss:  1.31793034e-01, mean val. rec. loss:  1.10205227e-01\n",
      "Epoch: 591 mean train loss:  1.31264690e-01, mean val. rec. loss:  1.09763755e-01\n",
      "Epoch: 592 mean train loss:  1.30753804e-01, mean val. rec. loss:  1.09318989e-01\n",
      "Epoch: 593 mean train loss:  1.30212763e-01, mean val. rec. loss:  1.08882132e-01\n",
      "Epoch: 594 mean train loss:  1.29695730e-01, mean val. rec. loss:  1.08461333e-01\n",
      "Epoch: 595 mean train loss:  1.29173339e-01, mean val. rec. loss:  1.08028412e-01\n",
      "Epoch: 596 mean train loss:  1.28656469e-01, mean val. rec. loss:  1.07609967e-01\n",
      "Epoch: 597 mean train loss:  1.28143474e-01, mean val. rec. loss:  1.07184331e-01\n",
      "Epoch: 598 mean train loss:  1.27621339e-01, mean val. rec. loss:  1.06743119e-01\n",
      "Epoch: 599 mean train loss:  1.27103424e-01, mean val. rec. loss:  1.06335245e-01\n",
      "Epoch: 600 mean train loss:  1.26597353e-01, mean val. rec. loss:  1.05895568e-01\n",
      "Epoch: 601 mean train loss:  1.26090045e-01, mean val. rec. loss:  1.05468955e-01\n",
      "Epoch: 602 mean train loss:  1.25578638e-01, mean val. rec. loss:  1.05048891e-01\n",
      "Epoch: 603 mean train loss:  1.25063670e-01, mean val. rec. loss:  1.04636495e-01\n",
      "Epoch: 604 mean train loss:  1.24566462e-01, mean val. rec. loss:  1.04231420e-01\n",
      "Epoch: 605 mean train loss:  1.24061491e-01, mean val. rec. loss:  1.03820680e-01\n",
      "Epoch: 606 mean train loss:  1.23553564e-01, mean val. rec. loss:  1.03409483e-01\n",
      "Epoch: 607 mean train loss:  1.23066924e-01, mean val. rec. loss:  1.02999710e-01\n",
      "Epoch: 608 mean train loss:  1.22566668e-01, mean val. rec. loss:  1.02607020e-01\n",
      "Epoch: 609 mean train loss:  1.22049615e-01, mean val. rec. loss:  1.02185394e-01\n",
      "Epoch: 610 mean train loss:  1.21572436e-01, mean val. rec. loss:  1.01755105e-01\n",
      "Epoch: 611 mean train loss:  1.21080625e-01, mean val. rec. loss:  1.01355139e-01\n",
      "Epoch: 612 mean train loss:  1.20587957e-01, mean val. rec. loss:  1.00967472e-01\n",
      "Epoch: 613 mean train loss:  1.20091085e-01, mean val. rec. loss:  1.00587919e-01\n",
      "Epoch: 614 mean train loss:  1.19614642e-01, mean val. rec. loss:  1.00206347e-01\n",
      "Epoch: 615 mean train loss:  1.19116541e-01, mean val. rec. loss:  9.98017195e-02\n",
      "Epoch: 616 mean train loss:  1.18636256e-01, mean val. rec. loss:  9.94033441e-02\n",
      "Epoch: 617 mean train loss:  1.18160309e-01, mean val. rec. loss:  9.90065877e-02\n",
      "Epoch: 618 mean train loss:  1.17674348e-01, mean val. rec. loss:  9.86117479e-02\n",
      "Epoch: 619 mean train loss:  1.17200258e-01, mean val. rec. loss:  9.82291803e-02\n",
      "Epoch: 620 mean train loss:  1.16711166e-01, mean val. rec. loss:  9.78415140e-02\n",
      "Epoch: 621 mean train loss:  1.16242209e-01, mean val. rec. loss:  9.74570112e-02\n",
      "Epoch: 622 mean train loss:  1.15765639e-01, mean val. rec. loss:  9.70843618e-02\n",
      "Epoch: 623 mean train loss:  1.15294031e-01, mean val. rec. loss:  9.67026501e-02\n",
      "Epoch: 624 mean train loss:  1.14834025e-01, mean val. rec. loss:  9.63218689e-02\n",
      "Epoch: 625 mean train loss:  1.14360321e-01, mean val. rec. loss:  9.59639852e-02\n",
      "Epoch: 626 mean train loss:  1.13885085e-01, mean val. rec. loss:  9.55808872e-02\n",
      "Epoch: 627 mean train loss:  1.13416561e-01, mean val. rec. loss:  9.52078284e-02\n",
      "Epoch: 628 mean train loss:  1.12956119e-01, mean val. rec. loss:  9.48175012e-02\n",
      "Epoch: 629 mean train loss:  1.12490651e-01, mean val. rec. loss:  9.44369060e-02\n",
      "Epoch: 630 mean train loss:  1.12031127e-01, mean val. rec. loss:  9.40802039e-02\n",
      "Epoch: 631 mean train loss:  1.11581248e-01, mean val. rec. loss:  9.37129974e-02\n",
      "Epoch: 632 mean train loss:  1.11114734e-01, mean val. rec. loss:  9.33639247e-02\n",
      "Epoch: 633 mean train loss:  1.10662495e-01, mean val. rec. loss:  9.30171874e-02\n",
      "Epoch: 634 mean train loss:  1.10211708e-01, mean val. rec. loss:  9.26425096e-02\n",
      "Epoch: 635 mean train loss:  1.09757215e-01, mean val. rec. loss:  9.22868310e-02\n",
      "Epoch: 636 mean train loss:  1.09312191e-01, mean val. rec. loss:  9.19428197e-02\n",
      "Epoch: 637 mean train loss:  1.08854472e-01, mean val. rec. loss:  9.15776509e-02\n",
      "Epoch: 638 mean train loss:  1.08419687e-01, mean val. rec. loss:  9.12101280e-02\n",
      "Epoch: 639 mean train loss:  1.07969702e-01, mean val. rec. loss:  9.08630464e-02\n",
      "Epoch: 640 mean train loss:  1.07530016e-01, mean val. rec. loss:  9.05026599e-02\n",
      "Epoch: 641 mean train loss:  1.07080394e-01, mean val. rec. loss:  9.01525730e-02\n",
      "Epoch: 642 mean train loss:  1.06646672e-01, mean val. rec. loss:  8.97966711e-02\n",
      "Epoch: 643 mean train loss:  1.06205909e-01, mean val. rec. loss:  8.94687932e-02\n",
      "Epoch: 644 mean train loss:  1.05765783e-01, mean val. rec. loss:  8.91297689e-02\n",
      "Epoch: 645 mean train loss:  1.05327830e-01, mean val. rec. loss:  8.87680798e-02\n",
      "Epoch: 646 mean train loss:  1.04897031e-01, mean val. rec. loss:  8.84389924e-02\n",
      "Epoch: 647 mean train loss:  1.04463659e-01, mean val. rec. loss:  8.81044900e-02\n",
      "Epoch: 648 mean train loss:  1.04036856e-01, mean val. rec. loss:  8.77653355e-02\n",
      "Epoch: 649 mean train loss:  1.03610488e-01, mean val. rec. loss:  8.74189051e-02\n",
      "Epoch: 650 mean train loss:  1.03173098e-01, mean val. rec. loss:  8.70703162e-02\n",
      "Epoch: 651 mean train loss:  1.02755588e-01, mean val. rec. loss:  8.67503841e-02\n",
      "Epoch: 652 mean train loss:  1.02338141e-01, mean val. rec. loss:  8.64100200e-02\n",
      "Epoch: 653 mean train loss:  1.01917858e-01, mean val. rec. loss:  8.60748012e-02\n",
      "Epoch: 654 mean train loss:  1.01496642e-01, mean val. rec. loss:  8.57792180e-02\n",
      "Epoch: 655 mean train loss:  1.01067159e-01, mean val. rec. loss:  8.54471718e-02\n",
      "Epoch: 656 mean train loss:  1.00648766e-01, mean val. rec. loss:  8.51274723e-02\n",
      "Epoch: 657 mean train loss:  1.00238503e-01, mean val. rec. loss:  8.47862337e-02\n",
      "Epoch: 658 mean train loss:  9.98269641e-02, mean val. rec. loss:  8.44532478e-02\n",
      "Epoch: 659 mean train loss:  9.94134997e-02, mean val. rec. loss:  8.41254816e-02\n",
      "Epoch: 660 mean train loss:  9.90059262e-02, mean val. rec. loss:  8.38264931e-02\n",
      "Epoch: 661 mean train loss:  9.85866705e-02, mean val. rec. loss:  8.35114177e-02\n",
      "Epoch: 662 mean train loss:  9.81892532e-02, mean val. rec. loss:  8.31823396e-02\n",
      "Epoch: 663 mean train loss:  9.77753766e-02, mean val. rec. loss:  8.28648637e-02\n",
      "Epoch: 664 mean train loss:  9.73855567e-02, mean val. rec. loss:  8.25427916e-02\n",
      "Epoch: 665 mean train loss:  9.69835317e-02, mean val. rec. loss:  8.22616577e-02\n",
      "Epoch: 666 mean train loss:  9.65749231e-02, mean val. rec. loss:  8.19450471e-02\n",
      "Epoch: 667 mean train loss:  9.61691748e-02, mean val. rec. loss:  8.16338516e-02\n",
      "Epoch: 668 mean train loss:  9.57795150e-02, mean val. rec. loss:  8.13177807e-02\n",
      "Epoch: 669 mean train loss:  9.53839364e-02, mean val. rec. loss:  8.10183362e-02\n",
      "Epoch: 670 mean train loss:  9.49925666e-02, mean val. rec. loss:  8.07160726e-02\n",
      "Epoch: 671 mean train loss:  9.45999619e-02, mean val. rec. loss:  8.04142277e-02\n",
      "Epoch: 672 mean train loss:  9.42171473e-02, mean val. rec. loss:  8.01034416e-02\n",
      "Epoch: 673 mean train loss:  9.38197079e-02, mean val. rec. loss:  7.97999498e-02\n",
      "Epoch: 674 mean train loss:  9.34340508e-02, mean val. rec. loss:  7.95014358e-02\n",
      "Epoch: 675 mean train loss:  9.30436499e-02, mean val. rec. loss:  7.91978138e-02\n",
      "Epoch: 676 mean train loss:  9.26654423e-02, mean val. rec. loss:  7.89130978e-02\n",
      "Epoch: 677 mean train loss:  9.22747258e-02, mean val. rec. loss:  7.86165749e-02\n",
      "Epoch: 678 mean train loss:  9.18979115e-02, mean val. rec. loss:  7.83136507e-02\n",
      "Epoch: 679 mean train loss:  9.15194472e-02, mean val. rec. loss:  7.80074143e-02\n",
      "Epoch: 680 mean train loss:  9.11441705e-02, mean val. rec. loss:  7.77235822e-02\n",
      "Epoch: 681 mean train loss:  9.07673871e-02, mean val. rec. loss:  7.74526782e-02\n",
      "Epoch: 682 mean train loss:  9.03924062e-02, mean val. rec. loss:  7.71393194e-02\n",
      "Epoch: 683 mean train loss:  9.00163138e-02, mean val. rec. loss:  7.68684573e-02\n",
      "Epoch: 684 mean train loss:  8.96508231e-02, mean val. rec. loss:  7.65776099e-02\n",
      "Epoch: 685 mean train loss:  8.92818262e-02, mean val. rec. loss:  7.62824687e-02\n",
      "Epoch: 686 mean train loss:  8.89145108e-02, mean val. rec. loss:  7.60084757e-02\n",
      "Epoch: 687 mean train loss:  8.85449865e-02, mean val. rec. loss:  7.57543145e-02\n",
      "Epoch: 688 mean train loss:  8.81879309e-02, mean val. rec. loss:  7.54600153e-02\n",
      "Epoch: 689 mean train loss:  8.78093636e-02, mean val. rec. loss:  7.51750109e-02\n",
      "Epoch: 690 mean train loss:  8.74608515e-02, mean val. rec. loss:  7.48880898e-02\n",
      "Epoch: 691 mean train loss:  8.70859719e-02, mean val. rec. loss:  7.46215634e-02\n",
      "Epoch: 692 mean train loss:  8.67229201e-02, mean val. rec. loss:  7.43453514e-02\n",
      "Epoch: 693 mean train loss:  8.63763247e-02, mean val. rec. loss:  7.40699303e-02\n",
      "Epoch: 694 mean train loss:  8.60270796e-02, mean val. rec. loss:  7.38033667e-02\n",
      "Epoch: 695 mean train loss:  8.56669208e-02, mean val. rec. loss:  7.35396315e-02\n",
      "Epoch: 696 mean train loss:  8.53149032e-02, mean val. rec. loss:  7.32574695e-02\n",
      "Epoch: 697 mean train loss:  8.49654439e-02, mean val. rec. loss:  7.29460646e-02\n",
      "Epoch: 698 mean train loss:  8.46086800e-02, mean val. rec. loss:  7.26966207e-02\n",
      "Epoch: 699 mean train loss:  8.42683836e-02, mean val. rec. loss:  7.24620726e-02\n",
      "Epoch: 700 mean train loss:  8.39200148e-02, mean val. rec. loss:  7.21989981e-02\n",
      "Epoch: 701 mean train loss:  8.35733158e-02, mean val. rec. loss:  7.19150450e-02\n",
      "Epoch: 702 mean train loss:  8.32310817e-02, mean val. rec. loss:  7.16432711e-02\n",
      "Epoch: 703 mean train loss:  8.28875552e-02, mean val. rec. loss:  7.13699155e-02\n",
      "Epoch: 704 mean train loss:  8.25512034e-02, mean val. rec. loss:  7.11270309e-02\n",
      "Epoch: 705 mean train loss:  8.22087440e-02, mean val. rec. loss:  7.08543731e-02\n",
      "Epoch: 706 mean train loss:  8.18720411e-02, mean val. rec. loss:  7.06102604e-02\n",
      "Epoch: 707 mean train loss:  8.15311794e-02, mean val. rec. loss:  7.03593184e-02\n",
      "Epoch: 708 mean train loss:  8.12044266e-02, mean val. rec. loss:  7.01137450e-02\n",
      "Epoch: 709 mean train loss:  8.08734958e-02, mean val. rec. loss:  6.98398776e-02\n",
      "Epoch: 710 mean train loss:  8.05364796e-02, mean val. rec. loss:  6.95923084e-02\n",
      "Epoch: 711 mean train loss:  8.02056219e-02, mean val. rec. loss:  6.93485493e-02\n",
      "Epoch: 712 mean train loss:  7.98791739e-02, mean val. rec. loss:  6.90949510e-02\n",
      "Epoch: 713 mean train loss:  7.95530993e-02, mean val. rec. loss:  6.88103466e-02\n",
      "Epoch: 714 mean train loss:  7.92297318e-02, mean val. rec. loss:  6.85646336e-02\n",
      "Epoch: 715 mean train loss:  7.89012696e-02, mean val. rec. loss:  6.83060716e-02\n",
      "Epoch: 716 mean train loss:  7.85807370e-02, mean val. rec. loss:  6.80783946e-02\n",
      "Epoch: 717 mean train loss:  7.82607740e-02, mean val. rec. loss:  6.78591100e-02\n",
      "Epoch: 718 mean train loss:  7.79326212e-02, mean val. rec. loss:  6.76104476e-02\n",
      "Epoch: 719 mean train loss:  7.76313498e-02, mean val. rec. loss:  6.73378177e-02\n",
      "Epoch: 720 mean train loss:  7.72997713e-02, mean val. rec. loss:  6.71118108e-02\n",
      "Epoch: 721 mean train loss:  7.69835852e-02, mean val. rec. loss:  6.68494760e-02\n",
      "Epoch: 722 mean train loss:  7.66712813e-02, mean val. rec. loss:  6.66291121e-02\n",
      "Epoch: 723 mean train loss:  7.63631510e-02, mean val. rec. loss:  6.63738065e-02\n",
      "Epoch: 724 mean train loss:  7.60545492e-02, mean val. rec. loss:  6.61346389e-02\n",
      "Epoch: 725 mean train loss:  7.57397150e-02, mean val. rec. loss:  6.58998164e-02\n",
      "Epoch: 726 mean train loss:  7.54295848e-02, mean val. rec. loss:  6.56539452e-02\n",
      "Epoch: 727 mean train loss:  7.51236623e-02, mean val. rec. loss:  6.54374192e-02\n",
      "Epoch: 728 mean train loss:  7.48258411e-02, mean val. rec. loss:  6.52031829e-02\n",
      "Epoch: 729 mean train loss:  7.45188226e-02, mean val. rec. loss:  6.49585305e-02\n",
      "Epoch: 730 mean train loss:  7.42188539e-02, mean val. rec. loss:  6.47103472e-02\n",
      "Epoch: 731 mean train loss:  7.39169581e-02, mean val. rec. loss:  6.44822842e-02\n",
      "Epoch: 732 mean train loss:  7.36110729e-02, mean val. rec. loss:  6.42525603e-02\n",
      "Epoch: 733 mean train loss:  7.33138278e-02, mean val. rec. loss:  6.40231202e-02\n",
      "Epoch: 734 mean train loss:  7.30162738e-02, mean val. rec. loss:  6.38000535e-02\n",
      "Epoch: 735 mean train loss:  7.27168352e-02, mean val. rec. loss:  6.35586855e-02\n",
      "Epoch: 736 mean train loss:  7.24263389e-02, mean val. rec. loss:  6.33386379e-02\n",
      "Epoch: 737 mean train loss:  7.21400844e-02, mean val. rec. loss:  6.30973630e-02\n",
      "Epoch: 738 mean train loss:  7.18353656e-02, mean val. rec. loss:  6.28807161e-02\n",
      "Epoch: 739 mean train loss:  7.15488275e-02, mean val. rec. loss:  6.26877017e-02\n",
      "Epoch: 740 mean train loss:  7.12675280e-02, mean val. rec. loss:  6.24444264e-02\n",
      "Epoch: 741 mean train loss:  7.09690559e-02, mean val. rec. loss:  6.22326828e-02\n",
      "Epoch: 742 mean train loss:  7.06918039e-02, mean val. rec. loss:  6.20132865e-02\n",
      "Epoch: 743 mean train loss:  7.04095768e-02, mean val. rec. loss:  6.17919968e-02\n",
      "Epoch: 744 mean train loss:  7.01135841e-02, mean val. rec. loss:  6.15582536e-02\n",
      "Epoch: 745 mean train loss:  6.98349391e-02, mean val. rec. loss:  6.13195094e-02\n",
      "Epoch: 746 mean train loss:  6.95536815e-02, mean val. rec. loss:  6.11101197e-02\n",
      "Epoch: 747 mean train loss:  6.92695023e-02, mean val. rec. loss:  6.09066428e-02\n",
      "Epoch: 748 mean train loss:  6.89974495e-02, mean val. rec. loss:  6.06680614e-02\n",
      "Epoch: 749 mean train loss:  6.87153162e-02, mean val. rec. loss:  6.04655103e-02\n",
      "Epoch: 750 mean train loss:  6.84357810e-02, mean val. rec. loss:  6.02584513e-02\n",
      "Epoch: 751 mean train loss:  6.81623376e-02, mean val. rec. loss:  6.00475776e-02\n",
      "Epoch: 752 mean train loss:  6.78954954e-02, mean val. rec. loss:  5.98293956e-02\n",
      "Epoch: 753 mean train loss:  6.76210849e-02, mean val. rec. loss:  5.96199175e-02\n",
      "Epoch: 754 mean train loss:  6.73505034e-02, mean val. rec. loss:  5.93951481e-02\n",
      "Epoch: 755 mean train loss:  6.70813321e-02, mean val. rec. loss:  5.91895266e-02\n",
      "Epoch: 756 mean train loss:  6.68103081e-02, mean val. rec. loss:  5.89711026e-02\n",
      "Epoch: 757 mean train loss:  6.65393074e-02, mean val. rec. loss:  5.87546046e-02\n",
      "Epoch: 758 mean train loss:  6.62756409e-02, mean val. rec. loss:  5.85771142e-02\n",
      "Epoch: 759 mean train loss:  6.60125301e-02, mean val. rec. loss:  5.83777079e-02\n",
      "Epoch: 760 mean train loss:  6.57462264e-02, mean val. rec. loss:  5.81660806e-02\n",
      "Epoch: 761 mean train loss:  6.54887181e-02, mean val. rec. loss:  5.79514480e-02\n",
      "Epoch: 762 mean train loss:  6.52306106e-02, mean val. rec. loss:  5.77747066e-02\n",
      "Epoch: 763 mean train loss:  6.49562648e-02, mean val. rec. loss:  5.75283934e-02\n",
      "Epoch: 764 mean train loss:  6.47040409e-02, mean val. rec. loss:  5.73101416e-02\n",
      "Epoch: 765 mean train loss:  6.44393072e-02, mean val. rec. loss:  5.71199371e-02\n",
      "Epoch: 766 mean train loss:  6.41836811e-02, mean val. rec. loss:  5.69244199e-02\n",
      "Epoch: 767 mean train loss:  6.39229042e-02, mean val. rec. loss:  5.67353923e-02\n",
      "Epoch: 768 mean train loss:  6.36724529e-02, mean val. rec. loss:  5.65199643e-02\n",
      "Epoch: 769 mean train loss:  6.34234496e-02, mean val. rec. loss:  5.63209906e-02\n",
      "Epoch: 770 mean train loss:  6.31717494e-02, mean val. rec. loss:  5.61360196e-02\n",
      "Epoch: 771 mean train loss:  6.29217331e-02, mean val. rec. loss:  5.59422284e-02\n",
      "Epoch: 772 mean train loss:  6.26886127e-02, mean val. rec. loss:  5.57281029e-02\n",
      "Epoch: 773 mean train loss:  6.24162554e-02, mean val. rec. loss:  5.55484167e-02\n",
      "Epoch: 774 mean train loss:  6.21703999e-02, mean val. rec. loss:  5.53717916e-02\n",
      "Epoch: 775 mean train loss:  6.19263448e-02, mean val. rec. loss:  5.51710454e-02\n",
      "Epoch: 776 mean train loss:  6.16712011e-02, mean val. rec. loss:  5.49750165e-02\n",
      "Epoch: 777 mean train loss:  6.14315942e-02, mean val. rec. loss:  5.47578021e-02\n",
      "Epoch: 778 mean train loss:  6.11895243e-02, mean val. rec. loss:  5.45447233e-02\n",
      "Epoch: 779 mean train loss:  6.09479801e-02, mean val. rec. loss:  5.43378783e-02\n",
      "Epoch: 780 mean train loss:  6.07120749e-02, mean val. rec. loss:  5.41498510e-02\n",
      "Epoch: 781 mean train loss:  6.04773712e-02, mean val. rec. loss:  5.39846234e-02\n",
      "Epoch: 782 mean train loss:  6.02261502e-02, mean val. rec. loss:  5.37938281e-02\n",
      "Epoch: 783 mean train loss:  5.99858409e-02, mean val. rec. loss:  5.36255999e-02\n",
      "Epoch: 784 mean train loss:  5.97515534e-02, mean val. rec. loss:  5.34319296e-02\n",
      "Epoch: 785 mean train loss:  5.95101478e-02, mean val. rec. loss:  5.32433394e-02\n",
      "Epoch: 786 mean train loss:  5.92828260e-02, mean val. rec. loss:  5.30491387e-02\n",
      "Epoch: 787 mean train loss:  5.90525628e-02, mean val. rec. loss:  5.28745000e-02\n",
      "Epoch: 788 mean train loss:  5.88149361e-02, mean val. rec. loss:  5.26788014e-02\n",
      "Epoch: 789 mean train loss:  5.85825469e-02, mean val. rec. loss:  5.24797253e-02\n",
      "Epoch: 790 mean train loss:  5.83484898e-02, mean val. rec. loss:  5.23105156e-02\n",
      "Epoch: 791 mean train loss:  5.81274527e-02, mean val. rec. loss:  5.21181618e-02\n",
      "Epoch: 792 mean train loss:  5.78918437e-02, mean val. rec. loss:  5.19375406e-02\n",
      "Epoch: 793 mean train loss:  5.76692425e-02, mean val. rec. loss:  5.17688565e-02\n",
      "Epoch: 794 mean train loss:  5.74402751e-02, mean val. rec. loss:  5.15754607e-02\n",
      "Epoch: 795 mean train loss:  5.72154478e-02, mean val. rec. loss:  5.14121684e-02\n",
      "Epoch: 796 mean train loss:  5.69898159e-02, mean val. rec. loss:  5.12091381e-02\n",
      "Epoch: 797 mean train loss:  5.67667609e-02, mean val. rec. loss:  5.10437477e-02\n",
      "Epoch: 798 mean train loss:  5.65512728e-02, mean val. rec. loss:  5.08577672e-02\n",
      "Epoch: 799 mean train loss:  5.63218157e-02, mean val. rec. loss:  5.06609289e-02\n",
      "Epoch: 800 mean train loss:  5.61028324e-02, mean val. rec. loss:  5.04904026e-02\n",
      "Epoch: 801 mean train loss:  5.58806391e-02, mean val. rec. loss:  5.03166385e-02\n",
      "Epoch: 802 mean train loss:  5.56642101e-02, mean val. rec. loss:  5.01286483e-02\n",
      "Epoch: 803 mean train loss:  5.54472582e-02, mean val. rec. loss:  4.99751254e-02\n",
      "Epoch: 804 mean train loss:  5.52328259e-02, mean val. rec. loss:  4.97877633e-02\n",
      "Epoch: 805 mean train loss:  5.50119213e-02, mean val. rec. loss:  4.96036902e-02\n",
      "Epoch: 806 mean train loss:  5.48005800e-02, mean val. rec. loss:  4.94341129e-02\n",
      "Epoch: 807 mean train loss:  5.45849125e-02, mean val. rec. loss:  4.92488489e-02\n",
      "Epoch: 808 mean train loss:  5.43753283e-02, mean val. rec. loss:  4.90879896e-02\n",
      "Epoch: 809 mean train loss:  5.41619693e-02, mean val. rec. loss:  4.89342666e-02\n",
      "Epoch: 810 mean train loss:  5.39456223e-02, mean val. rec. loss:  4.87500214e-02\n",
      "Epoch: 811 mean train loss:  5.37414842e-02, mean val. rec. loss:  4.85672369e-02\n",
      "Epoch: 812 mean train loss:  5.35270757e-02, mean val. rec. loss:  4.84003020e-02\n",
      "Epoch: 813 mean train loss:  5.33224401e-02, mean val. rec. loss:  4.82346605e-02\n",
      "Epoch: 814 mean train loss:  5.31167675e-02, mean val. rec. loss:  4.80572817e-02\n",
      "Epoch: 815 mean train loss:  5.29127482e-02, mean val. rec. loss:  4.79104996e-02\n",
      "Epoch: 816 mean train loss:  5.26962752e-02, mean val. rec. loss:  4.77459931e-02\n",
      "Epoch: 817 mean train loss:  5.24944508e-02, mean val. rec. loss:  4.75666884e-02\n",
      "Epoch: 818 mean train loss:  5.23043308e-02, mean val. rec. loss:  4.73917287e-02\n",
      "Epoch: 819 mean train loss:  5.20943111e-02, mean val. rec. loss:  4.72367729e-02\n",
      "Epoch: 820 mean train loss:  5.18911089e-02, mean val. rec. loss:  4.70726153e-02\n",
      "Epoch: 821 mean train loss:  5.16892367e-02, mean val. rec. loss:  4.69138355e-02\n",
      "Epoch: 822 mean train loss:  5.14887681e-02, mean val. rec. loss:  4.67304602e-02\n",
      "Epoch: 823 mean train loss:  5.12814845e-02, mean val. rec. loss:  4.65687124e-02\n",
      "Epoch: 824 mean train loss:  5.10892009e-02, mean val. rec. loss:  4.63841741e-02\n",
      "Epoch: 825 mean train loss:  5.08931990e-02, mean val. rec. loss:  4.62254780e-02\n",
      "Epoch: 826 mean train loss:  5.06907776e-02, mean val. rec. loss:  4.60659492e-02\n",
      "Epoch: 827 mean train loss:  5.04964589e-02, mean val. rec. loss:  4.59184740e-02\n",
      "Epoch: 828 mean train loss:  5.03030793e-02, mean val. rec. loss:  4.57440446e-02\n",
      "Epoch: 829 mean train loss:  5.01061918e-02, mean val. rec. loss:  4.56090788e-02\n",
      "Epoch: 830 mean train loss:  4.99196134e-02, mean val. rec. loss:  4.54478427e-02\n",
      "Epoch: 831 mean train loss:  4.97236502e-02, mean val. rec. loss:  4.52760882e-02\n",
      "Epoch: 832 mean train loss:  4.95262242e-02, mean val. rec. loss:  4.51219233e-02\n",
      "Epoch: 833 mean train loss:  4.93375542e-02, mean val. rec. loss:  4.49520809e-02\n",
      "Epoch: 834 mean train loss:  4.91423362e-02, mean val. rec. loss:  4.47962691e-02\n",
      "Epoch: 835 mean train loss:  4.89633799e-02, mean val. rec. loss:  4.46427275e-02\n",
      "Epoch: 836 mean train loss:  4.87705249e-02, mean val. rec. loss:  4.44769696e-02\n",
      "Epoch: 837 mean train loss:  4.85796474e-02, mean val. rec. loss:  4.43241305e-02\n",
      "Epoch: 838 mean train loss:  4.83979754e-02, mean val. rec. loss:  4.41522691e-02\n",
      "Epoch: 839 mean train loss:  4.82036977e-02, mean val. rec. loss:  4.40080550e-02\n",
      "Epoch: 840 mean train loss:  4.80249228e-02, mean val. rec. loss:  4.38594260e-02\n",
      "Epoch: 841 mean train loss:  4.78349355e-02, mean val. rec. loss:  4.37282934e-02\n",
      "Epoch: 842 mean train loss:  4.76555166e-02, mean val. rec. loss:  4.35714629e-02\n",
      "Epoch: 843 mean train loss:  4.74649430e-02, mean val. rec. loss:  4.34218802e-02\n",
      "Epoch: 844 mean train loss:  4.72858086e-02, mean val. rec. loss:  4.32614443e-02\n",
      "Epoch: 845 mean train loss:  4.71049418e-02, mean val. rec. loss:  4.30851215e-02\n",
      "Epoch: 846 mean train loss:  4.69169515e-02, mean val. rec. loss:  4.29398141e-02\n",
      "Epoch: 847 mean train loss:  4.67510852e-02, mean val. rec. loss:  4.27901524e-02\n",
      "Epoch: 848 mean train loss:  4.65609198e-02, mean val. rec. loss:  4.26456405e-02\n",
      "Epoch: 849 mean train loss:  4.63847107e-02, mean val. rec. loss:  4.24987235e-02\n",
      "Epoch: 850 mean train loss:  4.62058935e-02, mean val. rec. loss:  4.23304860e-02\n",
      "Epoch: 851 mean train loss:  4.60266723e-02, mean val. rec. loss:  4.21630952e-02\n",
      "Epoch: 852 mean train loss:  4.58601156e-02, mean val. rec. loss:  4.20416390e-02\n",
      "Epoch: 853 mean train loss:  4.56747691e-02, mean val. rec. loss:  4.18890790e-02\n",
      "Epoch: 854 mean train loss:  4.55012327e-02, mean val. rec. loss:  4.17605889e-02\n",
      "Epoch: 855 mean train loss:  4.53299841e-02, mean val. rec. loss:  4.16113179e-02\n",
      "Epoch: 856 mean train loss:  4.51503557e-02, mean val. rec. loss:  4.14483791e-02\n",
      "Epoch: 857 mean train loss:  4.49766691e-02, mean val. rec. loss:  4.12918975e-02\n",
      "Epoch: 858 mean train loss:  4.48064627e-02, mean val. rec. loss:  4.11418124e-02\n",
      "Epoch: 859 mean train loss:  4.46321469e-02, mean val. rec. loss:  4.10057300e-02\n",
      "Epoch: 860 mean train loss:  4.44606515e-02, mean val. rec. loss:  4.08717132e-02\n",
      "Epoch: 861 mean train loss:  4.42978678e-02, mean val. rec. loss:  4.07509781e-02\n",
      "Epoch: 862 mean train loss:  4.41215983e-02, mean val. rec. loss:  4.05859691e-02\n",
      "Epoch: 863 mean train loss:  4.39572966e-02, mean val. rec. loss:  4.04242027e-02\n",
      "Epoch: 864 mean train loss:  4.37878980e-02, mean val. rec. loss:  4.02863386e-02\n",
      "Epoch: 865 mean train loss:  4.36232460e-02, mean val. rec. loss:  4.01481303e-02\n",
      "Epoch: 866 mean train loss:  4.34543431e-02, mean val. rec. loss:  4.00060375e-02\n",
      "Epoch: 867 mean train loss:  4.32839891e-02, mean val. rec. loss:  3.98723416e-02\n",
      "Epoch: 868 mean train loss:  4.31253555e-02, mean val. rec. loss:  3.97331982e-02\n",
      "Epoch: 869 mean train loss:  4.29547805e-02, mean val. rec. loss:  3.96017214e-02\n",
      "Epoch: 870 mean train loss:  4.27964464e-02, mean val. rec. loss:  3.94458492e-02\n",
      "Epoch: 871 mean train loss:  4.26312596e-02, mean val. rec. loss:  3.92970201e-02\n",
      "Epoch: 872 mean train loss:  4.24660015e-02, mean val. rec. loss:  3.91735589e-02\n",
      "Epoch: 873 mean train loss:  4.22969368e-02, mean val. rec. loss:  3.90332013e-02\n",
      "Epoch: 874 mean train loss:  4.21413269e-02, mean val. rec. loss:  3.89070557e-02\n",
      "Epoch: 875 mean train loss:  4.19781950e-02, mean val. rec. loss:  3.87616716e-02\n",
      "Epoch: 876 mean train loss:  4.18140213e-02, mean val. rec. loss:  3.86273012e-02\n",
      "Epoch: 877 mean train loss:  4.16596049e-02, mean val. rec. loss:  3.84753344e-02\n",
      "Epoch: 878 mean train loss:  4.15005987e-02, mean val. rec. loss:  3.83320622e-02\n",
      "Epoch: 879 mean train loss:  4.13488428e-02, mean val. rec. loss:  3.81855895e-02\n",
      "Epoch: 880 mean train loss:  4.11790928e-02, mean val. rec. loss:  3.80596463e-02\n",
      "Epoch: 881 mean train loss:  4.10198935e-02, mean val. rec. loss:  3.79273019e-02\n",
      "Epoch: 882 mean train loss:  4.08660975e-02, mean val. rec. loss:  3.78093161e-02\n",
      "Epoch: 883 mean train loss:  4.07142827e-02, mean val. rec. loss:  3.76949403e-02\n",
      "Epoch: 884 mean train loss:  4.05572714e-02, mean val. rec. loss:  3.75406684e-02\n",
      "Epoch: 885 mean train loss:  4.04072606e-02, mean val. rec. loss:  3.74147183e-02\n",
      "Epoch: 886 mean train loss:  4.02426993e-02, mean val. rec. loss:  3.72699110e-02\n",
      "Epoch: 887 mean train loss:  4.00875153e-02, mean val. rec. loss:  3.71321166e-02\n",
      "Epoch: 888 mean train loss:  3.99413892e-02, mean val. rec. loss:  3.70031101e-02\n",
      "Epoch: 889 mean train loss:  3.97855422e-02, mean val. rec. loss:  3.68812491e-02\n",
      "Epoch: 890 mean train loss:  3.96363788e-02, mean val. rec. loss:  3.67631354e-02\n",
      "Epoch: 891 mean train loss:  3.94871433e-02, mean val. rec. loss:  3.66194934e-02\n",
      "Epoch: 892 mean train loss:  3.93317992e-02, mean val. rec. loss:  3.64777309e-02\n",
      "Epoch: 893 mean train loss:  3.91823569e-02, mean val. rec. loss:  3.63408531e-02\n",
      "Epoch: 894 mean train loss:  3.90303639e-02, mean val. rec. loss:  3.62242699e-02\n",
      "Epoch: 895 mean train loss:  3.88820561e-02, mean val. rec. loss:  3.60989175e-02\n",
      "Epoch: 896 mean train loss:  3.87374665e-02, mean val. rec. loss:  3.59592345e-02\n",
      "Epoch: 897 mean train loss:  3.85859289e-02, mean val. rec. loss:  3.58249920e-02\n",
      "Epoch: 898 mean train loss:  3.84392241e-02, mean val. rec. loss:  3.57060991e-02\n",
      "Epoch: 899 mean train loss:  3.82977839e-02, mean val. rec. loss:  3.55945331e-02\n",
      "Epoch: 900 mean train loss:  3.81492055e-02, mean val. rec. loss:  3.54760845e-02\n",
      "Epoch: 901 mean train loss:  3.79961474e-02, mean val. rec. loss:  3.53451217e-02\n",
      "Epoch: 902 mean train loss:  3.78593237e-02, mean val. rec. loss:  3.51953274e-02\n",
      "Epoch: 903 mean train loss:  3.77097970e-02, mean val. rec. loss:  3.50752249e-02\n",
      "Epoch: 904 mean train loss:  3.75635213e-02, mean val. rec. loss:  3.49481955e-02\n",
      "Epoch: 905 mean train loss:  3.74242351e-02, mean val. rec. loss:  3.48300794e-02\n",
      "Epoch: 906 mean train loss:  3.72787385e-02, mean val. rec. loss:  3.47077672e-02\n",
      "Epoch: 907 mean train loss:  3.71366886e-02, mean val. rec. loss:  3.45935612e-02\n",
      "Epoch: 908 mean train loss:  3.69938698e-02, mean val. rec. loss:  3.44605260e-02\n",
      "Epoch: 909 mean train loss:  3.68524832e-02, mean val. rec. loss:  3.43329523e-02\n",
      "Epoch: 910 mean train loss:  3.67166685e-02, mean val. rec. loss:  3.42077279e-02\n",
      "Epoch: 911 mean train loss:  3.65676362e-02, mean val. rec. loss:  3.40930101e-02\n",
      "Epoch: 912 mean train loss:  3.64290821e-02, mean val. rec. loss:  3.39807603e-02\n",
      "Epoch: 913 mean train loss:  3.62968646e-02, mean val. rec. loss:  3.38544008e-02\n",
      "Epoch: 914 mean train loss:  3.61520359e-02, mean val. rec. loss:  3.37147876e-02\n",
      "Epoch: 915 mean train loss:  3.60150104e-02, mean val. rec. loss:  3.35840318e-02\n",
      "Epoch: 916 mean train loss:  3.58778991e-02, mean val. rec. loss:  3.34606496e-02\n",
      "Epoch: 917 mean train loss:  3.57426868e-02, mean val. rec. loss:  3.33619978e-02\n",
      "Epoch: 918 mean train loss:  3.56053792e-02, mean val. rec. loss:  3.32516531e-02\n",
      "Epoch: 919 mean train loss:  3.54678066e-02, mean val. rec. loss:  3.31114420e-02\n",
      "Epoch: 920 mean train loss:  3.53317194e-02, mean val. rec. loss:  3.30135229e-02\n",
      "Epoch: 921 mean train loss:  3.51949553e-02, mean val. rec. loss:  3.28961163e-02\n",
      "Epoch: 922 mean train loss:  3.50592115e-02, mean val. rec. loss:  3.27783259e-02\n",
      "Epoch: 923 mean train loss:  3.49273543e-02, mean val. rec. loss:  3.26599935e-02\n",
      "Epoch: 924 mean train loss:  3.47986969e-02, mean val. rec. loss:  3.25508443e-02\n",
      "Epoch: 925 mean train loss:  3.46583476e-02, mean val. rec. loss:  3.24216168e-02\n",
      "Epoch: 926 mean train loss:  3.45250093e-02, mean val. rec. loss:  3.23078760e-02\n",
      "Epoch: 927 mean train loss:  3.44034487e-02, mean val. rec. loss:  3.21688722e-02\n",
      "Epoch: 928 mean train loss:  3.42751530e-02, mean val. rec. loss:  3.20783429e-02\n",
      "Epoch: 929 mean train loss:  3.41314926e-02, mean val. rec. loss:  3.19443586e-02\n",
      "Epoch: 930 mean train loss:  3.40037844e-02, mean val. rec. loss:  3.18476793e-02\n",
      "Epoch: 931 mean train loss:  3.38761185e-02, mean val. rec. loss:  3.17393326e-02\n",
      "Epoch: 932 mean train loss:  3.37412456e-02, mean val. rec. loss:  3.16160062e-02\n",
      "Epoch: 933 mean train loss:  3.36177741e-02, mean val. rec. loss:  3.15082434e-02\n",
      "Epoch: 934 mean train loss:  3.34852459e-02, mean val. rec. loss:  3.13882270e-02\n",
      "Epoch: 935 mean train loss:  3.33588848e-02, mean val. rec. loss:  3.12703877e-02\n",
      "Epoch: 936 mean train loss:  3.32339616e-02, mean val. rec. loss:  3.11648718e-02\n",
      "Epoch: 937 mean train loss:  3.31062196e-02, mean val. rec. loss:  3.10453275e-02\n",
      "Epoch: 938 mean train loss:  3.29722092e-02, mean val. rec. loss:  3.09316100e-02\n",
      "Epoch: 939 mean train loss:  3.28489578e-02, mean val. rec. loss:  3.08242984e-02\n",
      "Epoch: 940 mean train loss:  3.27289682e-02, mean val. rec. loss:  3.07095388e-02\n",
      "Epoch: 941 mean train loss:  3.26004743e-02, mean val. rec. loss:  3.06127734e-02\n",
      "Epoch: 942 mean train loss:  3.24779008e-02, mean val. rec. loss:  3.05009191e-02\n",
      "Epoch: 943 mean train loss:  3.23510362e-02, mean val. rec. loss:  3.03956427e-02\n",
      "Epoch: 944 mean train loss:  3.22317453e-02, mean val. rec. loss:  3.02647009e-02\n",
      "Epoch: 945 mean train loss:  3.21009737e-02, mean val. rec. loss:  3.01660794e-02\n",
      "Epoch: 946 mean train loss:  3.19791757e-02, mean val. rec. loss:  3.00617544e-02\n",
      "Epoch: 947 mean train loss:  3.18564102e-02, mean val. rec. loss:  2.99538543e-02\n",
      "Epoch: 948 mean train loss:  3.17373855e-02, mean val. rec. loss:  2.98417719e-02\n",
      "Epoch: 949 mean train loss:  3.16189471e-02, mean val. rec. loss:  2.97448507e-02\n",
      "Epoch: 950 mean train loss:  3.14926446e-02, mean val. rec. loss:  2.96379020e-02\n",
      "Epoch: 951 mean train loss:  3.13775490e-02, mean val. rec. loss:  2.95284597e-02\n",
      "Epoch: 952 mean train loss:  3.12535073e-02, mean val. rec. loss:  2.94121021e-02\n",
      "Epoch: 953 mean train loss:  3.11442109e-02, mean val. rec. loss:  2.92929812e-02\n",
      "Epoch: 954 mean train loss:  3.10158955e-02, mean val. rec. loss:  2.92053967e-02\n",
      "Epoch: 955 mean train loss:  3.08950346e-02, mean val. rec. loss:  2.91094152e-02\n",
      "Epoch: 956 mean train loss:  3.07812324e-02, mean val. rec. loss:  2.89974910e-02\n",
      "Epoch: 957 mean train loss:  3.06596570e-02, mean val. rec. loss:  2.89008210e-02\n",
      "Epoch: 958 mean train loss:  3.05442713e-02, mean val. rec. loss:  2.88014854e-02\n",
      "Epoch: 959 mean train loss:  3.04269679e-02, mean val. rec. loss:  2.87119005e-02\n",
      "Epoch: 960 mean train loss:  3.03131448e-02, mean val. rec. loss:  2.86015534e-02\n",
      "Epoch: 961 mean train loss:  3.01946553e-02, mean val. rec. loss:  2.84811950e-02\n",
      "Epoch: 962 mean train loss:  3.00788821e-02, mean val. rec. loss:  2.83686335e-02\n",
      "Epoch: 963 mean train loss:  2.99662487e-02, mean val. rec. loss:  2.82663415e-02\n",
      "Epoch: 964 mean train loss:  2.98531379e-02, mean val. rec. loss:  2.81730745e-02\n",
      "Epoch: 965 mean train loss:  2.97405548e-02, mean val. rec. loss:  2.80862017e-02\n",
      "Epoch: 966 mean train loss:  2.96298615e-02, mean val. rec. loss:  2.79818884e-02\n",
      "Epoch: 967 mean train loss:  2.95139245e-02, mean val. rec. loss:  2.78806082e-02\n",
      "Epoch: 968 mean train loss:  2.93995563e-02, mean val. rec. loss:  2.77574423e-02\n",
      "Epoch: 969 mean train loss:  2.92855712e-02, mean val. rec. loss:  2.76567738e-02\n",
      "Epoch: 970 mean train loss:  2.91797995e-02, mean val. rec. loss:  2.75485992e-02\n",
      "Epoch: 971 mean train loss:  2.90684400e-02, mean val. rec. loss:  2.74516617e-02\n",
      "Epoch: 972 mean train loss:  2.89523214e-02, mean val. rec. loss:  2.73847557e-02\n",
      "Epoch: 973 mean train loss:  2.88414775e-02, mean val. rec. loss:  2.72853479e-02\n",
      "Epoch: 974 mean train loss:  2.87352451e-02, mean val. rec. loss:  2.71859076e-02\n",
      "Epoch: 975 mean train loss:  2.86240207e-02, mean val. rec. loss:  2.70838179e-02\n",
      "Epoch: 976 mean train loss:  2.85163461e-02, mean val. rec. loss:  2.69965149e-02\n",
      "Epoch: 977 mean train loss:  2.84077504e-02, mean val. rec. loss:  2.68899476e-02\n",
      "Epoch: 978 mean train loss:  2.83006970e-02, mean val. rec. loss:  2.67845317e-02\n",
      "Epoch: 979 mean train loss:  2.81925769e-02, mean val. rec. loss:  2.67043230e-02\n",
      "Epoch: 980 mean train loss:  2.80875668e-02, mean val. rec. loss:  2.65897867e-02\n",
      "Epoch: 981 mean train loss:  2.79766956e-02, mean val. rec. loss:  2.64995203e-02\n",
      "Epoch: 982 mean train loss:  2.78702019e-02, mean val. rec. loss:  2.64128243e-02\n",
      "Epoch: 983 mean train loss:  2.77641006e-02, mean val. rec. loss:  2.63096460e-02\n",
      "Epoch: 984 mean train loss:  2.76611039e-02, mean val. rec. loss:  2.62196425e-02\n",
      "Epoch: 985 mean train loss:  2.75544705e-02, mean val. rec. loss:  2.61214303e-02\n",
      "Epoch: 986 mean train loss:  2.74509208e-02, mean val. rec. loss:  2.60217550e-02\n",
      "Epoch: 987 mean train loss:  2.73432801e-02, mean val. rec. loss:  2.59435979e-02\n",
      "Epoch: 988 mean train loss:  2.72436771e-02, mean val. rec. loss:  2.58422549e-02\n",
      "Epoch: 989 mean train loss:  2.71404939e-02, mean val. rec. loss:  2.57667588e-02\n",
      "Epoch: 990 mean train loss:  2.70387655e-02, mean val. rec. loss:  2.56829262e-02\n",
      "Epoch: 991 mean train loss:  2.69312805e-02, mean val. rec. loss:  2.55761309e-02\n",
      "Epoch: 992 mean train loss:  2.68392176e-02, mean val. rec. loss:  2.54748740e-02\n",
      "Epoch: 993 mean train loss:  2.67321991e-02, mean val. rec. loss:  2.53809673e-02\n",
      "Epoch: 994 mean train loss:  2.66300785e-02, mean val. rec. loss:  2.52941783e-02\n",
      "Epoch: 995 mean train loss:  2.65288443e-02, mean val. rec. loss:  2.51899905e-02\n",
      "Epoch: 996 mean train loss:  2.64304662e-02, mean val. rec. loss:  2.51120428e-02\n",
      "Epoch: 997 mean train loss:  2.63300286e-02, mean val. rec. loss:  2.50290103e-02\n",
      "Epoch: 998 mean train loss:  2.62308598e-02, mean val. rec. loss:  2.49423446e-02\n",
      "Epoch: 999 mean train loss:  2.61305654e-02, mean val. rec. loss:  2.48517711e-02\n",
      "Epoch: 1000 mean train loss:  2.60372492e-02, mean val. rec. loss:  2.47509700e-02\n",
      "Epoch: 1001 mean train loss:  2.59375007e-02, mean val. rec. loss:  2.46798981e-02\n",
      "Epoch: 1002 mean train loss:  2.58383847e-02, mean val. rec. loss:  2.45945442e-02\n",
      "Epoch: 1003 mean train loss:  2.57422324e-02, mean val. rec. loss:  2.44805685e-02\n",
      "Epoch: 1004 mean train loss:  2.56435726e-02, mean val. rec. loss:  2.43973709e-02\n",
      "Epoch: 1005 mean train loss:  2.55525616e-02, mean val. rec. loss:  2.43316185e-02\n",
      "Epoch: 1006 mean train loss:  2.54573502e-02, mean val. rec. loss:  2.42350788e-02\n",
      "Epoch: 1007 mean train loss:  2.53596629e-02, mean val. rec. loss:  2.41329751e-02\n",
      "Epoch: 1008 mean train loss:  2.52640051e-02, mean val. rec. loss:  2.40435670e-02\n",
      "Epoch: 1009 mean train loss:  2.51673690e-02, mean val. rec. loss:  2.39711226e-02\n",
      "Epoch: 1010 mean train loss:  2.50759000e-02, mean val. rec. loss:  2.39129229e-02\n",
      "Epoch: 1011 mean train loss:  2.49898249e-02, mean val. rec. loss:  2.38176648e-02\n",
      "Epoch: 1012 mean train loss:  2.48896338e-02, mean val. rec. loss:  2.37259911e-02\n",
      "Epoch: 1013 mean train loss:  2.47946932e-02, mean val. rec. loss:  2.36388858e-02\n",
      "Epoch: 1014 mean train loss:  2.47038612e-02, mean val. rec. loss:  2.35466469e-02\n",
      "Epoch: 1015 mean train loss:  2.46114878e-02, mean val. rec. loss:  2.34712903e-02\n",
      "Epoch: 1016 mean train loss:  2.45179657e-02, mean val. rec. loss:  2.33942427e-02\n",
      "Epoch: 1017 mean train loss:  2.44233103e-02, mean val. rec. loss:  2.33068978e-02\n",
      "Epoch: 1018 mean train loss:  2.43428184e-02, mean val. rec. loss:  2.32124026e-02\n",
      "Epoch: 1019 mean train loss:  2.42461925e-02, mean val. rec. loss:  2.31389488e-02\n",
      "Epoch: 1020 mean train loss:  2.41567746e-02, mean val. rec. loss:  2.30633154e-02\n",
      "Epoch: 1021 mean train loss:  2.40671445e-02, mean val. rec. loss:  2.29950416e-02\n",
      "Epoch: 1022 mean train loss:  2.39786977e-02, mean val. rec. loss:  2.28970411e-02\n",
      "Epoch: 1023 mean train loss:  2.38914260e-02, mean val. rec. loss:  2.28257482e-02\n",
      "Epoch: 1024 mean train loss:  2.38032650e-02, mean val. rec. loss:  2.27207416e-02\n",
      "Epoch: 1025 mean train loss:  2.37103888e-02, mean val. rec. loss:  2.26584179e-02\n",
      "Epoch: 1026 mean train loss:  2.36254926e-02, mean val. rec. loss:  2.25921259e-02\n",
      "Epoch: 1027 mean train loss:  2.35390034e-02, mean val. rec. loss:  2.24949883e-02\n",
      "Epoch: 1028 mean train loss:  2.34486809e-02, mean val. rec. loss:  2.24118861e-02\n",
      "Epoch: 1029 mean train loss:  2.33595178e-02, mean val. rec. loss:  2.23349129e-02\n",
      "Epoch: 1030 mean train loss:  2.32799155e-02, mean val. rec. loss:  2.22630896e-02\n",
      "Epoch: 1031 mean train loss:  2.31911746e-02, mean val. rec. loss:  2.21923502e-02\n",
      "Epoch: 1032 mean train loss:  2.31064078e-02, mean val. rec. loss:  2.21092340e-02\n",
      "Epoch: 1033 mean train loss:  2.30222875e-02, mean val. rec. loss:  2.20350149e-02\n",
      "Epoch: 1034 mean train loss:  2.29353762e-02, mean val. rec. loss:  2.19450183e-02\n",
      "Epoch: 1035 mean train loss:  2.28541159e-02, mean val. rec. loss:  2.18849554e-02\n",
      "Epoch: 1036 mean train loss:  2.27693973e-02, mean val. rec. loss:  2.18137741e-02\n",
      "Epoch: 1037 mean train loss:  2.26842172e-02, mean val. rec. loss:  2.17319419e-02\n",
      "Epoch: 1038 mean train loss:  2.26072264e-02, mean val. rec. loss:  2.16436572e-02\n",
      "Epoch: 1039 mean train loss:  2.25206245e-02, mean val. rec. loss:  2.15707081e-02\n",
      "Epoch: 1040 mean train loss:  2.24371064e-02, mean val. rec. loss:  2.15067584e-02\n",
      "Epoch: 1041 mean train loss:  2.23583813e-02, mean val. rec. loss:  2.14126750e-02\n",
      "Epoch: 1042 mean train loss:  2.22790198e-02, mean val. rec. loss:  2.13561733e-02\n",
      "Epoch: 1043 mean train loss:  2.21955287e-02, mean val. rec. loss:  2.12948822e-02\n",
      "Epoch: 1044 mean train loss:  2.21128848e-02, mean val. rec. loss:  2.12095470e-02\n",
      "Epoch: 1045 mean train loss:  2.20323539e-02, mean val. rec. loss:  2.11318784e-02\n",
      "Epoch: 1046 mean train loss:  2.19581600e-02, mean val. rec. loss:  2.10761419e-02\n",
      "Epoch: 1047 mean train loss:  2.18727880e-02, mean val. rec. loss:  2.09956216e-02\n",
      "Epoch: 1048 mean train loss:  2.18016670e-02, mean val. rec. loss:  2.09076021e-02\n",
      "Epoch: 1049 mean train loss:  2.17182913e-02, mean val. rec. loss:  2.08299311e-02\n",
      "Epoch: 1050 mean train loss:  2.16383273e-02, mean val. rec. loss:  2.07640183e-02\n",
      "Epoch: 1051 mean train loss:  2.15659529e-02, mean val. rec. loss:  2.07047765e-02\n",
      "Epoch: 1052 mean train loss:  2.14883513e-02, mean val. rec. loss:  2.06552901e-02\n",
      "Epoch: 1053 mean train loss:  2.14055024e-02, mean val. rec. loss:  2.05668473e-02\n",
      "Epoch: 1054 mean train loss:  2.13351590e-02, mean val. rec. loss:  2.04813515e-02\n",
      "Epoch: 1055 mean train loss:  2.12550350e-02, mean val. rec. loss:  2.04179973e-02\n",
      "Epoch: 1056 mean train loss:  2.11806217e-02, mean val. rec. loss:  2.03674549e-02\n",
      "Epoch: 1057 mean train loss:  2.11031549e-02, mean val. rec. loss:  2.02808845e-02\n",
      "Epoch: 1058 mean train loss:  2.10264204e-02, mean val. rec. loss:  2.02106290e-02\n",
      "Epoch: 1059 mean train loss:  2.09520550e-02, mean val. rec. loss:  2.01309716e-02\n",
      "Epoch: 1060 mean train loss:  2.08863453e-02, mean val. rec. loss:  2.00876608e-02\n",
      "Epoch: 1061 mean train loss:  2.08025034e-02, mean val. rec. loss:  2.00154095e-02\n",
      "Epoch: 1062 mean train loss:  2.07315468e-02, mean val. rec. loss:  1.99379526e-02\n",
      "Epoch: 1063 mean train loss:  2.06590293e-02, mean val. rec. loss:  1.98878777e-02\n",
      "Epoch: 1064 mean train loss:  2.05866279e-02, mean val. rec. loss:  1.98135399e-02\n",
      "Epoch: 1065 mean train loss:  2.05154103e-02, mean val. rec. loss:  1.97490134e-02\n",
      "Epoch: 1066 mean train loss:  2.04403173e-02, mean val. rec. loss:  1.96763643e-02\n",
      "Epoch: 1067 mean train loss:  2.03679694e-02, mean val. rec. loss:  1.96023011e-02\n",
      "Epoch: 1068 mean train loss:  2.02970582e-02, mean val. rec. loss:  1.95475671e-02\n",
      "Epoch: 1069 mean train loss:  2.02248679e-02, mean val. rec. loss:  1.94669258e-02\n",
      "Epoch: 1070 mean train loss:  2.01527842e-02, mean val. rec. loss:  1.93968331e-02\n",
      "Epoch: 1071 mean train loss:  2.00828714e-02, mean val. rec. loss:  1.93532025e-02\n",
      "Epoch: 1072 mean train loss:  2.00152175e-02, mean val. rec. loss:  1.92766190e-02\n",
      "Epoch: 1073 mean train loss:  1.99428816e-02, mean val. rec. loss:  1.92173434e-02\n",
      "Epoch: 1074 mean train loss:  1.98771203e-02, mean val. rec. loss:  1.91737059e-02\n",
      "Epoch: 1075 mean train loss:  1.98093734e-02, mean val. rec. loss:  1.91144513e-02\n",
      "Epoch: 1076 mean train loss:  1.97339219e-02, mean val. rec. loss:  1.90473603e-02\n",
      "Epoch: 1077 mean train loss:  1.96693962e-02, mean val. rec. loss:  1.89728772e-02\n",
      "Epoch: 1078 mean train loss:  1.96039404e-02, mean val. rec. loss:  1.89226197e-02\n",
      "Epoch: 1079 mean train loss:  1.95331376e-02, mean val. rec. loss:  1.88399943e-02\n",
      "Epoch: 1080 mean train loss:  1.94657961e-02, mean val. rec. loss:  1.87808176e-02\n",
      "Epoch: 1081 mean train loss:  1.93973848e-02, mean val. rec. loss:  1.87314720e-02\n",
      "Epoch: 1082 mean train loss:  1.93325772e-02, mean val. rec. loss:  1.86567493e-02\n",
      "Epoch: 1083 mean train loss:  1.92641628e-02, mean val. rec. loss:  1.85805553e-02\n",
      "Epoch: 1084 mean train loss:  1.91959440e-02, mean val. rec. loss:  1.85295140e-02\n",
      "Epoch: 1085 mean train loss:  1.91323192e-02, mean val. rec. loss:  1.84720970e-02\n",
      "Epoch: 1086 mean train loss:  1.90703910e-02, mean val. rec. loss:  1.84112874e-02\n",
      "Epoch: 1087 mean train loss:  1.90025884e-02, mean val. rec. loss:  1.83582864e-02\n",
      "Epoch: 1088 mean train loss:  1.89374436e-02, mean val. rec. loss:  1.83113110e-02\n",
      "Epoch: 1089 mean train loss:  1.88741384e-02, mean val. rec. loss:  1.82547755e-02\n",
      "Epoch: 1090 mean train loss:  1.88071888e-02, mean val. rec. loss:  1.81932426e-02\n",
      "Epoch: 1091 mean train loss:  1.87467845e-02, mean val. rec. loss:  1.81070560e-02\n",
      "Epoch: 1092 mean train loss:  1.86827053e-02, mean val. rec. loss:  1.80493436e-02\n",
      "Epoch: 1093 mean train loss:  1.86204245e-02, mean val. rec. loss:  1.80002026e-02\n",
      "Epoch: 1094 mean train loss:  1.85546197e-02, mean val. rec. loss:  1.79375486e-02\n",
      "Epoch: 1095 mean train loss:  1.84941865e-02, mean val. rec. loss:  1.78913000e-02\n",
      "Epoch: 1096 mean train loss:  1.84332576e-02, mean val. rec. loss:  1.78260582e-02\n",
      "Epoch: 1097 mean train loss:  1.83750747e-02, mean val. rec. loss:  1.77881822e-02\n",
      "Epoch: 1098 mean train loss:  1.83088212e-02, mean val. rec. loss:  1.77165183e-02\n",
      "Epoch: 1099 mean train loss:  1.82456961e-02, mean val. rec. loss:  1.76609993e-02\n",
      "Epoch: 1100 mean train loss:  1.81834729e-02, mean val. rec. loss:  1.76116734e-02\n",
      "Epoch: 1101 mean train loss:  1.81248584e-02, mean val. rec. loss:  1.75510313e-02\n",
      "Epoch: 1102 mean train loss:  1.80658852e-02, mean val. rec. loss:  1.74894926e-02\n",
      "Epoch: 1103 mean train loss:  1.80059028e-02, mean val. rec. loss:  1.74312638e-02\n",
      "Epoch: 1104 mean train loss:  1.79475153e-02, mean val. rec. loss:  1.73756088e-02\n",
      "Epoch: 1105 mean train loss:  1.78966452e-02, mean val. rec. loss:  1.73537673e-02\n",
      "Epoch: 1106 mean train loss:  1.78291942e-02, mean val. rec. loss:  1.72876346e-02\n",
      "Epoch: 1107 mean train loss:  1.77698603e-02, mean val. rec. loss:  1.72272636e-02\n",
      "Epoch: 1108 mean train loss:  1.77118276e-02, mean val. rec. loss:  1.71547680e-02\n",
      "Epoch: 1109 mean train loss:  1.76521222e-02, mean val. rec. loss:  1.71087405e-02\n",
      "Epoch: 1110 mean train loss:  1.75947108e-02, mean val. rec. loss:  1.70676627e-02\n",
      "Epoch: 1111 mean train loss:  1.75417291e-02, mean val. rec. loss:  1.70193928e-02\n",
      "Epoch: 1112 mean train loss:  1.74807084e-02, mean val. rec. loss:  1.69569248e-02\n",
      "Epoch: 1113 mean train loss:  1.74250375e-02, mean val. rec. loss:  1.69002254e-02\n",
      "Epoch: 1114 mean train loss:  1.73687921e-02, mean val. rec. loss:  1.68367618e-02\n",
      "Epoch: 1115 mean train loss:  1.73119377e-02, mean val. rec. loss:  1.68008246e-02\n",
      "Epoch: 1116 mean train loss:  1.72542016e-02, mean val. rec. loss:  1.67596677e-02\n",
      "Epoch: 1117 mean train loss:  1.71981928e-02, mean val. rec. loss:  1.67046314e-02\n",
      "Epoch: 1118 mean train loss:  1.71444291e-02, mean val. rec. loss:  1.66516920e-02\n",
      "Epoch: 1119 mean train loss:  1.70915443e-02, mean val. rec. loss:  1.65843591e-02\n",
      "Epoch: 1120 mean train loss:  1.70309891e-02, mean val. rec. loss:  1.65288297e-02\n",
      "Epoch: 1121 mean train loss:  1.69806830e-02, mean val. rec. loss:  1.64900663e-02\n",
      "Epoch: 1122 mean train loss:  1.69278884e-02, mean val. rec. loss:  1.64370455e-02\n",
      "Epoch: 1123 mean train loss:  1.68749994e-02, mean val. rec. loss:  1.64020957e-02\n",
      "Epoch: 1124 mean train loss:  1.68166738e-02, mean val. rec. loss:  1.63402627e-02\n",
      "Epoch: 1125 mean train loss:  1.67672599e-02, mean val. rec. loss:  1.62932001e-02\n",
      "Epoch: 1126 mean train loss:  1.67134066e-02, mean val. rec. loss:  1.62567174e-02\n",
      "Epoch: 1127 mean train loss:  1.66566688e-02, mean val. rec. loss:  1.62021497e-02\n",
      "Epoch: 1128 mean train loss:  1.66083541e-02, mean val. rec. loss:  1.61458899e-02\n",
      "Epoch: 1129 mean train loss:  1.65559345e-02, mean val. rec. loss:  1.60995169e-02\n",
      "Epoch: 1130 mean train loss:  1.65022903e-02, mean val. rec. loss:  1.60410707e-02\n",
      "Epoch: 1131 mean train loss:  1.64473769e-02, mean val. rec. loss:  1.60009384e-02\n",
      "Epoch: 1132 mean train loss:  1.63987245e-02, mean val. rec. loss:  1.59532059e-02\n",
      "Epoch: 1133 mean train loss:  1.63483145e-02, mean val. rec. loss:  1.58913008e-02\n",
      "Epoch: 1134 mean train loss:  1.62958250e-02, mean val. rec. loss:  1.58384474e-02\n",
      "Epoch: 1135 mean train loss:  1.62495450e-02, mean val. rec. loss:  1.58094778e-02\n",
      "Epoch: 1136 mean train loss:  1.61976277e-02, mean val. rec. loss:  1.57647028e-02\n",
      "Epoch: 1137 mean train loss:  1.61470197e-02, mean val. rec. loss:  1.57365950e-02\n",
      "Epoch: 1138 mean train loss:  1.60967900e-02, mean val. rec. loss:  1.56862375e-02\n",
      "Epoch: 1139 mean train loss:  1.60465302e-02, mean val. rec. loss:  1.56374257e-02\n",
      "Epoch: 1140 mean train loss:  1.59981931e-02, mean val. rec. loss:  1.55804797e-02\n",
      "Epoch: 1141 mean train loss:  1.59512665e-02, mean val. rec. loss:  1.55259923e-02\n",
      "Epoch: 1142 mean train loss:  1.59009299e-02, mean val. rec. loss:  1.54984242e-02\n",
      "Epoch: 1143 mean train loss:  1.58498893e-02, mean val. rec. loss:  1.54350467e-02\n",
      "Epoch: 1144 mean train loss:  1.58057657e-02, mean val. rec. loss:  1.53875723e-02\n",
      "Epoch: 1145 mean train loss:  1.57567982e-02, mean val. rec. loss:  1.53462805e-02\n",
      "Epoch: 1146 mean train loss:  1.57087343e-02, mean val. rec. loss:  1.52999285e-02\n",
      "Epoch: 1147 mean train loss:  1.56627289e-02, mean val. rec. loss:  1.52710135e-02\n",
      "Epoch: 1148 mean train loss:  1.56183622e-02, mean val. rec. loss:  1.52505119e-02\n",
      "Epoch: 1149 mean train loss:  1.55637893e-02, mean val. rec. loss:  1.51941602e-02\n",
      "Epoch: 1150 mean train loss:  1.55209506e-02, mean val. rec. loss:  1.51329785e-02\n",
      "Epoch: 1151 mean train loss:  1.54741051e-02, mean val. rec. loss:  1.50960527e-02\n",
      "Epoch: 1152 mean train loss:  1.54271827e-02, mean val. rec. loss:  1.50447124e-02\n",
      "Epoch: 1153 mean train loss:  1.53823044e-02, mean val. rec. loss:  1.49973520e-02\n",
      "Epoch: 1154 mean train loss:  1.53355440e-02, mean val. rec. loss:  1.49568081e-02\n",
      "Epoch: 1155 mean train loss:  1.52844217e-02, mean val. rec. loss:  1.49168247e-02\n",
      "Epoch: 1156 mean train loss:  1.52438284e-02, mean val. rec. loss:  1.48774950e-02\n",
      "Epoch: 1157 mean train loss:  1.52018443e-02, mean val. rec. loss:  1.48430650e-02\n",
      "Epoch: 1158 mean train loss:  1.51519902e-02, mean val. rec. loss:  1.47991227e-02\n",
      "Epoch: 1159 mean train loss:  1.51052558e-02, mean val. rec. loss:  1.47528707e-02\n",
      "Epoch: 1160 mean train loss:  1.50633451e-02, mean val. rec. loss:  1.47159925e-02\n",
      "Epoch: 1161 mean train loss:  1.50226447e-02, mean val. rec. loss:  1.46558796e-02\n",
      "Epoch: 1162 mean train loss:  1.49795323e-02, mean val. rec. loss:  1.46268635e-02\n",
      "Epoch: 1163 mean train loss:  1.49319533e-02, mean val. rec. loss:  1.45863184e-02\n",
      "Epoch: 1164 mean train loss:  1.48882642e-02, mean val. rec. loss:  1.45392069e-02\n",
      "Epoch: 1165 mean train loss:  1.48489471e-02, mean val. rec. loss:  1.45147510e-02\n",
      "Epoch: 1166 mean train loss:  1.48049619e-02, mean val. rec. loss:  1.44552882e-02\n",
      "Epoch: 1167 mean train loss:  1.47581075e-02, mean val. rec. loss:  1.44243473e-02\n",
      "Epoch: 1168 mean train loss:  1.47155501e-02, mean val. rec. loss:  1.43782185e-02\n",
      "Epoch: 1169 mean train loss:  1.46789331e-02, mean val. rec. loss:  1.43390772e-02\n",
      "Epoch: 1170 mean train loss:  1.46303283e-02, mean val. rec. loss:  1.42981715e-02\n",
      "Epoch: 1171 mean train loss:  1.45909593e-02, mean val. rec. loss:  1.42656664e-02\n",
      "Epoch: 1172 mean train loss:  1.45492878e-02, mean val. rec. loss:  1.42357361e-02\n",
      "Epoch: 1173 mean train loss:  1.45062038e-02, mean val. rec. loss:  1.41917182e-02\n",
      "Epoch: 1174 mean train loss:  1.44787203e-02, mean val. rec. loss:  1.41634522e-02\n",
      "Epoch: 1175 mean train loss:  1.44277091e-02, mean val. rec. loss:  1.41053886e-02\n",
      "Epoch: 1176 mean train loss:  1.43867552e-02, mean val. rec. loss:  1.40695037e-02\n",
      "Epoch: 1177 mean train loss:  1.43456079e-02, mean val. rec. loss:  1.40369031e-02\n",
      "Epoch: 1178 mean train loss:  1.43024956e-02, mean val. rec. loss:  1.39993807e-02\n",
      "Epoch: 1179 mean train loss:  1.42620691e-02, mean val. rec. loss:  1.39680467e-02\n",
      "Epoch: 1180 mean train loss:  1.42266278e-02, mean val. rec. loss:  1.39216818e-02\n",
      "Epoch: 1181 mean train loss:  1.41859531e-02, mean val. rec. loss:  1.38732433e-02\n",
      "Epoch: 1182 mean train loss:  1.41442140e-02, mean val. rec. loss:  1.38435003e-02\n",
      "Epoch: 1183 mean train loss:  1.41050033e-02, mean val. rec. loss:  1.38157100e-02\n",
      "Epoch: 1184 mean train loss:  1.40687338e-02, mean val. rec. loss:  1.37851110e-02\n",
      "Epoch: 1185 mean train loss:  1.40248561e-02, mean val. rec. loss:  1.37355804e-02\n",
      "Epoch: 1186 mean train loss:  1.39839301e-02, mean val. rec. loss:  1.36898041e-02\n",
      "Epoch: 1187 mean train loss:  1.39480492e-02, mean val. rec. loss:  1.36546868e-02\n",
      "Epoch: 1188 mean train loss:  1.39082597e-02, mean val. rec. loss:  1.36261789e-02\n",
      "Epoch: 1189 mean train loss:  1.38719022e-02, mean val. rec. loss:  1.35943041e-02\n",
      "Epoch: 1190 mean train loss:  1.38312473e-02, mean val. rec. loss:  1.35528425e-02\n",
      "Epoch: 1191 mean train loss:  1.37958797e-02, mean val. rec. loss:  1.35136500e-02\n",
      "Epoch: 1192 mean train loss:  1.37592221e-02, mean val. rec. loss:  1.34851456e-02\n",
      "Epoch: 1193 mean train loss:  1.37220175e-02, mean val. rec. loss:  1.34368594e-02\n",
      "Epoch: 1194 mean train loss:  1.36837745e-02, mean val. rec. loss:  1.34025155e-02\n",
      "Epoch: 1195 mean train loss:  1.36455304e-02, mean val. rec. loss:  1.33748299e-02\n",
      "Epoch: 1196 mean train loss:  1.36073022e-02, mean val. rec. loss:  1.33460347e-02\n",
      "Epoch: 1197 mean train loss:  1.35727660e-02, mean val. rec. loss:  1.33064921e-02\n",
      "Epoch: 1198 mean train loss:  1.35413529e-02, mean val. rec. loss:  1.32641222e-02\n",
      "Epoch: 1199 mean train loss:  1.34996024e-02, mean val. rec. loss:  1.32346095e-02\n",
      "Epoch: 1200 mean train loss:  1.34626099e-02, mean val. rec. loss:  1.32112712e-02\n",
      "Epoch: 1201 mean train loss:  1.34302524e-02, mean val. rec. loss:  1.31819702e-02\n",
      "Epoch: 1202 mean train loss:  1.33930325e-02, mean val. rec. loss:  1.31344202e-02\n",
      "Epoch: 1203 mean train loss:  1.33601084e-02, mean val. rec. loss:  1.30966396e-02\n",
      "Epoch: 1204 mean train loss:  1.33207830e-02, mean val. rec. loss:  1.30547698e-02\n",
      "Epoch: 1205 mean train loss:  1.32854943e-02, mean val. rec. loss:  1.30356824e-02\n",
      "Epoch: 1206 mean train loss:  1.32482026e-02, mean val. rec. loss:  1.30084445e-02\n",
      "Epoch: 1207 mean train loss:  1.32172582e-02, mean val. rec. loss:  1.29715571e-02\n",
      "Epoch: 1208 mean train loss:  1.31805410e-02, mean val. rec. loss:  1.29273135e-02\n",
      "Epoch: 1209 mean train loss:  1.31450180e-02, mean val. rec. loss:  1.28983544e-02\n",
      "Epoch: 1210 mean train loss:  1.31139556e-02, mean val. rec. loss:  1.28630766e-02\n",
      "Epoch: 1211 mean train loss:  1.30769242e-02, mean val. rec. loss:  1.28315600e-02\n",
      "Epoch: 1212 mean train loss:  1.30442306e-02, mean val. rec. loss:  1.27947679e-02\n",
      "Epoch: 1213 mean train loss:  1.30095309e-02, mean val. rec. loss:  1.27720717e-02\n",
      "Epoch: 1214 mean train loss:  1.29764615e-02, mean val. rec. loss:  1.27244682e-02\n",
      "Epoch: 1215 mean train loss:  1.29430718e-02, mean val. rec. loss:  1.27024488e-02\n",
      "Epoch: 1216 mean train loss:  1.29151935e-02, mean val. rec. loss:  1.26829380e-02\n",
      "Epoch: 1217 mean train loss:  1.28791117e-02, mean val. rec. loss:  1.26447305e-02\n",
      "Epoch: 1218 mean train loss:  1.28418935e-02, mean val. rec. loss:  1.26122033e-02\n",
      "Epoch: 1219 mean train loss:  1.28093339e-02, mean val. rec. loss:  1.25793608e-02\n",
      "Epoch: 1220 mean train loss:  1.27758321e-02, mean val. rec. loss:  1.25589347e-02\n",
      "Epoch: 1221 mean train loss:  1.27460771e-02, mean val. rec. loss:  1.25179105e-02\n",
      "Epoch: 1222 mean train loss:  1.27103999e-02, mean val. rec. loss:  1.24669540e-02\n",
      "Epoch: 1223 mean train loss:  1.26779343e-02, mean val. rec. loss:  1.24403488e-02\n",
      "Epoch: 1224 mean train loss:  1.26501597e-02, mean val. rec. loss:  1.24300724e-02\n",
      "Epoch: 1225 mean train loss:  1.26202929e-02, mean val. rec. loss:  1.23826457e-02\n",
      "Epoch: 1226 mean train loss:  1.25844045e-02, mean val. rec. loss:  1.23692722e-02\n",
      "Epoch: 1227 mean train loss:  1.25542958e-02, mean val. rec. loss:  1.23273849e-02\n",
      "Epoch: 1228 mean train loss:  1.25230191e-02, mean val. rec. loss:  1.23046642e-02\n",
      "Epoch: 1229 mean train loss:  1.24910365e-02, mean val. rec. loss:  1.22831519e-02\n",
      "Epoch: 1230 mean train loss:  1.24591670e-02, mean val. rec. loss:  1.22280888e-02\n",
      "Epoch: 1231 mean train loss:  1.24254478e-02, mean val. rec. loss:  1.22084943e-02\n",
      "Epoch: 1232 mean train loss:  1.23942838e-02, mean val. rec. loss:  1.21900268e-02\n",
      "Epoch: 1233 mean train loss:  1.23642816e-02, mean val. rec. loss:  1.21489222e-02\n",
      "Epoch: 1234 mean train loss:  1.23335775e-02, mean val. rec. loss:  1.21192002e-02\n",
      "Epoch: 1235 mean train loss:  1.23035133e-02, mean val. rec. loss:  1.20876161e-02\n",
      "Epoch: 1236 mean train loss:  1.22723239e-02, mean val. rec. loss:  1.20645267e-02\n",
      "Epoch: 1237 mean train loss:  1.22407396e-02, mean val. rec. loss:  1.20310783e-02\n",
      "Epoch: 1238 mean train loss:  1.22135835e-02, mean val. rec. loss:  1.20150357e-02\n",
      "Epoch: 1239 mean train loss:  1.21845706e-02, mean val. rec. loss:  1.19727507e-02\n",
      "Epoch: 1240 mean train loss:  1.21521446e-02, mean val. rec. loss:  1.19429984e-02\n",
      "Epoch: 1241 mean train loss:  1.21247945e-02, mean val. rec. loss:  1.19233806e-02\n",
      "Epoch: 1242 mean train loss:  1.20929053e-02, mean val. rec. loss:  1.18890390e-02\n",
      "Epoch: 1243 mean train loss:  1.20619314e-02, mean val. rec. loss:  1.18590262e-02\n",
      "Epoch: 1244 mean train loss:  1.20352692e-02, mean val. rec. loss:  1.18427381e-02\n",
      "Epoch: 1245 mean train loss:  1.20055801e-02, mean val. rec. loss:  1.18036549e-02\n",
      "Epoch: 1246 mean train loss:  1.19793949e-02, mean val. rec. loss:  1.17786698e-02\n",
      "Epoch: 1247 mean train loss:  1.19533588e-02, mean val. rec. loss:  1.17500421e-02\n",
      "Epoch: 1248 mean train loss:  1.19193166e-02, mean val. rec. loss:  1.17127465e-02\n",
      "Epoch: 1249 mean train loss:  1.18918008e-02, mean val. rec. loss:  1.16710523e-02\n",
      "Epoch: 1250 mean train loss:  1.18610248e-02, mean val. rec. loss:  1.16515450e-02\n",
      "Epoch: 1251 mean train loss:  1.18322860e-02, mean val. rec. loss:  1.16302211e-02\n",
      "Epoch: 1252 mean train loss:  1.18040424e-02, mean val. rec. loss:  1.16052651e-02\n",
      "Epoch: 1253 mean train loss:  1.17779063e-02, mean val. rec. loss:  1.15814860e-02\n",
      "Epoch: 1254 mean train loss:  1.17467020e-02, mean val. rec. loss:  1.15489297e-02\n",
      "Epoch: 1255 mean train loss:  1.17215026e-02, mean val. rec. loss:  1.15246994e-02\n",
      "Epoch: 1256 mean train loss:  1.16943701e-02, mean val. rec. loss:  1.15024322e-02\n",
      "Epoch: 1257 mean train loss:  1.16657448e-02, mean val. rec. loss:  1.14678639e-02\n",
      "Epoch: 1258 mean train loss:  1.16351370e-02, mean val. rec. loss:  1.14423729e-02\n",
      "Epoch: 1259 mean train loss:  1.16086396e-02, mean val. rec. loss:  1.14149024e-02\n",
      "Epoch: 1260 mean train loss:  1.15877283e-02, mean val. rec. loss:  1.13956742e-02\n",
      "Epoch: 1261 mean train loss:  1.15594770e-02, mean val. rec. loss:  1.13687190e-02\n",
      "Epoch: 1262 mean train loss:  1.15304801e-02, mean val. rec. loss:  1.13483638e-02\n",
      "Epoch: 1263 mean train loss:  1.15010222e-02, mean val. rec. loss:  1.13020734e-02\n",
      "Epoch: 1264 mean train loss:  1.14762113e-02, mean val. rec. loss:  1.12727095e-02\n",
      "Epoch: 1265 mean train loss:  1.14470244e-02, mean val. rec. loss:  1.12567762e-02\n",
      "Epoch: 1266 mean train loss:  1.14242674e-02, mean val. rec. loss:  1.12405672e-02\n",
      "Epoch: 1267 mean train loss:  1.13921282e-02, mean val. rec. loss:  1.12042136e-02\n",
      "Epoch: 1268 mean train loss:  1.13659834e-02, mean val. rec. loss:  1.11717108e-02\n",
      "Epoch: 1269 mean train loss:  1.13438647e-02, mean val. rec. loss:  1.11480399e-02\n",
      "Epoch: 1270 mean train loss:  1.13145272e-02, mean val. rec. loss:  1.11329195e-02\n",
      "Epoch: 1271 mean train loss:  1.12924459e-02, mean val. rec. loss:  1.11164768e-02\n",
      "Epoch: 1272 mean train loss:  1.12694531e-02, mean val. rec. loss:  1.10824852e-02\n",
      "Epoch: 1273 mean train loss:  1.12367795e-02, mean val. rec. loss:  1.10516467e-02\n",
      "Epoch: 1274 mean train loss:  1.12124773e-02, mean val. rec. loss:  1.10312008e-02\n",
      "Epoch: 1275 mean train loss:  1.11867983e-02, mean val. rec. loss:  1.09987410e-02\n",
      "Epoch: 1276 mean train loss:  1.11603614e-02, mean val. rec. loss:  1.09863142e-02\n",
      "Epoch: 1277 mean train loss:  1.11350232e-02, mean val. rec. loss:  1.09638284e-02\n",
      "Epoch: 1278 mean train loss:  1.11103511e-02, mean val. rec. loss:  1.09258710e-02\n",
      "Epoch: 1279 mean train loss:  1.10848783e-02, mean val. rec. loss:  1.09068917e-02\n",
      "Epoch: 1280 mean train loss:  1.10622745e-02, mean val. rec. loss:  1.08694995e-02\n",
      "Epoch: 1281 mean train loss:  1.10330228e-02, mean val. rec. loss:  1.08552514e-02\n",
      "Epoch: 1282 mean train loss:  1.10163430e-02, mean val. rec. loss:  1.08155192e-02\n",
      "Epoch: 1283 mean train loss:  1.09898632e-02, mean val. rec. loss:  1.08198084e-02\n",
      "Epoch: 1284 mean train loss:  1.09603033e-02, mean val. rec. loss:  1.07935743e-02\n",
      "Epoch: 1285 mean train loss:  1.09346837e-02, mean val. rec. loss:  1.07446089e-02\n",
      "Epoch: 1286 mean train loss:  1.09130262e-02, mean val. rec. loss:  1.07186434e-02\n",
      "Epoch: 1287 mean train loss:  1.08859658e-02, mean val. rec. loss:  1.06987488e-02\n",
      "Epoch: 1288 mean train loss:  1.08592108e-02, mean val. rec. loss:  1.06817664e-02\n",
      "Epoch: 1289 mean train loss:  1.08375661e-02, mean val. rec. loss:  1.06560963e-02\n",
      "Epoch: 1290 mean train loss:  1.08155740e-02, mean val. rec. loss:  1.06445475e-02\n",
      "Epoch: 1291 mean train loss:  1.07966052e-02, mean val. rec. loss:  1.06125401e-02\n",
      "Epoch: 1292 mean train loss:  1.07636909e-02, mean val. rec. loss:  1.05852906e-02\n",
      "Epoch: 1293 mean train loss:  1.07380353e-02, mean val. rec. loss:  1.05704750e-02\n",
      "Epoch: 1294 mean train loss:  1.07176338e-02, mean val. rec. loss:  1.05365869e-02\n",
      "Epoch: 1295 mean train loss:  1.06939862e-02, mean val. rec. loss:  1.05199581e-02\n",
      "Epoch: 1296 mean train loss:  1.06695368e-02, mean val. rec. loss:  1.04997309e-02\n",
      "Epoch: 1297 mean train loss:  1.06447196e-02, mean val. rec. loss:  1.04751587e-02\n",
      "Epoch: 1298 mean train loss:  1.06258549e-02, mean val. rec. loss:  1.04421162e-02\n",
      "Epoch: 1299 mean train loss:  1.06034994e-02, mean val. rec. loss:  1.04360359e-02\n",
      "Epoch: 1300 mean train loss:  1.05741196e-02, mean val. rec. loss:  1.03993183e-02\n",
      "Epoch: 1301 mean train loss:  1.05515065e-02, mean val. rec. loss:  1.03733749e-02\n",
      "Epoch: 1302 mean train loss:  1.05283811e-02, mean val. rec. loss:  1.03510612e-02\n",
      "Epoch: 1303 mean train loss:  1.05070888e-02, mean val. rec. loss:  1.03265751e-02\n",
      "Epoch: 1304 mean train loss:  1.04849501e-02, mean val. rec. loss:  1.03051453e-02\n",
      "Epoch: 1305 mean train loss:  1.04604936e-02, mean val. rec. loss:  1.02831550e-02\n",
      "Epoch: 1306 mean train loss:  1.04405357e-02, mean val. rec. loss:  1.02713178e-02\n",
      "Epoch: 1307 mean train loss:  1.04163831e-02, mean val. rec. loss:  1.02476795e-02\n",
      "Epoch: 1308 mean train loss:  1.03916441e-02, mean val. rec. loss:  1.02209487e-02\n",
      "Epoch: 1309 mean train loss:  1.03673503e-02, mean val. rec. loss:  1.01942365e-02\n",
      "Epoch: 1310 mean train loss:  1.03492713e-02, mean val. rec. loss:  1.01569978e-02\n",
      "Epoch: 1311 mean train loss:  1.03300651e-02, mean val. rec. loss:  1.01592227e-02\n",
      "Epoch: 1312 mean train loss:  1.03078445e-02, mean val. rec. loss:  1.01367578e-02\n",
      "Epoch: 1313 mean train loss:  1.02798525e-02, mean val. rec. loss:  1.01083976e-02\n",
      "Epoch: 1314 mean train loss:  1.02619986e-02, mean val. rec. loss:  1.00822193e-02\n",
      "Epoch: 1315 mean train loss:  1.02359278e-02, mean val. rec. loss:  1.00574331e-02\n",
      "Epoch: 1316 mean train loss:  1.02144650e-02, mean val. rec. loss:  1.00301045e-02\n",
      "Epoch: 1317 mean train loss:  1.01922782e-02, mean val. rec. loss:  1.00182091e-02\n",
      "Epoch: 1318 mean train loss:  1.01751569e-02, mean val. rec. loss:  9.99571870e-03\n",
      "Epoch: 1319 mean train loss:  1.01453552e-02, mean val. rec. loss:  9.98098211e-03\n",
      "Epoch: 1320 mean train loss:  1.01321368e-02, mean val. rec. loss:  9.96483128e-03\n",
      "Epoch: 1321 mean train loss:  1.01111707e-02, mean val. rec. loss:  9.93176088e-03\n",
      "Epoch: 1322 mean train loss:  1.00859115e-02, mean val. rec. loss:  9.92325573e-03\n",
      "Epoch: 1323 mean train loss:  1.00637200e-02, mean val. rec. loss:  9.88687306e-03\n",
      "Epoch: 1324 mean train loss:  1.00408324e-02, mean val. rec. loss:  9.85819770e-03\n",
      "Epoch: 1325 mean train loss:  1.00165015e-02, mean val. rec. loss:  9.83279507e-03\n",
      "Epoch: 1326 mean train loss:  9.99800663e-03, mean val. rec. loss:  9.82716956e-03\n",
      "Epoch: 1327 mean train loss:  9.97462800e-03, mean val. rec. loss:  9.81025231e-03\n",
      "Epoch: 1328 mean train loss:  9.95139325e-03, mean val. rec. loss:  9.79228369e-03\n",
      "Epoch: 1329 mean train loss:  9.93633227e-03, mean val. rec. loss:  9.76796732e-03\n",
      "Epoch: 1330 mean train loss:  9.91399246e-03, mean val. rec. loss:  9.73891282e-03\n",
      "Epoch: 1331 mean train loss:  9.89177703e-03, mean val. rec. loss:  9.71429755e-03\n",
      "Epoch: 1332 mean train loss:  9.87083135e-03, mean val. rec. loss:  9.69302189e-03\n",
      "Epoch: 1333 mean train loss:  9.85504754e-03, mean val. rec. loss:  9.66539674e-03\n",
      "Epoch: 1334 mean train loss:  9.83202024e-03, mean val. rec. loss:  9.65631532e-03\n",
      "Epoch: 1335 mean train loss:  9.81304308e-03, mean val. rec. loss:  9.64749732e-03\n",
      "Epoch: 1336 mean train loss:  9.79050546e-03, mean val. rec. loss:  9.61178512e-03\n",
      "Epoch: 1337 mean train loss:  9.76990955e-03, mean val. rec. loss:  9.58918944e-03\n",
      "Epoch: 1338 mean train loss:  9.75394416e-03, mean val. rec. loss:  9.57123128e-03\n",
      "Epoch: 1339 mean train loss:  9.72884864e-03, mean val. rec. loss:  9.55263522e-03\n",
      "Epoch: 1340 mean train loss:  9.71246836e-03, mean val. rec. loss:  9.54042877e-03\n",
      "Epoch: 1341 mean train loss:  9.68970011e-03, mean val. rec. loss:  9.51400443e-03\n",
      "Epoch: 1342 mean train loss:  9.66744618e-03, mean val. rec. loss:  9.48788829e-03\n",
      "Epoch: 1343 mean train loss:  9.64633027e-03, mean val. rec. loss:  9.47299585e-03\n",
      "Epoch: 1344 mean train loss:  9.62478199e-03, mean val. rec. loss:  9.44225846e-03\n",
      "Epoch: 1345 mean train loss:  9.60745290e-03, mean val. rec. loss:  9.42735381e-03\n",
      "Epoch: 1346 mean train loss:  9.58542233e-03, mean val. rec. loss:  9.40558852e-03\n",
      "Epoch: 1347 mean train loss:  9.56726686e-03, mean val. rec. loss:  9.38493042e-03\n",
      "Epoch: 1348 mean train loss:  9.54861184e-03, mean val. rec. loss:  9.37465691e-03\n",
      "Epoch: 1349 mean train loss:  9.52980064e-03, mean val. rec. loss:  9.35640917e-03\n",
      "Epoch: 1350 mean train loss:  9.50416282e-03, mean val. rec. loss:  9.33766598e-03\n",
      "Epoch: 1351 mean train loss:  9.48864542e-03, mean val. rec. loss:  9.31525696e-03\n",
      "Epoch: 1352 mean train loss:  9.46700995e-03, mean val. rec. loss:  9.30132110e-03\n",
      "Epoch: 1353 mean train loss:  9.44864583e-03, mean val. rec. loss:  9.28042226e-03\n",
      "Epoch: 1354 mean train loss:  9.43077564e-03, mean val. rec. loss:  9.24249509e-03\n",
      "Epoch: 1355 mean train loss:  9.41001365e-03, mean val. rec. loss:  9.22964957e-03\n",
      "Epoch: 1356 mean train loss:  9.39377913e-03, mean val. rec. loss:  9.22010701e-03\n",
      "Epoch: 1357 mean train loss:  9.36836950e-03, mean val. rec. loss:  9.19380595e-03\n",
      "Epoch: 1358 mean train loss:  9.35105934e-03, mean val. rec. loss:  9.17507672e-03\n",
      "Epoch: 1359 mean train loss:  9.33320618e-03, mean val. rec. loss:  9.15310325e-03\n",
      "Epoch: 1360 mean train loss:  9.31910128e-03, mean val. rec. loss:  9.15715985e-03\n",
      "Epoch: 1361 mean train loss:  9.29814191e-03, mean val. rec. loss:  9.12649166e-03\n",
      "Epoch: 1362 mean train loss:  9.28014178e-03, mean val. rec. loss:  9.09598630e-03\n",
      "Epoch: 1363 mean train loss:  9.25596562e-03, mean val. rec. loss:  9.06914618e-03\n",
      "Epoch: 1364 mean train loss:  9.23425410e-03, mean val. rec. loss:  9.05812775e-03\n",
      "Epoch: 1365 mean train loss:  9.21850734e-03, mean val. rec. loss:  9.04638069e-03\n",
      "Epoch: 1366 mean train loss:  9.19933313e-03, mean val. rec. loss:  9.03488020e-03\n",
      "Epoch: 1367 mean train loss:  9.18222463e-03, mean val. rec. loss:  8.99480610e-03\n",
      "Epoch: 1368 mean train loss:  9.15991390e-03, mean val. rec. loss:  8.98215015e-03\n",
      "Epoch: 1369 mean train loss:  9.14209077e-03, mean val. rec. loss:  8.96716583e-03\n",
      "Epoch: 1370 mean train loss:  9.12984183e-03, mean val. rec. loss:  8.94304020e-03\n",
      "Epoch: 1371 mean train loss:  9.10616980e-03, mean val. rec. loss:  8.93458971e-03\n",
      "Epoch: 1372 mean train loss:  9.08527837e-03, mean val. rec. loss:  8.91927801e-03\n",
      "Epoch: 1373 mean train loss:  9.07159154e-03, mean val. rec. loss:  8.89270131e-03\n",
      "Epoch: 1374 mean train loss:  9.05339665e-03, mean val. rec. loss:  8.87998953e-03\n",
      "Epoch: 1375 mean train loss:  9.03486077e-03, mean val. rec. loss:  8.85770263e-03\n",
      "Epoch: 1376 mean train loss:  9.01829159e-03, mean val. rec. loss:  8.84258340e-03\n",
      "Epoch: 1377 mean train loss:  9.00216650e-03, mean val. rec. loss:  8.79952035e-03\n",
      "Epoch: 1378 mean train loss:  8.97550284e-03, mean val. rec. loss:  8.79618540e-03\n",
      "Epoch: 1379 mean train loss:  8.96032433e-03, mean val. rec. loss:  8.79407918e-03\n",
      "Epoch: 1380 mean train loss:  8.94343274e-03, mean val. rec. loss:  8.77136486e-03\n",
      "Epoch: 1381 mean train loss:  8.92529703e-03, mean val. rec. loss:  8.74385485e-03\n",
      "Epoch: 1382 mean train loss:  8.90597115e-03, mean val. rec. loss:  8.72551232e-03\n",
      "Epoch: 1383 mean train loss:  8.88745638e-03, mean val. rec. loss:  8.70469140e-03\n",
      "Epoch: 1384 mean train loss:  8.86815721e-03, mean val. rec. loss:  8.69860300e-03\n",
      "Epoch: 1385 mean train loss:  8.85071736e-03, mean val. rec. loss:  8.68017906e-03\n",
      "Epoch: 1386 mean train loss:  8.83479254e-03, mean val. rec. loss:  8.64659577e-03\n",
      "Epoch: 1387 mean train loss:  8.81669300e-03, mean val. rec. loss:  8.63192605e-03\n",
      "Epoch: 1388 mean train loss:  8.80117221e-03, mean val. rec. loss:  8.62077038e-03\n",
      "Epoch: 1389 mean train loss:  8.78271937e-03, mean val. rec. loss:  8.60318440e-03\n",
      "Epoch: 1390 mean train loss:  8.76360804e-03, mean val. rec. loss:  8.58426094e-03\n",
      "Epoch: 1391 mean train loss:  8.74658771e-03, mean val. rec. loss:  8.56773214e-03\n",
      "Epoch: 1392 mean train loss:  8.73164689e-03, mean val. rec. loss:  8.54260166e-03\n",
      "Epoch: 1393 mean train loss:  8.71390828e-03, mean val. rec. loss:  8.53810659e-03\n",
      "Epoch: 1394 mean train loss:  8.69544533e-03, mean val. rec. loss:  8.51346807e-03\n",
      "Epoch: 1395 mean train loss:  8.67574796e-03, mean val. rec. loss:  8.49452484e-03\n",
      "Epoch: 1396 mean train loss:  8.66094490e-03, mean val. rec. loss:  8.47685279e-03\n",
      "Epoch: 1397 mean train loss:  8.64150516e-03, mean val. rec. loss:  8.45427978e-03\n",
      "Epoch: 1398 mean train loss:  8.62497841e-03, mean val. rec. loss:  8.45298767e-03\n",
      "Epoch: 1399 mean train loss:  8.60772676e-03, mean val. rec. loss:  8.42595449e-03\n",
      "Epoch: 1400 mean train loss:  8.59216350e-03, mean val. rec. loss:  8.40845922e-03\n",
      "Epoch: 1401 mean train loss:  8.57126473e-03, mean val. rec. loss:  8.38345086e-03\n",
      "Epoch: 1402 mean train loss:  8.55546946e-03, mean val. rec. loss:  8.37159971e-03\n",
      "Epoch: 1403 mean train loss:  8.54141481e-03, mean val. rec. loss:  8.36737679e-03\n",
      "Epoch: 1404 mean train loss:  8.52185339e-03, mean val. rec. loss:  8.34682744e-03\n",
      "Epoch: 1405 mean train loss:  8.50513421e-03, mean val. rec. loss:  8.32591406e-03\n",
      "Epoch: 1406 mean train loss:  8.48876593e-03, mean val. rec. loss:  8.29553372e-03\n",
      "Epoch: 1407 mean train loss:  8.47616053e-03, mean val. rec. loss:  8.28421058e-03\n",
      "Epoch: 1408 mean train loss:  8.45462055e-03, mean val. rec. loss:  8.27539258e-03\n",
      "Epoch: 1409 mean train loss:  8.43951390e-03, mean val. rec. loss:  8.25612022e-03\n",
      "Epoch: 1410 mean train loss:  8.43002639e-03, mean val. rec. loss:  8.24608221e-03\n",
      "Epoch: 1411 mean train loss:  8.40574175e-03, mean val. rec. loss:  8.22619461e-03\n",
      "Epoch: 1412 mean train loss:  8.38767660e-03, mean val. rec. loss:  8.20879936e-03\n",
      "Epoch: 1413 mean train loss:  8.36947771e-03, mean val. rec. loss:  8.18855123e-03\n",
      "Epoch: 1414 mean train loss:  8.35556181e-03, mean val. rec. loss:  8.16686212e-03\n",
      "Epoch: 1415 mean train loss:  8.33931278e-03, mean val. rec. loss:  8.15981365e-03\n",
      "Epoch: 1416 mean train loss:  8.32283209e-03, mean val. rec. loss:  8.13600261e-03\n",
      "Epoch: 1417 mean train loss:  8.30379860e-03, mean val. rec. loss:  8.12561803e-03\n",
      "Epoch: 1418 mean train loss:  8.29003819e-03, mean val. rec. loss:  8.10365328e-03\n",
      "Epoch: 1419 mean train loss:  8.27494329e-03, mean val. rec. loss:  8.09064726e-03\n",
      "Epoch: 1420 mean train loss:  8.26039026e-03, mean val. rec. loss:  8.07337994e-03\n",
      "Epoch: 1421 mean train loss:  8.23995096e-03, mean val. rec. loss:  8.04882573e-03\n",
      "Epoch: 1422 mean train loss:  8.22325165e-03, mean val. rec. loss:  8.04203545e-03\n",
      "Epoch: 1423 mean train loss:  8.20873388e-03, mean val. rec. loss:  8.02775592e-03\n",
      "Epoch: 1424 mean train loss:  8.20179162e-03, mean val. rec. loss:  7.99872643e-03\n",
      "Epoch: 1425 mean train loss:  8.17806948e-03, mean val. rec. loss:  7.97923135e-03\n",
      "Epoch: 1426 mean train loss:  8.16007943e-03, mean val. rec. loss:  7.98179580e-03\n",
      "Epoch: 1427 mean train loss:  8.14310338e-03, mean val. rec. loss:  7.96177911e-03\n",
      "Epoch: 1428 mean train loss:  8.12775315e-03, mean val. rec. loss:  7.93698357e-03\n",
      "Epoch: 1429 mean train loss:  8.11041737e-03, mean val. rec. loss:  7.91788973e-03\n",
      "Epoch: 1430 mean train loss:  8.09993103e-03, mean val. rec. loss:  7.91731811e-03\n",
      "Epoch: 1431 mean train loss:  8.07999837e-03, mean val. rec. loss:  7.89876915e-03\n",
      "Epoch: 1432 mean train loss:  8.06523301e-03, mean val. rec. loss:  7.87937525e-03\n",
      "Epoch: 1433 mean train loss:  8.04851362e-03, mean val. rec. loss:  7.86661288e-03\n",
      "Epoch: 1434 mean train loss:  8.03257425e-03, mean val. rec. loss:  7.83837946e-03\n",
      "Epoch: 1435 mean train loss:  8.01913526e-03, mean val. rec. loss:  7.84031240e-03\n",
      "Epoch: 1436 mean train loss:  8.00622980e-03, mean val. rec. loss:  7.82567117e-03\n",
      "Epoch: 1437 mean train loss:  7.98544703e-03, mean val. rec. loss:  7.80076166e-03\n",
      "Epoch: 1438 mean train loss:  7.97074367e-03, mean val. rec. loss:  7.77270038e-03\n",
      "Epoch: 1439 mean train loss:  7.95662316e-03, mean val. rec. loss:  7.75408629e-03\n",
      "Epoch: 1440 mean train loss:  7.93959402e-03, mean val. rec. loss:  7.76227393e-03\n",
      "Epoch: 1441 mean train loss:  7.92338407e-03, mean val. rec. loss:  7.74344468e-03\n",
      "Epoch: 1442 mean train loss:  7.91057016e-03, mean val. rec. loss:  7.73229599e-03\n",
      "Epoch: 1443 mean train loss:  7.89373120e-03, mean val. rec. loss:  7.70408002e-03\n",
      "Epoch: 1444 mean train loss:  7.87853802e-03, mean val. rec. loss:  7.69146303e-03\n",
      "Epoch: 1445 mean train loss:  7.86133371e-03, mean val. rec. loss:  7.68092202e-03\n",
      "Epoch: 1446 mean train loss:  7.84832166e-03, mean val. rec. loss:  7.66773108e-03\n",
      "Epoch: 1447 mean train loss:  7.83570942e-03, mean val. rec. loss:  7.63471592e-03\n",
      "Epoch: 1448 mean train loss:  7.81521481e-03, mean val. rec. loss:  7.63027319e-03\n",
      "Epoch: 1449 mean train loss:  7.80360028e-03, mean val. rec. loss:  7.62149764e-03\n",
      "Epoch: 1450 mean train loss:  7.78459968e-03, mean val. rec. loss:  7.61166317e-03\n",
      "Epoch: 1451 mean train loss:  7.77264178e-03, mean val. rec. loss:  7.58567205e-03\n",
      "Epoch: 1452 mean train loss:  7.75638055e-03, mean val. rec. loss:  7.56389804e-03\n",
      "Epoch: 1453 mean train loss:  7.74321892e-03, mean val. rec. loss:  7.54814613e-03\n",
      "Epoch: 1454 mean train loss:  7.72877858e-03, mean val. rec. loss:  7.54239268e-03\n",
      "Epoch: 1455 mean train loss:  7.70943331e-03, mean val. rec. loss:  7.52884062e-03\n",
      "Epoch: 1456 mean train loss:  7.69735227e-03, mean val. rec. loss:  7.50555527e-03\n",
      "Epoch: 1457 mean train loss:  7.67998497e-03, mean val. rec. loss:  7.50532150e-03\n",
      "Epoch: 1458 mean train loss:  7.66916582e-03, mean val. rec. loss:  7.48466166e-03\n",
      "Epoch: 1459 mean train loss:  7.65084740e-03, mean val. rec. loss:  7.47325769e-03\n",
      "Epoch: 1460 mean train loss:  7.63656816e-03, mean val. rec. loss:  7.43858756e-03\n",
      "Epoch: 1461 mean train loss:  7.62686472e-03, mean val. rec. loss:  7.42344798e-03\n",
      "Epoch: 1462 mean train loss:  7.61018335e-03, mean val. rec. loss:  7.42079223e-03\n",
      "Epoch: 1463 mean train loss:  7.59439154e-03, mean val. rec. loss:  7.40393371e-03\n",
      "Epoch: 1464 mean train loss:  7.58285488e-03, mean val. rec. loss:  7.38201723e-03\n",
      "Epoch: 1465 mean train loss:  7.56583917e-03, mean val. rec. loss:  7.36227094e-03\n",
      "Epoch: 1466 mean train loss:  7.54814234e-03, mean val. rec. loss:  7.36730855e-03\n",
      "Epoch: 1467 mean train loss:  7.54192592e-03, mean val. rec. loss:  7.36949153e-03\n",
      "Epoch: 1468 mean train loss:  7.52102754e-03, mean val. rec. loss:  7.34394702e-03\n",
      "Epoch: 1469 mean train loss:  7.50851642e-03, mean val. rec. loss:  7.32699138e-03\n",
      "Epoch: 1470 mean train loss:  7.49826613e-03, mean val. rec. loss:  7.29805667e-03\n",
      "Epoch: 1471 mean train loss:  7.47560585e-03, mean val. rec. loss:  7.27786902e-03\n",
      "Epoch: 1472 mean train loss:  7.46276218e-03, mean val. rec. loss:  7.26205837e-03\n",
      "Epoch: 1473 mean train loss:  7.45080195e-03, mean val. rec. loss:  7.26282538e-03\n",
      "Epoch: 1474 mean train loss:  7.43652282e-03, mean val. rec. loss:  7.24094786e-03\n",
      "Epoch: 1475 mean train loss:  7.41880310e-03, mean val. rec. loss:  7.22786973e-03\n",
      "Epoch: 1476 mean train loss:  7.40595171e-03, mean val. rec. loss:  7.22515292e-03\n",
      "Epoch: 1477 mean train loss:  7.39144275e-03, mean val. rec. loss:  7.20804901e-03\n",
      "Epoch: 1478 mean train loss:  7.37724207e-03, mean val. rec. loss:  7.19298968e-03\n",
      "Epoch: 1479 mean train loss:  7.36182011e-03, mean val. rec. loss:  7.16738468e-03\n",
      "Epoch: 1480 mean train loss:  7.34817363e-03, mean val. rec. loss:  7.15575509e-03\n",
      "Epoch: 1481 mean train loss:  7.33472321e-03, mean val. rec. loss:  7.13891809e-03\n",
      "Epoch: 1482 mean train loss:  7.31757381e-03, mean val. rec. loss:  7.12273412e-03\n",
      "Epoch: 1483 mean train loss:  7.30796095e-03, mean val. rec. loss:  7.12776184e-03\n",
      "Epoch: 1484 mean train loss:  7.29138867e-03, mean val. rec. loss:  7.10982579e-03\n",
      "Epoch: 1485 mean train loss:  7.27856923e-03, mean val. rec. loss:  7.08477555e-03\n",
      "Epoch: 1486 mean train loss:  7.26700113e-03, mean val. rec. loss:  7.06449951e-03\n",
      "Epoch: 1487 mean train loss:  7.25376300e-03, mean val. rec. loss:  7.06820314e-03\n",
      "Epoch: 1488 mean train loss:  7.23630750e-03, mean val. rec. loss:  7.04775496e-03\n",
      "Epoch: 1489 mean train loss:  7.22049612e-03, mean val. rec. loss:  7.02941127e-03\n",
      "Epoch: 1490 mean train loss:  7.21181087e-03, mean val. rec. loss:  7.01886386e-03\n",
      "Epoch: 1491 mean train loss:  7.19420850e-03, mean val. rec. loss:  7.00671731e-03\n",
      "Epoch: 1492 mean train loss:  7.18070747e-03, mean val. rec. loss:  6.98971167e-03\n",
      "Epoch: 1493 mean train loss:  7.16590255e-03, mean val. rec. loss:  6.97133018e-03\n",
      "Epoch: 1494 mean train loss:  7.16047489e-03, mean val. rec. loss:  6.96458235e-03\n",
      "Epoch: 1495 mean train loss:  7.13897046e-03, mean val. rec. loss:  6.94831871e-03\n",
      "Epoch: 1496 mean train loss:  7.12844886e-03, mean val. rec. loss:  6.95225553e-03\n",
      "Epoch: 1497 mean train loss:  7.11080465e-03, mean val. rec. loss:  6.93018902e-03\n",
      "Epoch: 1498 mean train loss:  7.09782102e-03, mean val. rec. loss:  6.90202248e-03\n",
      "Epoch: 1499 mean train loss:  7.08528585e-03, mean val. rec. loss:  6.88801161e-03\n",
      "Epoch: 1500 mean train loss:  7.07257362e-03, mean val. rec. loss:  6.87547544e-03\n",
      "Epoch: 1501 mean train loss:  7.06474055e-03, mean val. rec. loss:  6.85858494e-03\n",
      "Epoch: 1502 mean train loss:  7.04533640e-03, mean val. rec. loss:  6.86022770e-03\n",
      "Epoch: 1503 mean train loss:  7.03396520e-03, mean val. rec. loss:  6.84742346e-03\n",
      "Epoch: 1504 mean train loss:  7.02226570e-03, mean val. rec. loss:  6.83104120e-03\n",
      "Epoch: 1505 mean train loss:  7.00886797e-03, mean val. rec. loss:  6.81495783e-03\n",
      "Epoch: 1506 mean train loss:  6.99227593e-03, mean val. rec. loss:  6.79794056e-03\n",
      "Epoch: 1507 mean train loss:  6.98116898e-03, mean val. rec. loss:  6.78210607e-03\n",
      "Epoch: 1508 mean train loss:  6.96490405e-03, mean val. rec. loss:  6.77337646e-03\n",
      "Epoch: 1509 mean train loss:  6.95259536e-03, mean val. rec. loss:  6.77204190e-03\n",
      "Epoch: 1510 mean train loss:  6.94066388e-03, mean val. rec. loss:  6.76080890e-03\n",
      "Epoch: 1511 mean train loss:  6.92254105e-03, mean val. rec. loss:  6.73752820e-03\n",
      "Epoch: 1512 mean train loss:  6.91411187e-03, mean val. rec. loss:  6.71473364e-03\n",
      "Epoch: 1513 mean train loss:  6.90127765e-03, mean val. rec. loss:  6.70573479e-03\n",
      "Epoch: 1514 mean train loss:  6.88412840e-03, mean val. rec. loss:  6.69382723e-03\n",
      "Epoch: 1515 mean train loss:  6.87685121e-03, mean val. rec. loss:  6.67134087e-03\n",
      "Epoch: 1516 mean train loss:  6.86127584e-03, mean val. rec. loss:  6.67307202e-03\n",
      "Epoch: 1517 mean train loss:  6.84929302e-03, mean val. rec. loss:  6.66049864e-03\n",
      "Epoch: 1518 mean train loss:  6.84391120e-03, mean val. rec. loss:  6.65247498e-03\n",
      "Epoch: 1519 mean train loss:  6.81725257e-03, mean val. rec. loss:  6.63313866e-03\n",
      "Epoch: 1520 mean train loss:  6.80666078e-03, mean val. rec. loss:  6.61487812e-03\n",
      "Epoch: 1521 mean train loss:  6.79599475e-03, mean val. rec. loss:  6.59198179e-03\n",
      "Epoch: 1522 mean train loss:  6.78043779e-03, mean val. rec. loss:  6.58712387e-03\n",
      "Epoch: 1523 mean train loss:  6.76980939e-03, mean val. rec. loss:  6.57530703e-03\n",
      "Epoch: 1524 mean train loss:  6.75631428e-03, mean val. rec. loss:  6.56797363e-03\n",
      "Epoch: 1525 mean train loss:  6.74208358e-03, mean val. rec. loss:  6.55869507e-03\n",
      "Epoch: 1526 mean train loss:  6.72799975e-03, mean val. rec. loss:  6.54974507e-03\n",
      "Epoch: 1527 mean train loss:  6.71971736e-03, mean val. rec. loss:  6.52452911e-03\n",
      "Epoch: 1528 mean train loss:  6.70498904e-03, mean val. rec. loss:  6.51954558e-03\n",
      "Epoch: 1529 mean train loss:  6.69219792e-03, mean val. rec. loss:  6.50549051e-03\n",
      "Epoch: 1530 mean train loss:  6.67890561e-03, mean val. rec. loss:  6.48107645e-03\n",
      "Epoch: 1531 mean train loss:  6.66788748e-03, mean val. rec. loss:  6.47254106e-03\n",
      "Epoch: 1532 mean train loss:  6.65722487e-03, mean val. rec. loss:  6.45908263e-03\n",
      "Epoch: 1533 mean train loss:  6.63789586e-03, mean val. rec. loss:  6.44460539e-03\n",
      "Epoch: 1534 mean train loss:  6.63289475e-03, mean val. rec. loss:  6.44078487e-03\n",
      "Epoch: 1535 mean train loss:  6.61908794e-03, mean val. rec. loss:  6.42424328e-03\n",
      "Epoch: 1536 mean train loss:  6.60890985e-03, mean val. rec. loss:  6.42234291e-03\n",
      "Epoch: 1537 mean train loss:  6.59221174e-03, mean val. rec. loss:  6.40561174e-03\n",
      "Epoch: 1538 mean train loss:  6.57726269e-03, mean val. rec. loss:  6.39377571e-03\n",
      "Epoch: 1539 mean train loss:  6.56604465e-03, mean val. rec. loss:  6.37105616e-03\n",
      "Epoch: 1540 mean train loss:  6.55697350e-03, mean val. rec. loss:  6.35570782e-03\n",
      "Epoch: 1541 mean train loss:  6.54624907e-03, mean val. rec. loss:  6.34912979e-03\n",
      "Epoch: 1542 mean train loss:  6.52976845e-03, mean val. rec. loss:  6.34760798e-03\n",
      "Epoch: 1543 mean train loss:  6.52296941e-03, mean val. rec. loss:  6.33722398e-03\n",
      "Epoch: 1544 mean train loss:  6.50809070e-03, mean val. rec. loss:  6.30464961e-03\n",
      "Epoch: 1545 mean train loss:  6.49427283e-03, mean val. rec. loss:  6.29503553e-03\n",
      "Epoch: 1546 mean train loss:  6.47817667e-03, mean val. rec. loss:  6.28757361e-03\n",
      "Epoch: 1547 mean train loss:  6.46551894e-03, mean val. rec. loss:  6.28958621e-03\n",
      "Epoch: 1548 mean train loss:  6.45574367e-03, mean val. rec. loss:  6.26220587e-03\n",
      "Epoch: 1549 mean train loss:  6.44427771e-03, mean val. rec. loss:  6.25126769e-03\n",
      "Epoch: 1550 mean train loss:  6.42910330e-03, mean val. rec. loss:  6.24223221e-03\n",
      "Epoch: 1551 mean train loss:  6.42057940e-03, mean val. rec. loss:  6.23586527e-03\n",
      "Epoch: 1552 mean train loss:  6.41043995e-03, mean val. rec. loss:  6.20957468e-03\n",
      "Epoch: 1553 mean train loss:  6.39573478e-03, mean val. rec. loss:  6.19812244e-03\n",
      "Epoch: 1554 mean train loss:  6.38161041e-03, mean val. rec. loss:  6.19811895e-03\n",
      "Epoch: 1555 mean train loss:  6.37190792e-03, mean val. rec. loss:  6.18022768e-03\n",
      "Epoch: 1556 mean train loss:  6.35878725e-03, mean val. rec. loss:  6.16533233e-03\n",
      "Epoch: 1557 mean train loss:  6.34715136e-03, mean val. rec. loss:  6.15524780e-03\n",
      "Epoch: 1558 mean train loss:  6.33657019e-03, mean val. rec. loss:  6.14138580e-03\n",
      "Epoch: 1559 mean train loss:  6.32409946e-03, mean val. rec. loss:  6.13886903e-03\n",
      "Epoch: 1560 mean train loss:  6.30834568e-03, mean val. rec. loss:  6.12252049e-03\n",
      "Epoch: 1561 mean train loss:  6.29599329e-03, mean val. rec. loss:  6.10715936e-03\n",
      "Epoch: 1562 mean train loss:  6.29179655e-03, mean val. rec. loss:  6.10244797e-03\n",
      "Epoch: 1563 mean train loss:  6.27561490e-03, mean val. rec. loss:  6.08378736e-03\n",
      "Epoch: 1564 mean train loss:  6.26387912e-03, mean val. rec. loss:  6.07070399e-03\n",
      "Epoch: 1565 mean train loss:  6.24968928e-03, mean val. rec. loss:  6.06450162e-03\n",
      "Epoch: 1566 mean train loss:  6.24194356e-03, mean val. rec. loss:  6.04801992e-03\n",
      "Epoch: 1567 mean train loss:  6.22772632e-03, mean val. rec. loss:  6.03844247e-03\n",
      "Epoch: 1568 mean train loss:  6.21633383e-03, mean val. rec. loss:  6.03040137e-03\n",
      "Epoch: 1569 mean train loss:  6.21142332e-03, mean val. rec. loss:  6.03250527e-03\n",
      "Epoch: 1570 mean train loss:  6.19559043e-03, mean val. rec. loss:  6.01586307e-03\n",
      "Epoch: 1571 mean train loss:  6.17944663e-03, mean val. rec. loss:  5.99487816e-03\n",
      "Epoch: 1572 mean train loss:  6.16720053e-03, mean val. rec. loss:  5.97625477e-03\n",
      "Epoch: 1573 mean train loss:  6.15629128e-03, mean val. rec. loss:  5.96836776e-03\n",
      "Epoch: 1574 mean train loss:  6.15360037e-03, mean val. rec. loss:  5.95412371e-03\n",
      "Epoch: 1575 mean train loss:  6.13271630e-03, mean val. rec. loss:  5.94804810e-03\n",
      "Epoch: 1576 mean train loss:  6.12251928e-03, mean val. rec. loss:  5.93484088e-03\n",
      "Epoch: 1577 mean train loss:  6.11178104e-03, mean val. rec. loss:  5.91065186e-03\n",
      "Epoch: 1578 mean train loss:  6.10265729e-03, mean val. rec. loss:  5.90884860e-03\n",
      "Epoch: 1579 mean train loss:  6.08774845e-03, mean val. rec. loss:  5.90114245e-03\n",
      "Epoch: 1580 mean train loss:  6.07487334e-03, mean val. rec. loss:  5.88799802e-03\n",
      "Epoch: 1581 mean train loss:  6.06622841e-03, mean val. rec. loss:  5.87774893e-03\n",
      "Epoch: 1582 mean train loss:  6.05497114e-03, mean val. rec. loss:  5.85800264e-03\n",
      "Epoch: 1583 mean train loss:  6.04068873e-03, mean val. rec. loss:  5.85395301e-03\n",
      "Epoch: 1584 mean train loss:  6.03106436e-03, mean val. rec. loss:  5.85050990e-03\n",
      "Epoch: 1585 mean train loss:  6.01713263e-03, mean val. rec. loss:  5.83634725e-03\n",
      "Epoch: 1586 mean train loss:  6.00682008e-03, mean val. rec. loss:  5.82809332e-03\n",
      "Epoch: 1587 mean train loss:  5.99592604e-03, mean val. rec. loss:  5.81007585e-03\n",
      "Epoch: 1588 mean train loss:  5.98509699e-03, mean val. rec. loss:  5.79364009e-03\n",
      "Epoch: 1589 mean train loss:  5.97580831e-03, mean val. rec. loss:  5.77400719e-03\n",
      "Epoch: 1590 mean train loss:  5.96619493e-03, mean val. rec. loss:  5.77989439e-03\n",
      "Epoch: 1591 mean train loss:  5.95480928e-03, mean val. rec. loss:  5.77161719e-03\n",
      "Epoch: 1592 mean train loss:  5.94105989e-03, mean val. rec. loss:  5.76144370e-03\n",
      "Epoch: 1593 mean train loss:  5.93208051e-03, mean val. rec. loss:  5.73969062e-03\n",
      "Epoch: 1594 mean train loss:  5.92152449e-03, mean val. rec. loss:  5.72201508e-03\n",
      "Epoch: 1595 mean train loss:  5.91102159e-03, mean val. rec. loss:  5.72759698e-03\n",
      "Epoch: 1596 mean train loss:  5.89837652e-03, mean val. rec. loss:  5.70827345e-03\n",
      "Epoch: 1597 mean train loss:  5.88462241e-03, mean val. rec. loss:  5.70267584e-03\n",
      "Epoch: 1598 mean train loss:  5.87410175e-03, mean val. rec. loss:  5.69719629e-03\n",
      "Epoch: 1599 mean train loss:  5.86586368e-03, mean val. rec. loss:  5.67716854e-03\n",
      "Epoch: 1600 mean train loss:  5.85254841e-03, mean val. rec. loss:  5.66737128e-03\n",
      "Epoch: 1601 mean train loss:  5.83999505e-03, mean val. rec. loss:  5.64997720e-03\n",
      "Epoch: 1602 mean train loss:  5.82928462e-03, mean val. rec. loss:  5.64022588e-03\n",
      "Epoch: 1603 mean train loss:  5.81746762e-03, mean val. rec. loss:  5.62683954e-03\n",
      "Epoch: 1604 mean train loss:  5.81161680e-03, mean val. rec. loss:  5.63113864e-03\n",
      "Epoch: 1605 mean train loss:  5.79640442e-03, mean val. rec. loss:  5.61318049e-03\n",
      "Epoch: 1606 mean train loss:  5.78557488e-03, mean val. rec. loss:  5.60125491e-03\n",
      "Epoch: 1607 mean train loss:  5.77666027e-03, mean val. rec. loss:  5.59272824e-03\n",
      "Epoch: 1608 mean train loss:  5.76196274e-03, mean val. rec. loss:  5.57854350e-03\n",
      "Epoch: 1609 mean train loss:  5.75546118e-03, mean val. rec. loss:  5.57225739e-03\n",
      "Epoch: 1610 mean train loss:  5.74413175e-03, mean val. rec. loss:  5.55466210e-03\n",
      "Epoch: 1611 mean train loss:  5.73439891e-03, mean val. rec. loss:  5.54137811e-03\n",
      "Epoch: 1612 mean train loss:  5.71830281e-03, mean val. rec. loss:  5.53772101e-03\n",
      "Epoch: 1613 mean train loss:  5.70861342e-03, mean val. rec. loss:  5.53182858e-03\n",
      "Epoch: 1614 mean train loss:  5.70004458e-03, mean val. rec. loss:  5.51468919e-03\n",
      "Epoch: 1615 mean train loss:  5.68895793e-03, mean val. rec. loss:  5.50413481e-03\n",
      "Epoch: 1616 mean train loss:  5.67950472e-03, mean val. rec. loss:  5.48883182e-03\n",
      "Epoch: 1617 mean train loss:  5.66562606e-03, mean val. rec. loss:  5.48880158e-03\n",
      "Epoch: 1618 mean train loss:  5.65900398e-03, mean val. rec. loss:  5.47929392e-03\n",
      "Epoch: 1619 mean train loss:  5.64742099e-03, mean val. rec. loss:  5.45456409e-03\n",
      "Epoch: 1620 mean train loss:  5.63749158e-03, mean val. rec. loss:  5.45593064e-03\n",
      "Epoch: 1621 mean train loss:  5.62252488e-03, mean val. rec. loss:  5.44061254e-03\n",
      "Epoch: 1622 mean train loss:  5.61459372e-03, mean val. rec. loss:  5.43203645e-03\n",
      "Epoch: 1623 mean train loss:  5.60505919e-03, mean val. rec. loss:  5.42064702e-03\n",
      "Epoch: 1624 mean train loss:  5.59362962e-03, mean val. rec. loss:  5.40479916e-03\n",
      "Epoch: 1625 mean train loss:  5.58468266e-03, mean val. rec. loss:  5.39959233e-03\n",
      "Epoch: 1626 mean train loss:  5.57531637e-03, mean val. rec. loss:  5.38949036e-03\n",
      "Epoch: 1627 mean train loss:  5.56280798e-03, mean val. rec. loss:  5.37582258e-03\n",
      "Epoch: 1628 mean train loss:  5.55194159e-03, mean val. rec. loss:  5.36753492e-03\n",
      "Epoch: 1629 mean train loss:  5.54187372e-03, mean val. rec. loss:  5.35562911e-03\n",
      "Epoch: 1630 mean train loss:  5.53186323e-03, mean val. rec. loss:  5.35686132e-03\n",
      "Epoch: 1631 mean train loss:  5.52043039e-03, mean val. rec. loss:  5.32899484e-03\n",
      "Epoch: 1632 mean train loss:  5.51011995e-03, mean val. rec. loss:  5.32876747e-03\n",
      "Epoch: 1633 mean train loss:  5.50064266e-03, mean val. rec. loss:  5.31154726e-03\n",
      "Epoch: 1634 mean train loss:  5.49006979e-03, mean val. rec. loss:  5.30495120e-03\n",
      "Epoch: 1635 mean train loss:  5.48127617e-03, mean val. rec. loss:  5.29618205e-03\n",
      "Epoch: 1636 mean train loss:  5.46660299e-03, mean val. rec. loss:  5.29596457e-03\n",
      "Epoch: 1637 mean train loss:  5.46206219e-03, mean val. rec. loss:  5.27878273e-03\n",
      "Epoch: 1638 mean train loss:  5.45106011e-03, mean val. rec. loss:  5.25743205e-03\n",
      "Epoch: 1639 mean train loss:  5.44061416e-03, mean val. rec. loss:  5.25909866e-03\n",
      "Epoch: 1640 mean train loss:  5.43016967e-03, mean val. rec. loss:  5.23787475e-03\n",
      "Epoch: 1641 mean train loss:  5.41801716e-03, mean val. rec. loss:  5.22979701e-03\n",
      "Epoch: 1642 mean train loss:  5.40689547e-03, mean val. rec. loss:  5.22341670e-03\n",
      "Epoch: 1643 mean train loss:  5.39804105e-03, mean val. rec. loss:  5.22163670e-03\n",
      "Epoch: 1644 mean train loss:  5.38831076e-03, mean val. rec. loss:  5.20827247e-03\n",
      "Epoch: 1645 mean train loss:  5.37518864e-03, mean val. rec. loss:  5.19325501e-03\n",
      "Epoch: 1646 mean train loss:  5.36782459e-03, mean val. rec. loss:  5.18368919e-03\n",
      "Epoch: 1647 mean train loss:  5.35849457e-03, mean val. rec. loss:  5.18178998e-03\n",
      "Epoch: 1648 mean train loss:  5.35012783e-03, mean val. rec. loss:  5.16676554e-03\n",
      "Epoch: 1649 mean train loss:  5.33711939e-03, mean val. rec. loss:  5.14589519e-03\n",
      "Epoch: 1650 mean train loss:  5.32605895e-03, mean val. rec. loss:  5.14141525e-03\n",
      "Epoch: 1651 mean train loss:  5.31860706e-03, mean val. rec. loss:  5.13491979e-03\n",
      "Epoch: 1652 mean train loss:  5.30775890e-03, mean val. rec. loss:  5.12751428e-03\n",
      "Epoch: 1653 mean train loss:  5.29787217e-03, mean val. rec. loss:  5.11329697e-03\n",
      "Epoch: 1654 mean train loss:  5.28835573e-03, mean val. rec. loss:  5.10154933e-03\n",
      "Epoch: 1655 mean train loss:  5.27744132e-03, mean val. rec. loss:  5.09639542e-03\n",
      "Epoch: 1656 mean train loss:  5.26772085e-03, mean val. rec. loss:  5.08818510e-03\n",
      "Epoch: 1657 mean train loss:  5.25795562e-03, mean val. rec. loss:  5.07476621e-03\n",
      "Epoch: 1658 mean train loss:  5.25244943e-03, mean val. rec. loss:  5.06633549e-03\n",
      "Epoch: 1659 mean train loss:  5.24161376e-03, mean val. rec. loss:  5.04689100e-03\n",
      "Epoch: 1660 mean train loss:  5.22908698e-03, mean val. rec. loss:  5.04964387e-03\n",
      "Epoch: 1661 mean train loss:  5.21796480e-03, mean val. rec. loss:  5.03832305e-03\n",
      "Epoch: 1662 mean train loss:  5.20929396e-03, mean val. rec. loss:  5.03580919e-03\n",
      "Epoch: 1663 mean train loss:  5.19707548e-03, mean val. rec. loss:  5.01965546e-03\n",
      "Epoch: 1664 mean train loss:  5.18705891e-03, mean val. rec. loss:  5.00522474e-03\n",
      "Epoch: 1665 mean train loss:  5.18275825e-03, mean val. rec. loss:  4.98713517e-03\n",
      "Epoch: 1666 mean train loss:  5.17282146e-03, mean val. rec. loss:  4.98856044e-03\n",
      "Epoch: 1667 mean train loss:  5.16140108e-03, mean val. rec. loss:  4.97722975e-03\n",
      "Epoch: 1668 mean train loss:  5.15206164e-03, mean val. rec. loss:  4.97444316e-03\n",
      "Epoch: 1669 mean train loss:  5.14475559e-03, mean val. rec. loss:  4.97111169e-03\n",
      "Epoch: 1670 mean train loss:  5.13171604e-03, mean val. rec. loss:  4.95183003e-03\n",
      "Epoch: 1671 mean train loss:  5.12118514e-03, mean val. rec. loss:  4.93698004e-03\n",
      "Epoch: 1672 mean train loss:  5.11340264e-03, mean val. rec. loss:  4.92507423e-03\n",
      "Epoch: 1673 mean train loss:  5.10347894e-03, mean val. rec. loss:  4.91208216e-03\n",
      "Epoch: 1674 mean train loss:  5.09246816e-03, mean val. rec. loss:  4.91900037e-03\n",
      "Epoch: 1675 mean train loss:  5.08576795e-03, mean val. rec. loss:  4.90063574e-03\n",
      "Epoch: 1676 mean train loss:  5.07550944e-03, mean val. rec. loss:  4.90043396e-03\n",
      "Epoch: 1677 mean train loss:  5.06332498e-03, mean val. rec. loss:  4.89222306e-03\n",
      "Epoch: 1678 mean train loss:  5.05302925e-03, mean val. rec. loss:  4.87472022e-03\n",
      "Epoch: 1679 mean train loss:  5.04409885e-03, mean val. rec. loss:  4.87012020e-03\n",
      "Epoch: 1680 mean train loss:  5.03728190e-03, mean val. rec. loss:  4.85986209e-03\n",
      "Epoch: 1681 mean train loss:  5.02610701e-03, mean val. rec. loss:  4.83518519e-03\n",
      "Epoch: 1682 mean train loss:  5.01552770e-03, mean val. rec. loss:  4.84004893e-03\n",
      "Epoch: 1683 mean train loss:  5.01052015e-03, mean val. rec. loss:  4.82572085e-03\n",
      "Epoch: 1684 mean train loss:  4.99547183e-03, mean val. rec. loss:  4.82123858e-03\n",
      "Epoch: 1685 mean train loss:  4.99136733e-03, mean val. rec. loss:  4.80911179e-03\n",
      "Epoch: 1686 mean train loss:  4.97831057e-03, mean val. rec. loss:  4.79696117e-03\n",
      "Epoch: 1687 mean train loss:  4.97164038e-03, mean val. rec. loss:  4.78503995e-03\n",
      "Epoch: 1688 mean train loss:  4.96182250e-03, mean val. rec. loss:  4.79065151e-03\n",
      "Epoch: 1689 mean train loss:  4.95098519e-03, mean val. rec. loss:  4.77034115e-03\n",
      "Epoch: 1690 mean train loss:  4.94398444e-03, mean val. rec. loss:  4.76002373e-03\n",
      "Epoch: 1691 mean train loss:  4.93429152e-03, mean val. rec. loss:  4.75134355e-03\n",
      "Epoch: 1692 mean train loss:  4.92598995e-03, mean val. rec. loss:  4.75784598e-03\n",
      "Epoch: 1693 mean train loss:  4.91930903e-03, mean val. rec. loss:  4.73812644e-03\n",
      "Epoch: 1694 mean train loss:  4.90626769e-03, mean val. rec. loss:  4.72703300e-03\n",
      "Epoch: 1695 mean train loss:  4.89701933e-03, mean val. rec. loss:  4.71567933e-03\n",
      "Epoch: 1696 mean train loss:  4.88625021e-03, mean val. rec. loss:  4.71027741e-03\n",
      "Epoch: 1697 mean train loss:  4.87861645e-03, mean val. rec. loss:  4.69700069e-03\n",
      "Epoch: 1698 mean train loss:  4.86669239e-03, mean val. rec. loss:  4.68250484e-03\n",
      "Epoch: 1699 mean train loss:  4.85875952e-03, mean val. rec. loss:  4.67951821e-03\n",
      "Epoch: 1700 mean train loss:  4.85141002e-03, mean val. rec. loss:  4.68191403e-03\n",
      "Epoch: 1701 mean train loss:  4.84051020e-03, mean val. rec. loss:  4.66805144e-03\n",
      "Epoch: 1702 mean train loss:  4.83192473e-03, mean val. rec. loss:  4.65536059e-03\n",
      "Epoch: 1703 mean train loss:  4.82825191e-03, mean val. rec. loss:  4.64075455e-03\n",
      "Epoch: 1704 mean train loss:  4.81433222e-03, mean val. rec. loss:  4.63842386e-03\n",
      "Epoch: 1705 mean train loss:  4.81141480e-03, mean val. rec. loss:  4.62728274e-03\n",
      "Epoch: 1706 mean train loss:  4.79803024e-03, mean val. rec. loss:  4.62075385e-03\n",
      "Epoch: 1707 mean train loss:  4.78948311e-03, mean val. rec. loss:  4.61614713e-03\n",
      "Epoch: 1708 mean train loss:  4.78047915e-03, mean val. rec. loss:  4.60008208e-03\n",
      "Epoch: 1709 mean train loss:  4.76866051e-03, mean val. rec. loss:  4.58701180e-03\n",
      "Epoch: 1710 mean train loss:  4.76249525e-03, mean val. rec. loss:  4.58356636e-03\n",
      "Epoch: 1711 mean train loss:  4.75261050e-03, mean val. rec. loss:  4.57766231e-03\n",
      "Epoch: 1712 mean train loss:  4.74350693e-03, mean val. rec. loss:  4.56808427e-03\n",
      "Epoch: 1713 mean train loss:  4.73462194e-03, mean val. rec. loss:  4.56318332e-03\n",
      "Epoch: 1714 mean train loss:  4.72518347e-03, mean val. rec. loss:  4.55277809e-03\n",
      "Epoch: 1715 mean train loss:  4.71669517e-03, mean val. rec. loss:  4.53746639e-03\n",
      "Epoch: 1716 mean train loss:  4.70825096e-03, mean val. rec. loss:  4.52879667e-03\n",
      "Epoch: 1717 mean train loss:  4.70024806e-03, mean val. rec. loss:  4.53038594e-03\n",
      "Epoch: 1718 mean train loss:  4.68884494e-03, mean val. rec. loss:  4.51855427e-03\n",
      "Epoch: 1719 mean train loss:  4.68165465e-03, mean val. rec. loss:  4.50652402e-03\n",
      "Epoch: 1720 mean train loss:  4.67230578e-03, mean val. rec. loss:  4.49151644e-03\n",
      "Epoch: 1721 mean train loss:  4.66388993e-03, mean val. rec. loss:  4.48024069e-03\n",
      "Epoch: 1722 mean train loss:  4.65523583e-03, mean val. rec. loss:  4.48191282e-03\n",
      "Epoch: 1723 mean train loss:  4.64913511e-03, mean val. rec. loss:  4.46844828e-03\n",
      "Epoch: 1724 mean train loss:  4.63715585e-03, mean val. rec. loss:  4.45764937e-03\n",
      "Epoch: 1725 mean train loss:  4.63208487e-03, mean val. rec. loss:  4.46153414e-03\n",
      "Epoch: 1726 mean train loss:  4.61929083e-03, mean val. rec. loss:  4.44750640e-03\n",
      "Epoch: 1727 mean train loss:  4.61010231e-03, mean val. rec. loss:  4.44611282e-03\n",
      "Epoch: 1728 mean train loss:  4.60665195e-03, mean val. rec. loss:  4.43744747e-03\n",
      "Epoch: 1729 mean train loss:  4.59566874e-03, mean val. rec. loss:  4.41422143e-03\n",
      "Epoch: 1730 mean train loss:  4.58711138e-03, mean val. rec. loss:  4.40599744e-03\n",
      "Epoch: 1731 mean train loss:  4.57708174e-03, mean val. rec. loss:  4.39830408e-03\n",
      "Epoch: 1732 mean train loss:  4.57034869e-03, mean val. rec. loss:  4.39989131e-03\n",
      "Epoch: 1733 mean train loss:  4.56343311e-03, mean val. rec. loss:  4.39489760e-03\n",
      "Epoch: 1734 mean train loss:  4.55370615e-03, mean val. rec. loss:  4.37038119e-03\n",
      "Epoch: 1735 mean train loss:  4.54503904e-03, mean val. rec. loss:  4.35768976e-03\n",
      "Epoch: 1736 mean train loss:  4.53580934e-03, mean val. rec. loss:  4.36596231e-03\n",
      "Epoch: 1737 mean train loss:  4.52806574e-03, mean val. rec. loss:  4.35908161e-03\n",
      "Epoch: 1738 mean train loss:  4.51757223e-03, mean val. rec. loss:  4.34812802e-03\n",
      "Epoch: 1739 mean train loss:  4.50825318e-03, mean val. rec. loss:  4.33189724e-03\n",
      "Epoch: 1740 mean train loss:  4.50296072e-03, mean val. rec. loss:  4.31734701e-03\n",
      "Epoch: 1741 mean train loss:  4.49661352e-03, mean val. rec. loss:  4.31716180e-03\n",
      "Epoch: 1742 mean train loss:  4.48670230e-03, mean val. rec. loss:  4.31934886e-03\n",
      "Epoch: 1743 mean train loss:  4.47881189e-03, mean val. rec. loss:  4.31608950e-03\n",
      "Epoch: 1744 mean train loss:  4.46747707e-03, mean val. rec. loss:  4.28646541e-03\n",
      "Epoch: 1745 mean train loss:  4.46150584e-03, mean val. rec. loss:  4.28338342e-03\n",
      "Epoch: 1746 mean train loss:  4.45235642e-03, mean val. rec. loss:  4.28529484e-03\n",
      "Epoch: 1747 mean train loss:  4.44695377e-03, mean val. rec. loss:  4.27189920e-03\n",
      "Epoch: 1748 mean train loss:  4.43781070e-03, mean val. rec. loss:  4.26202693e-03\n",
      "Epoch: 1749 mean train loss:  4.42735247e-03, mean val. rec. loss:  4.25442021e-03\n",
      "Epoch: 1750 mean train loss:  4.42116838e-03, mean val. rec. loss:  4.24912557e-03\n",
      "Epoch: 1751 mean train loss:  4.40976702e-03, mean val. rec. loss:  4.23294916e-03\n",
      "Epoch: 1752 mean train loss:  4.40195690e-03, mean val. rec. loss:  4.22903619e-03\n",
      "Epoch: 1753 mean train loss:  4.39498340e-03, mean val. rec. loss:  4.22580795e-03\n",
      "Epoch: 1754 mean train loss:  4.38527470e-03, mean val. rec. loss:  4.21339942e-03\n",
      "Epoch: 1755 mean train loss:  4.37722614e-03, mean val. rec. loss:  4.20474541e-03\n",
      "Epoch: 1756 mean train loss:  4.36777511e-03, mean val. rec. loss:  4.20240542e-03\n",
      "Epoch: 1757 mean train loss:  4.35984042e-03, mean val. rec. loss:  4.18894960e-03\n",
      "Epoch: 1758 mean train loss:  4.35510784e-03, mean val. rec. loss:  4.19226449e-03\n",
      "Epoch: 1759 mean train loss:  4.34380893e-03, mean val. rec. loss:  4.17046082e-03\n",
      "Epoch: 1760 mean train loss:  4.33824773e-03, mean val. rec. loss:  4.15824855e-03\n",
      "Epoch: 1761 mean train loss:  4.33180087e-03, mean val. rec. loss:  4.15641622e-03\n",
      "Epoch: 1762 mean train loss:  4.32281762e-03, mean val. rec. loss:  4.15569224e-03\n",
      "Epoch: 1763 mean train loss:  4.31229705e-03, mean val. rec. loss:  4.14222915e-03\n",
      "Epoch: 1764 mean train loss:  4.30305903e-03, mean val. rec. loss:  4.12865092e-03\n",
      "Epoch: 1765 mean train loss:  4.29568004e-03, mean val. rec. loss:  4.11920722e-03\n",
      "Epoch: 1766 mean train loss:  4.29121973e-03, mean val. rec. loss:  4.12073833e-03\n",
      "Epoch: 1767 mean train loss:  4.28093797e-03, mean val. rec. loss:  4.10697198e-03\n",
      "Epoch: 1768 mean train loss:  4.27625429e-03, mean val. rec. loss:  4.10410427e-03\n",
      "Epoch: 1769 mean train loss:  4.26276350e-03, mean val. rec. loss:  4.09624693e-03\n",
      "Epoch: 1770 mean train loss:  4.25782459e-03, mean val. rec. loss:  4.09121920e-03\n",
      "Epoch: 1771 mean train loss:  4.24901231e-03, mean val. rec. loss:  4.07825505e-03\n",
      "Epoch: 1772 mean train loss:  4.23994737e-03, mean val. rec. loss:  4.07294994e-03\n",
      "Epoch: 1773 mean train loss:  4.23295445e-03, mean val. rec. loss:  4.05820288e-03\n",
      "Epoch: 1774 mean train loss:  4.22728666e-03, mean val. rec. loss:  4.05807495e-03\n",
      "Epoch: 1775 mean train loss:  4.21462602e-03, mean val. rec. loss:  4.04686957e-03\n",
      "Epoch: 1776 mean train loss:  4.21178559e-03, mean val. rec. loss:  4.03321138e-03\n",
      "Epoch: 1777 mean train loss:  4.20220575e-03, mean val. rec. loss:  4.03068066e-03\n",
      "Epoch: 1778 mean train loss:  4.19608646e-03, mean val. rec. loss:  4.02293612e-03\n",
      "Epoch: 1779 mean train loss:  4.19200367e-03, mean val. rec. loss:  4.02485278e-03\n",
      "Epoch: 1780 mean train loss:  4.17579119e-03, mean val. rec. loss:  4.01462520e-03\n",
      "Epoch: 1781 mean train loss:  4.17154926e-03, mean val. rec. loss:  3.99965368e-03\n",
      "Epoch: 1782 mean train loss:  4.16349903e-03, mean val. rec. loss:  3.99372781e-03\n",
      "Epoch: 1783 mean train loss:  4.15711656e-03, mean val. rec. loss:  3.98713263e-03\n",
      "Epoch: 1784 mean train loss:  4.14861544e-03, mean val. rec. loss:  3.98407157e-03\n",
      "Epoch: 1785 mean train loss:  4.14119852e-03, mean val. rec. loss:  3.96349401e-03\n",
      "Epoch: 1786 mean train loss:  4.13692054e-03, mean val. rec. loss:  3.96016633e-03\n",
      "Epoch: 1787 mean train loss:  4.12091889e-03, mean val. rec. loss:  3.95402298e-03\n",
      "Epoch: 1788 mean train loss:  4.11589935e-03, mean val. rec. loss:  3.94647151e-03\n",
      "Epoch: 1789 mean train loss:  4.10789390e-03, mean val. rec. loss:  3.93980771e-03\n",
      "Epoch: 1790 mean train loss:  4.10500924e-03, mean val. rec. loss:  3.93069488e-03\n",
      "Epoch: 1791 mean train loss:  4.09657409e-03, mean val. rec. loss:  3.93284065e-03\n",
      "Epoch: 1792 mean train loss:  4.08829659e-03, mean val. rec. loss:  3.92076213e-03\n",
      "Epoch: 1793 mean train loss:  4.07857676e-03, mean val. rec. loss:  3.91557100e-03\n",
      "Epoch: 1794 mean train loss:  4.07214528e-03, mean val. rec. loss:  3.90527626e-03\n",
      "Epoch: 1795 mean train loss:  4.06141155e-03, mean val. rec. loss:  3.90249520e-03\n",
      "Epoch: 1796 mean train loss:  4.05567703e-03, mean val. rec. loss:  3.88859976e-03\n",
      "Epoch: 1797 mean train loss:  4.04787759e-03, mean val. rec. loss:  3.88460857e-03\n",
      "Epoch: 1798 mean train loss:  4.04193695e-03, mean val. rec. loss:  3.86061000e-03\n",
      "Epoch: 1799 mean train loss:  4.03408088e-03, mean val. rec. loss:  3.85747770e-03\n",
      "Epoch: 1800 mean train loss:  4.02734022e-03, mean val. rec. loss:  3.85571747e-03\n",
      "Epoch: 1801 mean train loss:  4.01945429e-03, mean val. rec. loss:  3.85444455e-03\n",
      "Epoch: 1802 mean train loss:  4.01590115e-03, mean val. rec. loss:  3.84954359e-03\n",
      "Epoch: 1803 mean train loss:  4.00693818e-03, mean val. rec. loss:  3.84611386e-03\n",
      "Epoch: 1804 mean train loss:  3.99679983e-03, mean val. rec. loss:  3.82747592e-03\n",
      "Epoch: 1805 mean train loss:  3.98855233e-03, mean val. rec. loss:  3.81452805e-03\n",
      "Epoch: 1806 mean train loss:  3.98024586e-03, mean val. rec. loss:  3.81530320e-03\n",
      "Epoch: 1807 mean train loss:  3.97380315e-03, mean val. rec. loss:  3.80784738e-03\n",
      "Epoch: 1808 mean train loss:  3.96668112e-03, mean val. rec. loss:  3.80601708e-03\n",
      "Epoch: 1809 mean train loss:  3.95986209e-03, mean val. rec. loss:  3.79774570e-03\n",
      "Epoch: 1810 mean train loss:  3.95124747e-03, mean val. rec. loss:  3.78389824e-03\n",
      "Epoch: 1811 mean train loss:  3.94480822e-03, mean val. rec. loss:  3.77184472e-03\n",
      "Epoch: 1812 mean train loss:  3.93837890e-03, mean val. rec. loss:  3.76623200e-03\n",
      "Epoch: 1813 mean train loss:  3.92824983e-03, mean val. rec. loss:  3.76626253e-03\n",
      "Epoch: 1814 mean train loss:  3.92311868e-03, mean val. rec. loss:  3.75812693e-03\n",
      "Epoch: 1815 mean train loss:  3.91643348e-03, mean val. rec. loss:  3.74583849e-03\n",
      "Epoch: 1816 mean train loss:  3.91194568e-03, mean val. rec. loss:  3.75389704e-03\n",
      "Epoch: 1817 mean train loss:  3.90014138e-03, mean val. rec. loss:  3.74388317e-03\n",
      "Epoch: 1818 mean train loss:  3.89376668e-03, mean val. rec. loss:  3.72009510e-03\n",
      "Epoch: 1819 mean train loss:  3.88751355e-03, mean val. rec. loss:  3.71378835e-03\n",
      "Epoch: 1820 mean train loss:  3.87897895e-03, mean val. rec. loss:  3.71055690e-03\n",
      "Epoch: 1821 mean train loss:  3.87228492e-03, mean val. rec. loss:  3.71013967e-03\n",
      "Epoch: 1822 mean train loss:  3.86447096e-03, mean val. rec. loss:  3.70428969e-03\n",
      "Epoch: 1823 mean train loss:  3.86083319e-03, mean val. rec. loss:  3.69882467e-03\n",
      "Epoch: 1824 mean train loss:  3.85171721e-03, mean val. rec. loss:  3.68552266e-03\n",
      "Epoch: 1825 mean train loss:  3.84347958e-03, mean val. rec. loss:  3.67426872e-03\n",
      "Epoch: 1826 mean train loss:  3.83727313e-03, mean val. rec. loss:  3.67886904e-03\n",
      "Epoch: 1827 mean train loss:  3.83167469e-03, mean val. rec. loss:  3.66175407e-03\n",
      "Epoch: 1828 mean train loss:  3.82344165e-03, mean val. rec. loss:  3.66049453e-03\n",
      "Epoch: 1829 mean train loss:  3.81480116e-03, mean val. rec. loss:  3.65417527e-03\n",
      "Epoch: 1830 mean train loss:  3.81029540e-03, mean val. rec. loss:  3.65816733e-03\n",
      "Epoch: 1831 mean train loss:  3.80237217e-03, mean val. rec. loss:  3.63946165e-03\n",
      "Epoch: 1832 mean train loss:  3.79486347e-03, mean val. rec. loss:  3.62124094e-03\n",
      "Epoch: 1833 mean train loss:  3.78685846e-03, mean val. rec. loss:  3.63012349e-03\n",
      "Epoch: 1834 mean train loss:  3.77976192e-03, mean val. rec. loss:  3.62316574e-03\n",
      "Epoch: 1835 mean train loss:  3.77314832e-03, mean val. rec. loss:  3.60857510e-03\n",
      "Epoch: 1836 mean train loss:  3.76684220e-03, mean val. rec. loss:  3.60844223e-03\n",
      "Epoch: 1837 mean train loss:  3.75854865e-03, mean val. rec. loss:  3.59601480e-03\n",
      "Epoch: 1838 mean train loss:  3.75395931e-03, mean val. rec. loss:  3.58662140e-03\n",
      "Epoch: 1839 mean train loss:  3.74639831e-03, mean val. rec. loss:  3.58380806e-03\n",
      "Epoch: 1840 mean train loss:  3.73748138e-03, mean val. rec. loss:  3.57021559e-03\n",
      "Epoch: 1841 mean train loss:  3.73170560e-03, mean val. rec. loss:  3.56914328e-03\n",
      "Epoch: 1842 mean train loss:  3.72643243e-03, mean val. rec. loss:  3.56499393e-03\n",
      "Epoch: 1843 mean train loss:  3.71769014e-03, mean val. rec. loss:  3.55805159e-03\n",
      "Epoch: 1844 mean train loss:  3.71056093e-03, mean val. rec. loss:  3.54551746e-03\n",
      "Epoch: 1845 mean train loss:  3.70631583e-03, mean val. rec. loss:  3.54018008e-03\n",
      "Epoch: 1846 mean train loss:  3.69670860e-03, mean val. rec. loss:  3.53024326e-03\n",
      "Epoch: 1847 mean train loss:  3.69060296e-03, mean val. rec. loss:  3.52814488e-03\n",
      "Epoch: 1848 mean train loss:  3.68772645e-03, mean val. rec. loss:  3.51359146e-03\n",
      "Epoch: 1849 mean train loss:  3.67944356e-03, mean val. rec. loss:  3.51627164e-03\n",
      "Epoch: 1850 mean train loss:  3.67181665e-03, mean val. rec. loss:  3.50752953e-03\n",
      "Epoch: 1851 mean train loss:  3.66670949e-03, mean val. rec. loss:  3.51295471e-03\n",
      "Epoch: 1852 mean train loss:  3.65671126e-03, mean val. rec. loss:  3.50314611e-03\n",
      "Epoch: 1853 mean train loss:  3.64991972e-03, mean val. rec. loss:  3.48209637e-03\n",
      "Epoch: 1854 mean train loss:  3.64515134e-03, mean val. rec. loss:  3.47508018e-03\n",
      "Epoch: 1855 mean train loss:  3.63778633e-03, mean val. rec. loss:  3.48294508e-03\n",
      "Epoch: 1856 mean train loss:  3.63029854e-03, mean val. rec. loss:  3.47522090e-03\n",
      "Epoch: 1857 mean train loss:  3.62510697e-03, mean val. rec. loss:  3.46310807e-03\n",
      "Epoch: 1858 mean train loss:  3.61971488e-03, mean val. rec. loss:  3.45539727e-03\n",
      "Epoch: 1859 mean train loss:  3.60863004e-03, mean val. rec. loss:  3.45041606e-03\n",
      "Epoch: 1860 mean train loss:  3.60257076e-03, mean val. rec. loss:  3.45330529e-03\n",
      "Epoch: 1861 mean train loss:  3.59887960e-03, mean val. rec. loss:  3.43319207e-03\n",
      "Epoch: 1862 mean train loss:  3.59143235e-03, mean val. rec. loss:  3.42871910e-03\n",
      "Epoch: 1863 mean train loss:  3.58242311e-03, mean val. rec. loss:  3.41935390e-03\n",
      "Epoch: 1864 mean train loss:  3.57492343e-03, mean val. rec. loss:  3.41483820e-03\n",
      "Epoch: 1865 mean train loss:  3.57632912e-03, mean val. rec. loss:  3.40865094e-03\n",
      "Epoch: 1866 mean train loss:  3.56849923e-03, mean val. rec. loss:  3.40928217e-03\n",
      "Epoch: 1867 mean train loss:  3.55763912e-03, mean val. rec. loss:  3.40310248e-03\n",
      "Epoch: 1868 mean train loss:  3.54936320e-03, mean val. rec. loss:  3.39549402e-03\n",
      "Epoch: 1869 mean train loss:  3.54352683e-03, mean val. rec. loss:  3.37914432e-03\n",
      "Epoch: 1870 mean train loss:  3.53676747e-03, mean val. rec. loss:  3.37149603e-03\n",
      "Epoch: 1871 mean train loss:  3.53241225e-03, mean val. rec. loss:  3.37736083e-03\n",
      "Epoch: 1872 mean train loss:  3.52712301e-03, mean val. rec. loss:  3.37127593e-03\n",
      "Epoch: 1873 mean train loss:  3.52149137e-03, mean val. rec. loss:  3.36272658e-03\n",
      "Epoch: 1874 mean train loss:  3.51064808e-03, mean val. rec. loss:  3.36590860e-03\n",
      "Epoch: 1875 mean train loss:  3.50589261e-03, mean val. rec. loss:  3.34723955e-03\n",
      "Epoch: 1876 mean train loss:  3.49702984e-03, mean val. rec. loss:  3.33618711e-03\n",
      "Epoch: 1877 mean train loss:  3.49991007e-03, mean val. rec. loss:  3.33819796e-03\n",
      "Epoch: 1878 mean train loss:  3.48615159e-03, mean val. rec. loss:  3.31832636e-03\n",
      "Epoch: 1879 mean train loss:  3.48079459e-03, mean val. rec. loss:  3.32599151e-03\n",
      "Epoch: 1880 mean train loss:  3.47514789e-03, mean val. rec. loss:  3.32018137e-03\n",
      "Epoch: 1881 mean train loss:  3.46612728e-03, mean val. rec. loss:  3.30854044e-03\n",
      "Epoch: 1882 mean train loss:  3.45990602e-03, mean val. rec. loss:  3.29600253e-03\n",
      "Epoch: 1883 mean train loss:  3.45454945e-03, mean val. rec. loss:  3.30092325e-03\n",
      "Epoch: 1884 mean train loss:  3.44934794e-03, mean val. rec. loss:  3.29731819e-03\n",
      "Epoch: 1885 mean train loss:  3.44369096e-03, mean val. rec. loss:  3.28320352e-03\n",
      "Epoch: 1886 mean train loss:  3.43992071e-03, mean val. rec. loss:  3.27408227e-03\n",
      "Epoch: 1887 mean train loss:  3.43138766e-03, mean val. rec. loss:  3.27641353e-03\n",
      "Epoch: 1888 mean train loss:  3.42279542e-03, mean val. rec. loss:  3.26570854e-03\n",
      "Epoch: 1889 mean train loss:  3.41525343e-03, mean val. rec. loss:  3.25956635e-03\n",
      "Epoch: 1890 mean train loss:  3.41000078e-03, mean val. rec. loss:  3.25440837e-03\n",
      "Epoch: 1891 mean train loss:  3.40503891e-03, mean val. rec. loss:  3.24556188e-03\n",
      "Epoch: 1892 mean train loss:  3.39831288e-03, mean val. rec. loss:  3.24207166e-03\n",
      "Epoch: 1893 mean train loss:  3.39384996e-03, mean val. rec. loss:  3.23382820e-03\n",
      "Epoch: 1894 mean train loss:  3.38669909e-03, mean val. rec. loss:  3.22877343e-03\n",
      "Epoch: 1895 mean train loss:  3.37904976e-03, mean val. rec. loss:  3.22236259e-03\n",
      "Epoch: 1896 mean train loss:  3.37610302e-03, mean val. rec. loss:  3.22750313e-03\n",
      "Epoch: 1897 mean train loss:  3.36544238e-03, mean val. rec. loss:  3.21590144e-03\n",
      "Epoch: 1898 mean train loss:  3.36219241e-03, mean val. rec. loss:  3.21126856e-03\n",
      "Epoch: 1899 mean train loss:  3.35301006e-03, mean val. rec. loss:  3.20237148e-03\n",
      "Epoch: 1900 mean train loss:  3.34872587e-03, mean val. rec. loss:  3.19696287e-03\n",
      "Epoch: 1901 mean train loss:  3.34069740e-03, mean val. rec. loss:  3.19238407e-03\n",
      "Epoch: 1902 mean train loss:  3.33665488e-03, mean val. rec. loss:  3.17638502e-03\n",
      "Epoch: 1903 mean train loss:  3.32969807e-03, mean val. rec. loss:  3.17508011e-03\n",
      "Epoch: 1904 mean train loss:  3.32572378e-03, mean val. rec. loss:  3.17718140e-03\n",
      "Epoch: 1905 mean train loss:  3.31783486e-03, mean val. rec. loss:  3.16354967e-03\n",
      "Epoch: 1906 mean train loss:  3.31151182e-03, mean val. rec. loss:  3.16549569e-03\n",
      "Epoch: 1907 mean train loss:  3.30533136e-03, mean val. rec. loss:  3.15131356e-03\n",
      "Epoch: 1908 mean train loss:  3.30055218e-03, mean val. rec. loss:  3.14083681e-03\n",
      "Epoch: 1909 mean train loss:  3.29587651e-03, mean val. rec. loss:  3.14113454e-03\n",
      "Epoch: 1910 mean train loss:  3.29080080e-03, mean val. rec. loss:  3.13936152e-03\n",
      "Epoch: 1911 mean train loss:  3.28317752e-03, mean val. rec. loss:  3.12729958e-03\n",
      "Epoch: 1912 mean train loss:  3.27604640e-03, mean val. rec. loss:  3.12004758e-03\n",
      "Epoch: 1913 mean train loss:  3.27160007e-03, mean val. rec. loss:  3.11866330e-03\n",
      "Epoch: 1914 mean train loss:  3.26458868e-03, mean val. rec. loss:  3.12088873e-03\n",
      "Epoch: 1915 mean train loss:  3.25799954e-03, mean val. rec. loss:  3.11001394e-03\n",
      "Epoch: 1916 mean train loss:  3.25063795e-03, mean val. rec. loss:  3.09787553e-03\n",
      "Epoch: 1917 mean train loss:  3.24775371e-03, mean val. rec. loss:  3.09594841e-03\n",
      "Epoch: 1918 mean train loss:  3.24094867e-03, mean val. rec. loss:  3.08609678e-03\n",
      "Epoch: 1919 mean train loss:  3.23739735e-03, mean val. rec. loss:  3.08282986e-03\n",
      "Epoch: 1920 mean train loss:  3.23315014e-03, mean val. rec. loss:  3.08048987e-03\n",
      "Epoch: 1921 mean train loss:  3.22164371e-03, mean val. rec. loss:  3.07559967e-03\n",
      "Epoch: 1922 mean train loss:  3.21671036e-03, mean val. rec. loss:  3.06461730e-03\n",
      "Epoch: 1923 mean train loss:  3.21431409e-03, mean val. rec. loss:  3.05044448e-03\n",
      "Epoch: 1924 mean train loss:  3.20824029e-03, mean val. rec. loss:  3.06271722e-03\n",
      "Epoch: 1925 mean train loss:  3.19780531e-03, mean val. rec. loss:  3.05788313e-03\n",
      "Epoch: 1926 mean train loss:  3.19392071e-03, mean val. rec. loss:  3.04322824e-03\n",
      "Epoch: 1927 mean train loss:  3.18773371e-03, mean val. rec. loss:  3.03693108e-03\n",
      "Epoch: 1928 mean train loss:  3.18288299e-03, mean val. rec. loss:  3.02770777e-03\n",
      "Epoch: 1929 mean train loss:  3.17916281e-03, mean val. rec. loss:  3.02400240e-03\n",
      "Epoch: 1930 mean train loss:  3.17259335e-03, mean val. rec. loss:  3.03172745e-03\n",
      "Epoch: 1931 mean train loss:  3.16749251e-03, mean val. rec. loss:  3.01672715e-03\n",
      "Epoch: 1932 mean train loss:  3.15888981e-03, mean val. rec. loss:  3.00754977e-03\n",
      "Epoch: 1933 mean train loss:  3.15782126e-03, mean val. rec. loss:  3.01112489e-03\n",
      "Epoch: 1934 mean train loss:  3.14722340e-03, mean val. rec. loss:  3.00158349e-03\n",
      "Epoch: 1935 mean train loss:  3.14688803e-03, mean val. rec. loss:  2.99471297e-03\n",
      "Epoch: 1936 mean train loss:  3.13902094e-03, mean val. rec. loss:  2.99394654e-03\n",
      "Epoch: 1937 mean train loss:  3.13304759e-03, mean val. rec. loss:  2.97749101e-03\n",
      "Epoch: 1938 mean train loss:  3.12510089e-03, mean val. rec. loss:  2.97937684e-03\n",
      "Epoch: 1939 mean train loss:  3.11982786e-03, mean val. rec. loss:  2.97470791e-03\n",
      "Epoch: 1940 mean train loss:  3.11583683e-03, mean val. rec. loss:  2.97083622e-03\n",
      "Epoch: 1941 mean train loss:  3.10895378e-03, mean val. rec. loss:  2.95751211e-03\n",
      "Epoch: 1942 mean train loss:  3.10464091e-03, mean val. rec. loss:  2.95936131e-03\n",
      "Epoch: 1943 mean train loss:  3.10428570e-03, mean val. rec. loss:  2.95810670e-03\n",
      "Epoch: 1944 mean train loss:  3.09284645e-03, mean val. rec. loss:  2.94200676e-03\n",
      "Epoch: 1945 mean train loss:  3.08695870e-03, mean val. rec. loss:  2.94352450e-03\n",
      "Epoch: 1946 mean train loss:  3.08514122e-03, mean val. rec. loss:  2.93286661e-03\n",
      "Epoch: 1947 mean train loss:  3.07485769e-03, mean val. rec. loss:  2.92681194e-03\n",
      "Epoch: 1948 mean train loss:  3.06868546e-03, mean val. rec. loss:  2.91920406e-03\n",
      "Epoch: 1949 mean train loss:  3.06207626e-03, mean val. rec. loss:  2.91934275e-03\n",
      "Epoch: 1950 mean train loss:  3.05936206e-03, mean val. rec. loss:  2.91868187e-03\n",
      "Epoch: 1951 mean train loss:  3.05352497e-03, mean val. rec. loss:  2.90628410e-03\n",
      "Epoch: 1952 mean train loss:  3.04610405e-03, mean val. rec. loss:  2.90764716e-03\n",
      "Epoch: 1953 mean train loss:  3.04509954e-03, mean val. rec. loss:  2.89742074e-03\n",
      "Epoch: 1954 mean train loss:  3.03964839e-03, mean val. rec. loss:  2.89131955e-03\n",
      "Epoch: 1955 mean train loss:  3.03425057e-03, mean val. rec. loss:  2.88635056e-03\n",
      "Epoch: 1956 mean train loss:  3.02754308e-03, mean val. rec. loss:  2.88485696e-03\n",
      "Epoch: 1957 mean train loss:  3.02010809e-03, mean val. rec. loss:  2.87444301e-03\n",
      "Epoch: 1958 mean train loss:  3.01558078e-03, mean val. rec. loss:  2.86599456e-03\n",
      "Epoch: 1959 mean train loss:  3.00878601e-03, mean val. rec. loss:  2.86926816e-03\n",
      "Epoch: 1960 mean train loss:  3.00260050e-03, mean val. rec. loss:  2.85982010e-03\n",
      "Epoch: 1961 mean train loss:  3.00092928e-03, mean val. rec. loss:  2.85703845e-03\n",
      "Epoch: 1962 mean train loss:  2.99316383e-03, mean val. rec. loss:  2.85618538e-03\n",
      "Epoch: 1963 mean train loss:  2.98876567e-03, mean val. rec. loss:  2.84512130e-03\n",
      "Epoch: 1964 mean train loss:  2.98499079e-03, mean val. rec. loss:  2.84177181e-03\n",
      "Epoch: 1965 mean train loss:  2.97742594e-03, mean val. rec. loss:  2.83392290e-03\n",
      "Epoch: 1966 mean train loss:  2.97447821e-03, mean val. rec. loss:  2.83192164e-03\n",
      "Epoch: 1967 mean train loss:  2.96682578e-03, mean val. rec. loss:  2.81752377e-03\n",
      "Epoch: 1968 mean train loss:  2.96285929e-03, mean val. rec. loss:  2.82400498e-03\n",
      "Epoch: 1969 mean train loss:  2.95667237e-03, mean val. rec. loss:  2.81628283e-03\n",
      "Epoch: 1970 mean train loss:  2.95324385e-03, mean val. rec. loss:  2.80413628e-03\n",
      "Epoch: 1971 mean train loss:  2.94465740e-03, mean val. rec. loss:  2.81007086e-03\n",
      "Epoch: 1972 mean train loss:  2.93908585e-03, mean val. rec. loss:  2.80295116e-03\n",
      "Epoch: 1973 mean train loss:  2.93403569e-03, mean val. rec. loss:  2.78922290e-03\n",
      "Epoch: 1974 mean train loss:  2.92854595e-03, mean val. rec. loss:  2.78700241e-03\n",
      "Epoch: 1975 mean train loss:  2.92417483e-03, mean val. rec. loss:  2.78885830e-03\n",
      "Epoch: 1976 mean train loss:  2.91851983e-03, mean val. rec. loss:  2.77462936e-03\n",
      "Epoch: 1977 mean train loss:  2.91598676e-03, mean val. rec. loss:  2.77134413e-03\n",
      "Epoch: 1978 mean train loss:  2.90884559e-03, mean val. rec. loss:  2.77259989e-03\n",
      "Epoch: 1979 mean train loss:  2.90197643e-03, mean val. rec. loss:  2.76977260e-03\n",
      "Epoch: 1980 mean train loss:  2.90234585e-03, mean val. rec. loss:  2.76522404e-03\n",
      "Epoch: 1981 mean train loss:  2.89565851e-03, mean val. rec. loss:  2.75225290e-03\n",
      "Epoch: 1982 mean train loss:  2.88977910e-03, mean val. rec. loss:  2.74254520e-03\n",
      "Epoch: 1983 mean train loss:  2.88378203e-03, mean val. rec. loss:  2.74768457e-03\n",
      "Epoch: 1984 mean train loss:  2.87720030e-03, mean val. rec. loss:  2.73412815e-03\n",
      "Epoch: 1985 mean train loss:  2.87236637e-03, mean val. rec. loss:  2.73329688e-03\n",
      "Epoch: 1986 mean train loss:  2.86805248e-03, mean val. rec. loss:  2.72689505e-03\n",
      "Epoch: 1987 mean train loss:  2.86491726e-03, mean val. rec. loss:  2.73242724e-03\n",
      "Epoch: 1988 mean train loss:  2.85678145e-03, mean val. rec. loss:  2.71942993e-03\n",
      "Epoch: 1989 mean train loss:  2.85189380e-03, mean val. rec. loss:  2.70903169e-03\n",
      "Epoch: 1990 mean train loss:  2.85054165e-03, mean val. rec. loss:  2.70635762e-03\n",
      "Epoch: 1991 mean train loss:  2.84218373e-03, mean val. rec. loss:  2.70470875e-03\n",
      "Epoch: 1992 mean train loss:  2.83696845e-03, mean val. rec. loss:  2.70091964e-03\n",
      "Epoch: 1993 mean train loss:  2.83216115e-03, mean val. rec. loss:  2.69509379e-03\n",
      "Epoch: 1994 mean train loss:  2.82623123e-03, mean val. rec. loss:  2.68912490e-03\n",
      "Epoch: 1995 mean train loss:  2.82181790e-03, mean val. rec. loss:  2.68445131e-03\n",
      "Epoch: 1996 mean train loss:  2.81712462e-03, mean val. rec. loss:  2.67910434e-03\n",
      "Epoch: 1997 mean train loss:  2.81153301e-03, mean val. rec. loss:  2.67617877e-03\n",
      "Epoch: 1998 mean train loss:  2.80761551e-03, mean val. rec. loss:  2.67097776e-03\n",
      "Epoch: 1999 mean train loss:  2.80519821e-03, mean val. rec. loss:  2.66825978e-03\n",
      "Epoch: 2000 mean train loss:  2.79651639e-03, mean val. rec. loss:  2.66087811e-03\n",
      "Epoch: 2001 mean train loss:  2.79246222e-03, mean val. rec. loss:  2.66348443e-03\n",
      "Epoch: 2002 mean train loss:  2.78560391e-03, mean val. rec. loss:  2.65066827e-03\n",
      "Epoch: 2003 mean train loss:  2.78330744e-03, mean val. rec. loss:  2.64927905e-03\n",
      "Epoch: 2004 mean train loss:  2.77533711e-03, mean val. rec. loss:  2.64474415e-03\n",
      "Epoch: 2005 mean train loss:  2.77398485e-03, mean val. rec. loss:  2.63267930e-03\n",
      "Epoch: 2006 mean train loss:  2.76739875e-03, mean val. rec. loss:  2.63273658e-03\n",
      "Epoch: 2007 mean train loss:  2.76228786e-03, mean val. rec. loss:  2.63548130e-03\n",
      "Epoch: 2008 mean train loss:  2.75941632e-03, mean val. rec. loss:  2.62010737e-03\n",
      "Epoch: 2009 mean train loss:  2.75036427e-03, mean val. rec. loss:  2.61066512e-03\n",
      "Epoch: 2010 mean train loss:  2.74849577e-03, mean val. rec. loss:  2.60872899e-03\n",
      "Epoch: 2011 mean train loss:  2.74184950e-03, mean val. rec. loss:  2.61977213e-03\n",
      "Epoch: 2012 mean train loss:  2.73665928e-03, mean val. rec. loss:  2.61073868e-03\n",
      "Epoch: 2013 mean train loss:  2.73469302e-03, mean val. rec. loss:  2.59759775e-03\n",
      "Epoch: 2014 mean train loss:  2.72887823e-03, mean val. rec. loss:  2.59219321e-03\n",
      "Epoch: 2015 mean train loss:  2.72376501e-03, mean val. rec. loss:  2.58299781e-03\n",
      "Epoch: 2016 mean train loss:  2.71902303e-03, mean val. rec. loss:  2.58919059e-03\n",
      "Epoch: 2017 mean train loss:  2.71638355e-03, mean val. rec. loss:  2.57845739e-03\n",
      "Epoch: 2018 mean train loss:  2.70975236e-03, mean val. rec. loss:  2.58821976e-03\n",
      "Epoch: 2019 mean train loss:  2.70490419e-03, mean val. rec. loss:  2.58202495e-03\n",
      "Epoch: 2020 mean train loss:  2.70248286e-03, mean val. rec. loss:  2.55820955e-03\n",
      "Epoch: 2021 mean train loss:  2.69872749e-03, mean val. rec. loss:  2.56701679e-03\n",
      "Epoch: 2022 mean train loss:  2.68981385e-03, mean val. rec. loss:  2.56180182e-03\n",
      "Epoch: 2023 mean train loss:  2.68432760e-03, mean val. rec. loss:  2.55866429e-03\n",
      "Epoch: 2024 mean train loss:  2.68261984e-03, mean val. rec. loss:  2.54979192e-03\n",
      "Epoch: 2025 mean train loss:  2.67429582e-03, mean val. rec. loss:  2.53879035e-03\n",
      "Epoch: 2026 mean train loss:  2.67258399e-03, mean val. rec. loss:  2.53974403e-03\n",
      "Epoch: 2027 mean train loss:  2.66929104e-03, mean val. rec. loss:  2.54043602e-03\n",
      "Epoch: 2028 mean train loss:  2.66455572e-03, mean val. rec. loss:  2.53209603e-03\n",
      "Epoch: 2029 mean train loss:  2.65952770e-03, mean val. rec. loss:  2.52604630e-03\n",
      "Epoch: 2030 mean train loss:  2.65307210e-03, mean val. rec. loss:  2.52675051e-03\n",
      "Epoch: 2031 mean train loss:  2.64844198e-03, mean val. rec. loss:  2.51666133e-03\n",
      "Epoch: 2032 mean train loss:  2.64481960e-03, mean val. rec. loss:  2.51144869e-03\n",
      "Epoch: 2033 mean train loss:  2.63895124e-03, mean val. rec. loss:  2.51648862e-03\n",
      "Epoch: 2034 mean train loss:  2.63533395e-03, mean val. rec. loss:  2.51303591e-03\n",
      "Epoch: 2035 mean train loss:  2.63196472e-03, mean val. rec. loss:  2.49269096e-03\n",
      "Epoch: 2036 mean train loss:  2.62361071e-03, mean val. rec. loss:  2.49326055e-03\n",
      "Epoch: 2037 mean train loss:  2.61990797e-03, mean val. rec. loss:  2.49539410e-03\n",
      "Epoch: 2038 mean train loss:  2.61706529e-03, mean val. rec. loss:  2.49537520e-03\n",
      "Epoch: 2039 mean train loss:  2.61510752e-03, mean val. rec. loss:  2.47962504e-03\n",
      "Epoch: 2040 mean train loss:  2.60659703e-03, mean val. rec. loss:  2.48165363e-03\n",
      "Epoch: 2041 mean train loss:  2.60098979e-03, mean val. rec. loss:  2.48477401e-03\n",
      "Epoch: 2042 mean train loss:  2.59772280e-03, mean val. rec. loss:  2.46571157e-03\n",
      "Epoch: 2043 mean train loss:  2.59298394e-03, mean val. rec. loss:  2.45589425e-03\n",
      "Epoch: 2044 mean train loss:  2.58866866e-03, mean val. rec. loss:  2.46516030e-03\n",
      "Epoch: 2045 mean train loss:  2.58279769e-03, mean val. rec. loss:  2.46245512e-03\n",
      "Epoch: 2046 mean train loss:  2.57967845e-03, mean val. rec. loss:  2.45617076e-03\n",
      "Epoch: 2047 mean train loss:  2.57398601e-03, mean val. rec. loss:  2.44398640e-03\n",
      "Epoch: 2048 mean train loss:  2.56997825e-03, mean val. rec. loss:  2.44115446e-03\n",
      "Epoch: 2049 mean train loss:  2.56603717e-03, mean val. rec. loss:  2.43493493e-03\n",
      "Epoch: 2050 mean train loss:  2.56128236e-03, mean val. rec. loss:  2.43791778e-03\n",
      "Epoch: 2051 mean train loss:  2.55638565e-03, mean val. rec. loss:  2.43375098e-03\n",
      "Epoch: 2052 mean train loss:  2.55126720e-03, mean val. rec. loss:  2.42478717e-03\n",
      "Epoch: 2053 mean train loss:  2.54740924e-03, mean val. rec. loss:  2.42125174e-03\n",
      "Epoch: 2054 mean train loss:  2.54395015e-03, mean val. rec. loss:  2.42608117e-03\n",
      "Epoch: 2055 mean train loss:  2.53798382e-03, mean val. rec. loss:  2.41701981e-03\n",
      "Epoch: 2056 mean train loss:  2.53454145e-03, mean val. rec. loss:  2.40688818e-03\n",
      "Epoch: 2057 mean train loss:  2.52947453e-03, mean val. rec. loss:  2.40971083e-03\n",
      "Epoch: 2058 mean train loss:  2.52937481e-03, mean val. rec. loss:  2.40408313e-03\n",
      "Epoch: 2059 mean train loss:  2.52230550e-03, mean val. rec. loss:  2.39737848e-03\n",
      "Epoch: 2060 mean train loss:  2.51973244e-03, mean val. rec. loss:  2.39588066e-03\n",
      "Epoch: 2061 mean train loss:  2.51440342e-03, mean val. rec. loss:  2.38855917e-03\n",
      "Epoch: 2062 mean train loss:  2.50712302e-03, mean val. rec. loss:  2.38590706e-03\n",
      "Epoch: 2063 mean train loss:  2.50374216e-03, mean val. rec. loss:  2.37891267e-03\n",
      "Epoch: 2064 mean train loss:  2.50017846e-03, mean val. rec. loss:  2.37825280e-03\n",
      "Epoch: 2065 mean train loss:  2.49457650e-03, mean val. rec. loss:  2.37180503e-03\n",
      "Epoch: 2066 mean train loss:  2.49058497e-03, mean val. rec. loss:  2.36745157e-03\n",
      "Epoch: 2067 mean train loss:  2.48521676e-03, mean val. rec. loss:  2.36471804e-03\n",
      "Epoch: 2068 mean train loss:  2.48379849e-03, mean val. rec. loss:  2.36353932e-03\n",
      "Epoch: 2069 mean train loss:  2.48001509e-03, mean val. rec. loss:  2.36158676e-03\n",
      "Epoch: 2070 mean train loss:  2.47725779e-03, mean val. rec. loss:  2.35311912e-03\n",
      "Epoch: 2071 mean train loss:  2.47141734e-03, mean val. rec. loss:  2.34178130e-03\n",
      "Epoch: 2072 mean train loss:  2.46416413e-03, mean val. rec. loss:  2.34783248e-03\n",
      "Epoch: 2073 mean train loss:  2.46042796e-03, mean val. rec. loss:  2.34431726e-03\n",
      "Epoch: 2074 mean train loss:  2.45999284e-03, mean val. rec. loss:  2.33541262e-03\n",
      "Epoch: 2075 mean train loss:  2.45635423e-03, mean val. rec. loss:  2.33000168e-03\n",
      "Epoch: 2076 mean train loss:  2.44549502e-03, mean val. rec. loss:  2.32732092e-03\n",
      "Epoch: 2077 mean train loss:  2.44339549e-03, mean val. rec. loss:  2.32408803e-03\n",
      "Epoch: 2078 mean train loss:  2.43865714e-03, mean val. rec. loss:  2.32882209e-03\n",
      "Epoch: 2079 mean train loss:  2.43375073e-03, mean val. rec. loss:  2.31805473e-03\n",
      "Epoch: 2080 mean train loss:  2.43483908e-03, mean val. rec. loss:  2.30926362e-03\n",
      "Epoch: 2081 mean train loss:  2.42863046e-03, mean val. rec. loss:  2.30756591e-03\n",
      "Epoch: 2082 mean train loss:  2.42322247e-03, mean val. rec. loss:  2.30432574e-03\n",
      "Epoch: 2083 mean train loss:  2.42131682e-03, mean val. rec. loss:  2.29870982e-03\n",
      "Epoch: 2084 mean train loss:  2.41405167e-03, mean val. rec. loss:  2.29426273e-03\n",
      "Epoch: 2085 mean train loss:  2.40931788e-03, mean val. rec. loss:  2.29347915e-03\n",
      "Epoch: 2086 mean train loss:  2.40422025e-03, mean val. rec. loss:  2.28740122e-03\n",
      "Epoch: 2087 mean train loss:  2.40074240e-03, mean val. rec. loss:  2.28874567e-03\n",
      "Epoch: 2088 mean train loss:  2.39901900e-03, mean val. rec. loss:  2.28358885e-03\n",
      "Epoch: 2089 mean train loss:  2.39600795e-03, mean val. rec. loss:  2.27562131e-03\n",
      "Epoch: 2090 mean train loss:  2.38852196e-03, mean val. rec. loss:  2.26802855e-03\n",
      "Epoch: 2091 mean train loss:  2.38769042e-03, mean val. rec. loss:  2.27124328e-03\n",
      "Epoch: 2092 mean train loss:  2.37940323e-03, mean val. rec. loss:  2.26773634e-03\n",
      "Epoch: 2093 mean train loss:  2.37984028e-03, mean val. rec. loss:  2.25562700e-03\n",
      "Epoch: 2094 mean train loss:  2.37293649e-03, mean val. rec. loss:  2.26498813e-03\n",
      "Epoch: 2095 mean train loss:  2.36957852e-03, mean val. rec. loss:  2.25483950e-03\n",
      "Epoch: 2096 mean train loss:  2.36977282e-03, mean val. rec. loss:  2.25110403e-03\n",
      "Epoch: 2097 mean train loss:  2.36144465e-03, mean val. rec. loss:  2.24639526e-03\n",
      "Epoch: 2098 mean train loss:  2.35775635e-03, mean val. rec. loss:  2.24053772e-03\n",
      "Epoch: 2099 mean train loss:  2.35828492e-03, mean val. rec. loss:  2.23924852e-03\n",
      "Epoch: 2100 mean train loss:  2.34782308e-03, mean val. rec. loss:  2.23726543e-03\n",
      "Epoch: 2101 mean train loss:  2.34695079e-03, mean val. rec. loss:  2.22955593e-03\n",
      "Epoch: 2102 mean train loss:  2.34091326e-03, mean val. rec. loss:  2.22809489e-03\n",
      "Epoch: 2103 mean train loss:  2.33650630e-03, mean val. rec. loss:  2.22131581e-03\n",
      "Epoch: 2104 mean train loss:  2.33263995e-03, mean val. rec. loss:  2.21950659e-03\n",
      "Epoch: 2105 mean train loss:  2.32917787e-03, mean val. rec. loss:  2.22066583e-03\n",
      "Epoch: 2106 mean train loss:  2.32527128e-03, mean val. rec. loss:  2.20714604e-03\n",
      "Epoch: 2107 mean train loss:  2.32302251e-03, mean val. rec. loss:  2.21242163e-03\n",
      "Epoch: 2108 mean train loss:  2.31839715e-03, mean val. rec. loss:  2.20589740e-03\n",
      "Epoch: 2109 mean train loss:  2.31340148e-03, mean val. rec. loss:  2.20135741e-03\n",
      "Epoch: 2110 mean train loss:  2.31178341e-03, mean val. rec. loss:  2.19199716e-03\n",
      "Epoch: 2111 mean train loss:  2.30526538e-03, mean val. rec. loss:  2.19375389e-03\n",
      "Epoch: 2112 mean train loss:  2.30087026e-03, mean val. rec. loss:  2.18579769e-03\n",
      "Epoch: 2113 mean train loss:  2.29980331e-03, mean val. rec. loss:  2.19142902e-03\n",
      "Epoch: 2114 mean train loss:  2.29305487e-03, mean val. rec. loss:  2.17977152e-03\n",
      "Epoch: 2115 mean train loss:  2.29101121e-03, mean val. rec. loss:  2.17694553e-03\n",
      "Epoch: 2116 mean train loss:  2.28528573e-03, mean val. rec. loss:  2.17709221e-03\n",
      "Epoch: 2117 mean train loss:  2.28189625e-03, mean val. rec. loss:  2.17223051e-03\n",
      "Epoch: 2118 mean train loss:  2.27820377e-03, mean val. rec. loss:  2.16956575e-03\n",
      "Epoch: 2119 mean train loss:  2.27447087e-03, mean val. rec. loss:  2.16307233e-03\n",
      "Epoch: 2120 mean train loss:  2.27084372e-03, mean val. rec. loss:  2.15522952e-03\n",
      "Epoch: 2121 mean train loss:  2.26599497e-03, mean val. rec. loss:  2.15966934e-03\n",
      "Epoch: 2122 mean train loss:  2.26293013e-03, mean val. rec. loss:  2.15429678e-03\n",
      "Epoch: 2123 mean train loss:  2.25799104e-03, mean val. rec. loss:  2.14499148e-03\n",
      "Epoch: 2124 mean train loss:  2.25356028e-03, mean val. rec. loss:  2.14567068e-03\n",
      "Epoch: 2125 mean train loss:  2.25104404e-03, mean val. rec. loss:  2.14276038e-03\n",
      "Epoch: 2126 mean train loss:  2.24886396e-03, mean val. rec. loss:  2.14048798e-03\n",
      "Epoch: 2127 mean train loss:  2.24129370e-03, mean val. rec. loss:  2.13517619e-03\n",
      "Epoch: 2128 mean train loss:  2.24144475e-03, mean val. rec. loss:  2.12676293e-03\n",
      "Epoch: 2129 mean train loss:  2.24055101e-03, mean val. rec. loss:  2.13078275e-03\n",
      "Epoch: 2130 mean train loss:  2.23484960e-03, mean val. rec. loss:  2.12251660e-03\n",
      "Epoch: 2131 mean train loss:  2.22994846e-03, mean val. rec. loss:  2.12446393e-03\n",
      "Epoch: 2132 mean train loss:  2.22541874e-03, mean val. rec. loss:  2.11791411e-03\n",
      "Epoch: 2133 mean train loss:  2.22124571e-03, mean val. rec. loss:  2.11774198e-03\n",
      "Epoch: 2134 mean train loss:  2.21743503e-03, mean val. rec. loss:  2.11056413e-03\n",
      "Epoch: 2135 mean train loss:  2.21459891e-03, mean val. rec. loss:  2.10603330e-03\n",
      "Epoch: 2136 mean train loss:  2.21068581e-03, mean val. rec. loss:  2.10891846e-03\n",
      "Epoch: 2137 mean train loss:  2.20571831e-03, mean val. rec. loss:  2.09748338e-03\n",
      "Epoch: 2138 mean train loss:  2.20012334e-03, mean val. rec. loss:  2.09610055e-03\n",
      "Epoch: 2139 mean train loss:  2.19723177e-03, mean val. rec. loss:  2.09235447e-03\n",
      "Epoch: 2140 mean train loss:  2.19363645e-03, mean val. rec. loss:  2.09479550e-03\n",
      "Epoch: 2141 mean train loss:  2.19273656e-03, mean val. rec. loss:  2.08276714e-03\n",
      "Epoch: 2142 mean train loss:  2.18653489e-03, mean val. rec. loss:  2.08038034e-03\n",
      "Epoch: 2143 mean train loss:  2.18537637e-03, mean val. rec. loss:  2.08322013e-03\n",
      "Epoch: 2144 mean train loss:  2.17897428e-03, mean val. rec. loss:  2.07889313e-03\n",
      "Epoch: 2145 mean train loss:  2.17659466e-03, mean val. rec. loss:  2.07586725e-03\n",
      "Epoch: 2146 mean train loss:  2.17239607e-03, mean val. rec. loss:  2.06769647e-03\n",
      "Epoch: 2147 mean train loss:  2.16887854e-03, mean val. rec. loss:  2.06343764e-03\n",
      "Epoch: 2148 mean train loss:  2.16610819e-03, mean val. rec. loss:  2.06068870e-03\n",
      "Epoch: 2149 mean train loss:  2.16137683e-03, mean val. rec. loss:  2.05775994e-03\n",
      "Epoch: 2150 mean train loss:  2.15987747e-03, mean val. rec. loss:  2.05123105e-03\n",
      "Epoch: 2151 mean train loss:  2.15548381e-03, mean val. rec. loss:  2.05483756e-03\n",
      "Epoch: 2152 mean train loss:  2.15290072e-03, mean val. rec. loss:  2.04564507e-03\n",
      "Epoch: 2153 mean train loss:  2.14881635e-03, mean val. rec. loss:  2.04315011e-03\n",
      "Epoch: 2154 mean train loss:  2.14205027e-03, mean val. rec. loss:  2.04061837e-03\n",
      "Epoch: 2155 mean train loss:  2.14085483e-03, mean val. rec. loss:  2.03749000e-03\n",
      "Epoch: 2156 mean train loss:  2.13679021e-03, mean val. rec. loss:  2.03930023e-03\n",
      "Epoch: 2157 mean train loss:  2.13467360e-03, mean val. rec. loss:  2.02919986e-03\n",
      "Epoch: 2158 mean train loss:  2.13150852e-03, mean val. rec. loss:  2.02718508e-03\n",
      "Epoch: 2159 mean train loss:  2.12664278e-03, mean val. rec. loss:  2.02997138e-03\n",
      "Epoch: 2160 mean train loss:  2.12264414e-03, mean val. rec. loss:  2.01737910e-03\n",
      "Epoch: 2161 mean train loss:  2.11825445e-03, mean val. rec. loss:  2.01209057e-03\n",
      "Epoch: 2162 mean train loss:  2.11489078e-03, mean val. rec. loss:  2.01819539e-03\n",
      "Epoch: 2163 mean train loss:  2.11223005e-03, mean val. rec. loss:  2.01577762e-03\n",
      "Epoch: 2164 mean train loss:  2.10727065e-03, mean val. rec. loss:  2.00059995e-03\n",
      "Epoch: 2165 mean train loss:  2.10487672e-03, mean val. rec. loss:  2.00328274e-03\n",
      "Epoch: 2166 mean train loss:  2.10021223e-03, mean val. rec. loss:  1.99858677e-03\n",
      "Epoch: 2167 mean train loss:  2.09762658e-03, mean val. rec. loss:  2.00659226e-03\n",
      "Epoch: 2168 mean train loss:  2.09384653e-03, mean val. rec. loss:  1.99392220e-03\n",
      "Epoch: 2169 mean train loss:  2.09216918e-03, mean val. rec. loss:  1.98991313e-03\n",
      "Epoch: 2170 mean train loss:  2.08857852e-03, mean val. rec. loss:  1.99052226e-03\n",
      "Epoch: 2171 mean train loss:  2.08253580e-03, mean val. rec. loss:  1.98666177e-03\n",
      "Epoch: 2172 mean train loss:  2.07832811e-03, mean val. rec. loss:  1.97976740e-03\n",
      "Epoch: 2173 mean train loss:  2.07958575e-03, mean val. rec. loss:  1.97281256e-03\n",
      "Epoch: 2174 mean train loss:  2.07387537e-03, mean val. rec. loss:  1.98092751e-03\n",
      "Epoch: 2175 mean train loss:  2.07135349e-03, mean val. rec. loss:  1.97170769e-03\n",
      "Epoch: 2176 mean train loss:  2.06553527e-03, mean val. rec. loss:  1.96987536e-03\n",
      "Epoch: 2177 mean train loss:  2.06339992e-03, mean val. rec. loss:  1.96571699e-03\n",
      "Epoch: 2178 mean train loss:  2.05966183e-03, mean val. rec. loss:  1.95991441e-03\n",
      "Epoch: 2179 mean train loss:  2.05584674e-03, mean val. rec. loss:  1.95066028e-03\n",
      "Epoch: 2180 mean train loss:  2.05160414e-03, mean val. rec. loss:  1.96258469e-03\n",
      "Epoch: 2181 mean train loss:  2.05017628e-03, mean val. rec. loss:  1.95711284e-03\n",
      "Epoch: 2182 mean train loss:  2.04631084e-03, mean val. rec. loss:  1.95173636e-03\n",
      "Epoch: 2183 mean train loss:  2.04162550e-03, mean val. rec. loss:  1.94902682e-03\n",
      "Epoch: 2184 mean train loss:  2.03882482e-03, mean val. rec. loss:  1.94038298e-03\n",
      "Epoch: 2185 mean train loss:  2.03677808e-03, mean val. rec. loss:  1.93469379e-03\n",
      "Epoch: 2186 mean train loss:  2.03369359e-03, mean val. rec. loss:  1.93987387e-03\n",
      "Epoch: 2187 mean train loss:  2.03194580e-03, mean val. rec. loss:  1.93429865e-03\n",
      "Epoch: 2188 mean train loss:  2.02586432e-03, mean val. rec. loss:  1.93063166e-03\n",
      "Epoch: 2189 mean train loss:  2.02512337e-03, mean val. rec. loss:  1.92756973e-03\n",
      "Epoch: 2190 mean train loss:  2.01957318e-03, mean val. rec. loss:  1.92488476e-03\n",
      "Epoch: 2191 mean train loss:  2.01482752e-03, mean val. rec. loss:  1.92001898e-03\n",
      "Epoch: 2192 mean train loss:  2.01460565e-03, mean val. rec. loss:  1.92090302e-03\n",
      "Epoch: 2193 mean train loss:  2.00872600e-03, mean val. rec. loss:  1.91717758e-03\n",
      "Epoch: 2194 mean train loss:  2.00570225e-03, mean val. rec. loss:  1.91288416e-03\n",
      "Epoch: 2195 mean train loss:  2.00086719e-03, mean val. rec. loss:  1.90614113e-03\n",
      "Epoch: 2196 mean train loss:  2.00043963e-03, mean val. rec. loss:  1.90954804e-03\n",
      "Epoch: 2197 mean train loss:  1.99595938e-03, mean val. rec. loss:  1.90567927e-03\n",
      "Epoch: 2198 mean train loss:  1.99256231e-03, mean val. rec. loss:  1.89506659e-03\n",
      "Epoch: 2199 mean train loss:  1.99395129e-03, mean val. rec. loss:  1.90167805e-03\n",
      "Epoch: 2200 mean train loss:  1.98652697e-03, mean val. rec. loss:  1.89218550e-03\n",
      "Epoch: 2201 mean train loss:  1.98334858e-03, mean val. rec. loss:  1.88878339e-03\n",
      "Epoch: 2202 mean train loss:  1.97909814e-03, mean val. rec. loss:  1.88990381e-03\n",
      "Epoch: 2203 mean train loss:  1.98057066e-03, mean val. rec. loss:  1.88737221e-03\n",
      "Epoch: 2204 mean train loss:  1.97041510e-03, mean val. rec. loss:  1.87856250e-03\n",
      "Epoch: 2205 mean train loss:  1.97254090e-03, mean val. rec. loss:  1.87558183e-03\n",
      "Epoch: 2206 mean train loss:  1.96534377e-03, mean val. rec. loss:  1.88669853e-03\n",
      "Epoch: 2207 mean train loss:  1.96551221e-03, mean val. rec. loss:  1.87247672e-03\n",
      "Epoch: 2208 mean train loss:  1.96152795e-03, mean val. rec. loss:  1.86829887e-03\n",
      "Epoch: 2209 mean train loss:  1.96081057e-03, mean val. rec. loss:  1.86593562e-03\n",
      "Epoch: 2210 mean train loss:  1.95402102e-03, mean val. rec. loss:  1.86190460e-03\n",
      "Epoch: 2211 mean train loss:  1.94961257e-03, mean val. rec. loss:  1.85436069e-03\n",
      "Epoch: 2212 mean train loss:  1.94654338e-03, mean val. rec. loss:  1.86064883e-03\n",
      "Epoch: 2213 mean train loss:  1.94494224e-03, mean val. rec. loss:  1.85226682e-03\n",
      "Epoch: 2214 mean train loss:  1.93941891e-03, mean val. rec. loss:  1.85214107e-03\n",
      "Epoch: 2215 mean train loss:  1.93685977e-03, mean val. rec. loss:  1.85357318e-03\n",
      "Epoch: 2216 mean train loss:  1.93448418e-03, mean val. rec. loss:  1.84646176e-03\n",
      "Epoch: 2217 mean train loss:  1.92950756e-03, mean val. rec. loss:  1.84519611e-03\n",
      "Epoch: 2218 mean train loss:  1.92867540e-03, mean val. rec. loss:  1.83721752e-03\n",
      "Epoch: 2219 mean train loss:  1.92522024e-03, mean val. rec. loss:  1.83435286e-03\n",
      "Epoch: 2220 mean train loss:  1.92147504e-03, mean val. rec. loss:  1.83498162e-03\n",
      "Epoch: 2221 mean train loss:  1.91913609e-03, mean val. rec. loss:  1.83161701e-03\n",
      "Epoch: 2222 mean train loss:  1.91857459e-03, mean val. rec. loss:  1.83552445e-03\n",
      "Epoch: 2223 mean train loss:  1.91241206e-03, mean val. rec. loss:  1.82246900e-03\n",
      "Epoch: 2224 mean train loss:  1.90903309e-03, mean val. rec. loss:  1.81936375e-03\n",
      "Epoch: 2225 mean train loss:  1.90627179e-03, mean val. rec. loss:  1.82134756e-03\n",
      "Epoch: 2226 mean train loss:  1.90377281e-03, mean val. rec. loss:  1.81537401e-03\n",
      "Epoch: 2227 mean train loss:  1.90053686e-03, mean val. rec. loss:  1.81737513e-03\n",
      "Epoch: 2228 mean train loss:  1.89944572e-03, mean val. rec. loss:  1.81061233e-03\n",
      "Epoch: 2229 mean train loss:  1.89719048e-03, mean val. rec. loss:  1.81000974e-03\n",
      "Epoch: 2230 mean train loss:  1.89248710e-03, mean val. rec. loss:  1.81129386e-03\n",
      "Epoch: 2231 mean train loss:  1.88880641e-03, mean val. rec. loss:  1.80138088e-03\n",
      "Epoch: 2232 mean train loss:  1.88607030e-03, mean val. rec. loss:  1.79586890e-03\n",
      "Epoch: 2233 mean train loss:  1.88066309e-03, mean val. rec. loss:  1.80340017e-03\n",
      "Epoch: 2234 mean train loss:  1.87835428e-03, mean val. rec. loss:  1.79310005e-03\n",
      "Epoch: 2235 mean train loss:  1.87629410e-03, mean val. rec. loss:  1.79133735e-03\n",
      "Epoch: 2236 mean train loss:  1.87216226e-03, mean val. rec. loss:  1.78711487e-03\n",
      "Epoch: 2237 mean train loss:  1.87076544e-03, mean val. rec. loss:  1.78036603e-03\n",
      "Epoch: 2238 mean train loss:  1.86660444e-03, mean val. rec. loss:  1.78482940e-03\n",
      "Epoch: 2239 mean train loss:  1.86251470e-03, mean val. rec. loss:  1.78176688e-03\n",
      "Epoch: 2240 mean train loss:  1.85957599e-03, mean val. rec. loss:  1.77616753e-03\n",
      "Epoch: 2241 mean train loss:  1.85966263e-03, mean val. rec. loss:  1.77676242e-03\n",
      "Epoch: 2242 mean train loss:  1.85551608e-03, mean val. rec. loss:  1.77473978e-03\n",
      "Epoch: 2243 mean train loss:  1.85236038e-03, mean val. rec. loss:  1.76827355e-03\n",
      "Epoch: 2244 mean train loss:  1.85109115e-03, mean val. rec. loss:  1.76203920e-03\n",
      "Epoch: 2245 mean train loss:  1.84808795e-03, mean val. rec. loss:  1.77135264e-03\n",
      "Epoch: 2246 mean train loss:  1.84178512e-03, mean val. rec. loss:  1.76138035e-03\n",
      "Epoch: 2247 mean train loss:  1.84252429e-03, mean val. rec. loss:  1.75267080e-03\n",
      "Epoch: 2248 mean train loss:  1.83572689e-03, mean val. rec. loss:  1.75941892e-03\n",
      "Epoch: 2249 mean train loss:  1.83548947e-03, mean val. rec. loss:  1.75629244e-03\n",
      "Epoch: 2250 mean train loss:  1.82969696e-03, mean val. rec. loss:  1.74661860e-03\n",
      "Epoch: 2251 mean train loss:  1.82929653e-03, mean val. rec. loss:  1.74431016e-03\n",
      "Epoch: 2252 mean train loss:  1.82527968e-03, mean val. rec. loss:  1.75247221e-03\n",
      "Epoch: 2253 mean train loss:  1.82186403e-03, mean val. rec. loss:  1.74034209e-03\n",
      "Epoch: 2254 mean train loss:  1.81902412e-03, mean val. rec. loss:  1.73637954e-03\n",
      "Epoch: 2255 mean train loss:  1.82142018e-03, mean val. rec. loss:  1.73315188e-03\n",
      "Epoch: 2256 mean train loss:  1.81427528e-03, mean val. rec. loss:  1.73162106e-03\n",
      "Epoch: 2257 mean train loss:  1.81370380e-03, mean val. rec. loss:  1.73695087e-03\n",
      "Epoch: 2258 mean train loss:  1.80639201e-03, mean val. rec. loss:  1.72577936e-03\n",
      "Epoch: 2259 mean train loss:  1.80898277e-03, mean val. rec. loss:  1.72105490e-03\n",
      "Epoch: 2260 mean train loss:  1.80330548e-03, mean val. rec. loss:  1.72603596e-03\n",
      "Epoch: 2261 mean train loss:  1.80092962e-03, mean val. rec. loss:  1.72170590e-03\n",
      "Epoch: 2262 mean train loss:  1.79732277e-03, mean val. rec. loss:  1.71331313e-03\n",
      "Epoch: 2263 mean train loss:  1.79268218e-03, mean val. rec. loss:  1.70986144e-03\n",
      "Epoch: 2264 mean train loss:  1.79333812e-03, mean val. rec. loss:  1.71626196e-03\n",
      "Epoch: 2265 mean train loss:  1.78730450e-03, mean val. rec. loss:  1.70959874e-03\n",
      "Epoch: 2266 mean train loss:  1.78483517e-03, mean val. rec. loss:  1.70186424e-03\n",
      "Epoch: 2267 mean train loss:  1.78323375e-03, mean val. rec. loss:  1.70874072e-03\n",
      "Epoch: 2268 mean train loss:  1.77884840e-03, mean val. rec. loss:  1.70162451e-03\n",
      "Epoch: 2269 mean train loss:  1.77724829e-03, mean val. rec. loss:  1.68984721e-03\n",
      "Epoch: 2270 mean train loss:  1.77168371e-03, mean val. rec. loss:  1.69326358e-03\n",
      "Epoch: 2271 mean train loss:  1.76853586e-03, mean val. rec. loss:  1.69812528e-03\n",
      "Epoch: 2272 mean train loss:  1.76851106e-03, mean val. rec. loss:  1.69708380e-03\n",
      "Epoch: 2273 mean train loss:  1.76500006e-03, mean val. rec. loss:  1.68389547e-03\n",
      "Epoch: 2274 mean train loss:  1.76164335e-03, mean val. rec. loss:  1.68801255e-03\n",
      "Epoch: 2275 mean train loss:  1.76040354e-03, mean val. rec. loss:  1.67793152e-03\n",
      "Epoch: 2276 mean train loss:  1.76072425e-03, mean val. rec. loss:  1.68490962e-03\n",
      "Epoch: 2277 mean train loss:  1.75314799e-03, mean val. rec. loss:  1.67442502e-03\n",
      "Epoch: 2278 mean train loss:  1.75274532e-03, mean val. rec. loss:  1.67359128e-03\n",
      "Epoch: 2279 mean train loss:  1.74735257e-03, mean val. rec. loss:  1.67621389e-03\n",
      "Epoch: 2280 mean train loss:  1.74521195e-03, mean val. rec. loss:  1.66820462e-03\n",
      "Epoch: 2281 mean train loss:  1.74322077e-03, mean val. rec. loss:  1.66476994e-03\n",
      "Epoch: 2282 mean train loss:  1.74030474e-03, mean val. rec. loss:  1.66162078e-03\n",
      "Epoch: 2283 mean train loss:  1.73762621e-03, mean val. rec. loss:  1.66440199e-03\n",
      "Epoch: 2284 mean train loss:  1.73695067e-03, mean val. rec. loss:  1.65924662e-03\n",
      "Epoch: 2285 mean train loss:  1.73350181e-03, mean val. rec. loss:  1.65551450e-03\n",
      "Epoch: 2286 mean train loss:  1.72908278e-03, mean val. rec. loss:  1.65717166e-03\n",
      "Epoch: 2287 mean train loss:  1.72564012e-03, mean val. rec. loss:  1.65084600e-03\n",
      "Epoch: 2288 mean train loss:  1.72652643e-03, mean val. rec. loss:  1.65519104e-03\n",
      "Epoch: 2289 mean train loss:  1.72172036e-03, mean val. rec. loss:  1.64420430e-03\n",
      "Epoch: 2290 mean train loss:  1.71872384e-03, mean val. rec. loss:  1.64577859e-03\n",
      "Epoch: 2291 mean train loss:  1.71596316e-03, mean val. rec. loss:  1.64417217e-03\n",
      "Epoch: 2292 mean train loss:  1.71337158e-03, mean val. rec. loss:  1.63237816e-03\n",
      "Epoch: 2293 mean train loss:  1.70917536e-03, mean val. rec. loss:  1.63897160e-03\n",
      "Epoch: 2294 mean train loss:  1.70770939e-03, mean val. rec. loss:  1.64213471e-03\n",
      "Epoch: 2295 mean train loss:  1.70578287e-03, mean val. rec. loss:  1.62786042e-03\n",
      "Epoch: 2296 mean train loss:  1.70301973e-03, mean val. rec. loss:  1.62584287e-03\n",
      "Epoch: 2297 mean train loss:  1.69909523e-03, mean val. rec. loss:  1.63069949e-03\n",
      "Epoch: 2298 mean train loss:  1.69529734e-03, mean val. rec. loss:  1.62723937e-03\n",
      "Epoch: 2299 mean train loss:  1.69387568e-03, mean val. rec. loss:  1.62327217e-03\n",
      "Epoch: 2300 mean train loss:  1.69045107e-03, mean val. rec. loss:  1.61373819e-03\n",
      "Epoch: 2301 mean train loss:  1.68876039e-03, mean val. rec. loss:  1.61459141e-03\n",
      "Epoch: 2302 mean train loss:  1.68845250e-03, mean val. rec. loss:  1.62053108e-03\n",
      "Epoch: 2303 mean train loss:  1.68464978e-03, mean val. rec. loss:  1.60620939e-03\n",
      "Epoch: 2304 mean train loss:  1.68179492e-03, mean val. rec. loss:  1.60736950e-03\n",
      "Epoch: 2305 mean train loss:  1.67871005e-03, mean val. rec. loss:  1.61145926e-03\n",
      "Epoch: 2306 mean train loss:  1.67505239e-03, mean val. rec. loss:  1.60426352e-03\n",
      "Epoch: 2307 mean train loss:  1.67284169e-03, mean val. rec. loss:  1.60319689e-03\n",
      "Epoch: 2308 mean train loss:  1.66927006e-03, mean val. rec. loss:  1.60104880e-03\n",
      "Epoch: 2309 mean train loss:  1.66842744e-03, mean val. rec. loss:  1.60130582e-03\n",
      "Epoch: 2310 mean train loss:  1.66414305e-03, mean val. rec. loss:  1.59566301e-03\n",
      "Epoch: 2311 mean train loss:  1.66548437e-03, mean val. rec. loss:  1.59302136e-03\n",
      "Epoch: 2312 mean train loss:  1.65920019e-03, mean val. rec. loss:  1.58744105e-03\n",
      "Epoch: 2313 mean train loss:  1.66346494e-03, mean val. rec. loss:  1.58878652e-03\n",
      "Epoch: 2314 mean train loss:  1.65606773e-03, mean val. rec. loss:  1.59180281e-03\n",
      "Epoch: 2315 mean train loss:  1.65349936e-03, mean val. rec. loss:  1.58647851e-03\n",
      "Epoch: 2316 mean train loss:  1.64897143e-03, mean val. rec. loss:  1.57867525e-03\n",
      "Epoch: 2317 mean train loss:  1.64870852e-03, mean val. rec. loss:  1.57955173e-03\n",
      "Epoch: 2318 mean train loss:  1.64405048e-03, mean val. rec. loss:  1.57062411e-03\n",
      "Epoch: 2319 mean train loss:  1.64071883e-03, mean val. rec. loss:  1.57000946e-03\n",
      "Epoch: 2320 mean train loss:  1.63921065e-03, mean val. rec. loss:  1.57109121e-03\n",
      "Epoch: 2321 mean train loss:  1.63785229e-03, mean val. rec. loss:  1.56567155e-03\n",
      "Epoch: 2322 mean train loss:  1.63661991e-03, mean val. rec. loss:  1.56927618e-03\n",
      "Epoch: 2323 mean train loss:  1.63277263e-03, mean val. rec. loss:  1.56159415e-03\n",
      "Epoch: 2324 mean train loss:  1.63136256e-03, mean val. rec. loss:  1.56363976e-03\n",
      "Epoch: 2325 mean train loss:  1.62821687e-03, mean val. rec. loss:  1.56382279e-03\n",
      "Epoch: 2326 mean train loss:  1.62431818e-03, mean val. rec. loss:  1.55384773e-03\n",
      "Epoch: 2327 mean train loss:  1.62147842e-03, mean val. rec. loss:  1.56034231e-03\n",
      "Epoch: 2328 mean train loss:  1.62206672e-03, mean val. rec. loss:  1.55240137e-03\n",
      "Epoch: 2329 mean train loss:  1.61985187e-03, mean val. rec. loss:  1.54761657e-03\n",
      "Epoch: 2330 mean train loss:  1.61429886e-03, mean val. rec. loss:  1.54480672e-03\n",
      "Epoch: 2331 mean train loss:  1.61108513e-03, mean val. rec. loss:  1.54776602e-03\n",
      "Epoch: 2332 mean train loss:  1.60946107e-03, mean val. rec. loss:  1.55002867e-03\n",
      "Epoch: 2333 mean train loss:  1.60545593e-03, mean val. rec. loss:  1.53912858e-03\n",
      "Epoch: 2334 mean train loss:  1.60388397e-03, mean val. rec. loss:  1.53912669e-03\n",
      "Epoch: 2335 mean train loss:  1.60138025e-03, mean val. rec. loss:  1.53983773e-03\n",
      "Epoch: 2336 mean train loss:  1.59934047e-03, mean val. rec. loss:  1.53558618e-03\n",
      "Epoch: 2337 mean train loss:  1.59701770e-03, mean val. rec. loss:  1.52845761e-03\n",
      "Epoch: 2338 mean train loss:  1.59481810e-03, mean val. rec. loss:  1.52830932e-03\n",
      "Epoch: 2339 mean train loss:  1.59266020e-03, mean val. rec. loss:  1.52987489e-03\n",
      "Epoch: 2340 mean train loss:  1.58971507e-03, mean val. rec. loss:  1.52368662e-03\n",
      "Epoch: 2341 mean train loss:  1.58759725e-03, mean val. rec. loss:  1.51972029e-03\n",
      "Epoch: 2342 mean train loss:  1.58838425e-03, mean val. rec. loss:  1.52228198e-03\n",
      "Epoch: 2343 mean train loss:  1.58235354e-03, mean val. rec. loss:  1.52044820e-03\n",
      "Epoch: 2344 mean train loss:  1.58038049e-03, mean val. rec. loss:  1.51603222e-03\n",
      "Epoch: 2345 mean train loss:  1.57789107e-03, mean val. rec. loss:  1.51444572e-03\n",
      "Epoch: 2346 mean train loss:  1.57793678e-03, mean val. rec. loss:  1.51027993e-03\n",
      "Epoch: 2347 mean train loss:  1.57267009e-03, mean val. rec. loss:  1.51120366e-03\n",
      "Epoch: 2348 mean train loss:  1.57071395e-03, mean val. rec. loss:  1.50700241e-03\n",
      "Epoch: 2349 mean train loss:  1.56859016e-03, mean val. rec. loss:  1.50488819e-03\n",
      "Epoch: 2350 mean train loss:  1.56641901e-03, mean val. rec. loss:  1.50525890e-03\n",
      "Epoch: 2351 mean train loss:  1.56318257e-03, mean val. rec. loss:  1.49703927e-03\n",
      "Epoch: 2352 mean train loss:  1.56262948e-03, mean val. rec. loss:  1.49681423e-03\n",
      "Epoch: 2353 mean train loss:  1.55883295e-03, mean val. rec. loss:  1.49896145e-03\n",
      "Epoch: 2354 mean train loss:  1.55637596e-03, mean val. rec. loss:  1.49533080e-03\n",
      "Epoch: 2355 mean train loss:  1.55181954e-03, mean val. rec. loss:  1.49297016e-03\n",
      "Epoch: 2356 mean train loss:  1.54981012e-03, mean val. rec. loss:  1.48903975e-03\n",
      "Epoch: 2357 mean train loss:  1.54859761e-03, mean val. rec. loss:  1.48627525e-03\n",
      "Epoch: 2358 mean train loss:  1.54776857e-03, mean val. rec. loss:  1.48676677e-03\n",
      "Epoch: 2359 mean train loss:  1.54357212e-03, mean val. rec. loss:  1.47676686e-03\n",
      "Epoch: 2360 mean train loss:  1.54144220e-03, mean val. rec. loss:  1.47862682e-03\n",
      "Epoch: 2361 mean train loss:  1.53770373e-03, mean val. rec. loss:  1.47561111e-03\n",
      "Epoch: 2362 mean train loss:  1.53615721e-03, mean val. rec. loss:  1.47930326e-03\n",
      "Epoch: 2363 mean train loss:  1.53473389e-03, mean val. rec. loss:  1.47727655e-03\n",
      "Epoch: 2364 mean train loss:  1.53129619e-03, mean val. rec. loss:  1.47201462e-03\n",
      "Epoch: 2365 mean train loss:  1.52892477e-03, mean val. rec. loss:  1.46531652e-03\n",
      "Epoch: 2366 mean train loss:  1.52582709e-03, mean val. rec. loss:  1.46468107e-03\n",
      "Epoch: 2367 mean train loss:  1.52477327e-03, mean val. rec. loss:  1.46493345e-03\n",
      "Epoch: 2368 mean train loss:  1.52170439e-03, mean val. rec. loss:  1.46414158e-03\n",
      "Epoch: 2369 mean train loss:  1.52051771e-03, mean val. rec. loss:  1.46090723e-03\n",
      "Epoch: 2370 mean train loss:  1.51834787e-03, mean val. rec. loss:  1.45696634e-03\n",
      "Epoch: 2371 mean train loss:  1.51594135e-03, mean val. rec. loss:  1.45983870e-03\n",
      "Epoch: 2372 mean train loss:  1.51396169e-03, mean val. rec. loss:  1.45464409e-03\n",
      "Epoch: 2373 mean train loss:  1.51024093e-03, mean val. rec. loss:  1.45384771e-03\n",
      "Epoch: 2374 mean train loss:  1.51003592e-03, mean val. rec. loss:  1.44999958e-03\n",
      "Epoch: 2375 mean train loss:  1.50699012e-03, mean val. rec. loss:  1.44610420e-03\n",
      "Epoch: 2376 mean train loss:  1.50389598e-03, mean val. rec. loss:  1.44518788e-03\n",
      "Epoch: 2377 mean train loss:  1.50227125e-03, mean val. rec. loss:  1.44714495e-03\n",
      "Epoch: 2378 mean train loss:  1.49929274e-03, mean val. rec. loss:  1.44036514e-03\n",
      "Epoch: 2379 mean train loss:  1.49602829e-03, mean val. rec. loss:  1.43757390e-03\n",
      "Epoch: 2380 mean train loss:  1.49345063e-03, mean val. rec. loss:  1.43779981e-03\n",
      "Epoch: 2381 mean train loss:  1.49340537e-03, mean val. rec. loss:  1.43236620e-03\n",
      "Epoch: 2382 mean train loss:  1.48992076e-03, mean val. rec. loss:  1.43146747e-03\n",
      "Epoch: 2383 mean train loss:  1.49017279e-03, mean val. rec. loss:  1.43728562e-03\n",
      "Epoch: 2384 mean train loss:  1.48744377e-03, mean val. rec. loss:  1.42548012e-03\n",
      "Epoch: 2385 mean train loss:  1.48363974e-03, mean val. rec. loss:  1.42574310e-03\n",
      "Epoch: 2386 mean train loss:  1.48096390e-03, mean val. rec. loss:  1.42707069e-03\n",
      "Epoch: 2387 mean train loss:  1.48096448e-03, mean val. rec. loss:  1.42239739e-03\n",
      "Epoch: 2388 mean train loss:  1.47774936e-03, mean val. rec. loss:  1.42452775e-03\n",
      "Epoch: 2389 mean train loss:  1.47427130e-03, mean val. rec. loss:  1.42132872e-03\n",
      "Epoch: 2390 mean train loss:  1.47210970e-03, mean val. rec. loss:  1.41553472e-03\n",
      "Epoch: 2391 mean train loss:  1.47138632e-03, mean val. rec. loss:  1.41076140e-03\n",
      "Epoch: 2392 mean train loss:  1.46955294e-03, mean val. rec. loss:  1.41228321e-03\n",
      "Epoch: 2393 mean train loss:  1.46820967e-03, mean val. rec. loss:  1.41608977e-03\n",
      "Epoch: 2394 mean train loss:  1.46302090e-03, mean val. rec. loss:  1.40995485e-03\n",
      "Epoch: 2395 mean train loss:  1.46441593e-03, mean val. rec. loss:  1.40351929e-03\n",
      "Epoch: 2396 mean train loss:  1.45894251e-03, mean val. rec. loss:  1.39914315e-03\n",
      "Epoch: 2397 mean train loss:  1.45707521e-03, mean val. rec. loss:  1.40914452e-03\n",
      "Epoch: 2398 mean train loss:  1.45482211e-03, mean val. rec. loss:  1.40127554e-03\n",
      "Epoch: 2399 mean train loss:  1.45343582e-03, mean val. rec. loss:  1.39842019e-03\n",
      "Epoch: 2400 mean train loss:  1.45162340e-03, mean val. rec. loss:  1.40364635e-03\n",
      "Epoch: 2401 mean train loss:  1.45050168e-03, mean val. rec. loss:  1.39017599e-03\n",
      "Epoch: 2402 mean train loss:  1.44813387e-03, mean val. rec. loss:  1.38577892e-03\n",
      "Epoch: 2403 mean train loss:  1.44673218e-03, mean val. rec. loss:  1.39772848e-03\n",
      "Epoch: 2404 mean train loss:  1.44426077e-03, mean val. rec. loss:  1.39005926e-03\n",
      "Epoch: 2405 mean train loss:  1.43861882e-03, mean val. rec. loss:  1.38463625e-03\n",
      "Epoch: 2406 mean train loss:  1.44284636e-03, mean val. rec. loss:  1.38353545e-03\n",
      "Epoch: 2407 mean train loss:  1.43561020e-03, mean val. rec. loss:  1.38548409e-03\n",
      "Epoch: 2408 mean train loss:  1.43282186e-03, mean val. rec. loss:  1.38489357e-03\n",
      "Epoch: 2409 mean train loss:  1.43050361e-03, mean val. rec. loss:  1.37925235e-03\n",
      "Epoch: 2410 mean train loss:  1.42847309e-03, mean val. rec. loss:  1.36939985e-03\n",
      "Epoch: 2411 mean train loss:  1.42685250e-03, mean val. rec. loss:  1.37431273e-03\n",
      "Epoch: 2412 mean train loss:  1.42299179e-03, mean val. rec. loss:  1.37517859e-03\n",
      "Epoch: 2413 mean train loss:  1.42255170e-03, mean val. rec. loss:  1.36899367e-03\n",
      "Epoch: 2414 mean train loss:  1.42089146e-03, mean val. rec. loss:  1.36937485e-03\n",
      "Epoch: 2415 mean train loss:  1.41961004e-03, mean val. rec. loss:  1.36791090e-03\n",
      "Epoch: 2416 mean train loss:  1.41706482e-03, mean val. rec. loss:  1.36177918e-03\n",
      "Epoch: 2417 mean train loss:  1.41445747e-03, mean val. rec. loss:  1.35674942e-03\n",
      "Epoch: 2418 mean train loss:  1.41279141e-03, mean val. rec. loss:  1.36726426e-03\n",
      "Epoch: 2419 mean train loss:  1.41001528e-03, mean val. rec. loss:  1.35637391e-03\n",
      "Epoch: 2420 mean train loss:  1.40734850e-03, mean val. rec. loss:  1.35564688e-03\n",
      "Epoch: 2421 mean train loss:  1.40479973e-03, mean val. rec. loss:  1.35898503e-03\n",
      "Epoch: 2422 mean train loss:  1.40306499e-03, mean val. rec. loss:  1.35246006e-03\n",
      "Epoch: 2423 mean train loss:  1.40209946e-03, mean val. rec. loss:  1.34655746e-03\n",
      "Epoch: 2424 mean train loss:  1.40001933e-03, mean val. rec. loss:  1.34858024e-03\n",
      "Epoch: 2425 mean train loss:  1.39871105e-03, mean val. rec. loss:  1.35106808e-03\n",
      "Epoch: 2426 mean train loss:  1.39551832e-03, mean val. rec. loss:  1.34280964e-03\n",
      "Epoch: 2427 mean train loss:  1.39308895e-03, mean val. rec. loss:  1.34379951e-03\n",
      "Epoch: 2428 mean train loss:  1.39253985e-03, mean val. rec. loss:  1.34449383e-03\n",
      "Epoch: 2429 mean train loss:  1.39213536e-03, mean val. rec. loss:  1.33494139e-03\n",
      "Epoch: 2430 mean train loss:  1.38721052e-03, mean val. rec. loss:  1.34230096e-03\n",
      "Epoch: 2431 mean train loss:  1.38549683e-03, mean val. rec. loss:  1.34018049e-03\n",
      "Epoch: 2432 mean train loss:  1.38286895e-03, mean val. rec. loss:  1.33178379e-03\n",
      "Epoch: 2433 mean train loss:  1.38116238e-03, mean val. rec. loss:  1.33110881e-03\n",
      "Epoch: 2434 mean train loss:  1.38124448e-03, mean val. rec. loss:  1.32877958e-03\n",
      "Epoch: 2435 mean train loss:  1.37570554e-03, mean val. rec. loss:  1.32969603e-03\n",
      "Epoch: 2436 mean train loss:  1.37430930e-03, mean val. rec. loss:  1.32730792e-03\n",
      "Epoch: 2437 mean train loss:  1.37415826e-03, mean val. rec. loss:  1.32365706e-03\n",
      "Epoch: 2438 mean train loss:  1.37051666e-03, mean val. rec. loss:  1.32380433e-03\n",
      "Epoch: 2439 mean train loss:  1.36938383e-03, mean val. rec. loss:  1.31963651e-03\n",
      "Epoch: 2440 mean train loss:  1.36584051e-03, mean val. rec. loss:  1.31906387e-03\n",
      "Epoch: 2441 mean train loss:  1.36491423e-03, mean val. rec. loss:  1.31616403e-03\n",
      "Epoch: 2442 mean train loss:  1.36310927e-03, mean val. rec. loss:  1.31245008e-03\n",
      "Epoch: 2443 mean train loss:  1.35893981e-03, mean val. rec. loss:  1.31707366e-03\n",
      "Epoch: 2444 mean train loss:  1.35953120e-03, mean val. rec. loss:  1.30988592e-03\n",
      "Epoch: 2445 mean train loss:  1.35528190e-03, mean val. rec. loss:  1.30735200e-03\n",
      "Epoch: 2446 mean train loss:  1.35502230e-03, mean val. rec. loss:  1.30326777e-03\n",
      "Epoch: 2447 mean train loss:  1.35343612e-03, mean val. rec. loss:  1.31325227e-03\n",
      "Epoch: 2448 mean train loss:  1.35136395e-03, mean val. rec. loss:  1.30027358e-03\n",
      "Epoch: 2449 mean train loss:  1.35039998e-03, mean val. rec. loss:  1.29917976e-03\n",
      "Epoch: 2450 mean train loss:  1.34790645e-03, mean val. rec. loss:  1.30795531e-03\n",
      "Epoch: 2451 mean train loss:  1.34584241e-03, mean val. rec. loss:  1.30082892e-03\n",
      "Epoch: 2452 mean train loss:  1.34351615e-03, mean val. rec. loss:  1.29814875e-03\n",
      "Epoch: 2453 mean train loss:  1.34312432e-03, mean val. rec. loss:  1.29510571e-03\n",
      "Epoch: 2454 mean train loss:  1.34088728e-03, mean val. rec. loss:  1.29209771e-03\n",
      "Epoch: 2455 mean train loss:  1.34126684e-03, mean val. rec. loss:  1.29046731e-03\n",
      "Epoch: 2456 mean train loss:  1.33480107e-03, mean val. rec. loss:  1.29090010e-03\n",
      "Epoch: 2457 mean train loss:  1.33467553e-03, mean val. rec. loss:  1.28884650e-03\n",
      "Epoch: 2458 mean train loss:  1.33338504e-03, mean val. rec. loss:  1.29207285e-03\n",
      "Epoch: 2459 mean train loss:  1.32781646e-03, mean val. rec. loss:  1.28145945e-03\n",
      "Epoch: 2460 mean train loss:  1.32747623e-03, mean val. rec. loss:  1.27736039e-03\n",
      "Epoch: 2461 mean train loss:  1.32535000e-03, mean val. rec. loss:  1.28283791e-03\n",
      "Epoch: 2462 mean train loss:  1.32297882e-03, mean val. rec. loss:  1.27880879e-03\n",
      "Epoch: 2463 mean train loss:  1.32034526e-03, mean val. rec. loss:  1.27639611e-03\n",
      "Epoch: 2464 mean train loss:  1.31917370e-03, mean val. rec. loss:  1.27213612e-03\n",
      "Epoch: 2465 mean train loss:  1.31702878e-03, mean val. rec. loss:  1.27367246e-03\n",
      "Epoch: 2466 mean train loss:  1.31481576e-03, mean val. rec. loss:  1.26953125e-03\n",
      "Epoch: 2467 mean train loss:  1.31398129e-03, mean val. rec. loss:  1.27143598e-03\n",
      "Epoch: 2468 mean train loss:  1.31218291e-03, mean val. rec. loss:  1.27003106e-03\n",
      "Epoch: 2469 mean train loss:  1.30992931e-03, mean val. rec. loss:  1.26541068e-03\n",
      "Epoch: 2470 mean train loss:  1.30929499e-03, mean val. rec. loss:  1.26679293e-03\n",
      "Epoch: 2471 mean train loss:  1.30665712e-03, mean val. rec. loss:  1.26291121e-03\n",
      "Epoch: 2472 mean train loss:  1.30600941e-03, mean val. rec. loss:  1.25954936e-03\n",
      "Epoch: 2473 mean train loss:  1.30174694e-03, mean val. rec. loss:  1.26250808e-03\n",
      "Epoch: 2474 mean train loss:  1.29916735e-03, mean val. rec. loss:  1.25546165e-03\n",
      "Epoch: 2475 mean train loss:  1.29841541e-03, mean val. rec. loss:  1.25139864e-03\n",
      "Epoch: 2476 mean train loss:  1.29593161e-03, mean val. rec. loss:  1.25550293e-03\n",
      "Epoch: 2477 mean train loss:  1.29474756e-03, mean val. rec. loss:  1.25076218e-03\n",
      "Epoch: 2478 mean train loss:  1.29353217e-03, mean val. rec. loss:  1.24812911e-03\n",
      "Epoch: 2479 mean train loss:  1.29105435e-03, mean val. rec. loss:  1.24741691e-03\n",
      "Epoch: 2480 mean train loss:  1.28940028e-03, mean val. rec. loss:  1.24975806e-03\n",
      "Epoch: 2481 mean train loss:  1.28507671e-03, mean val. rec. loss:  1.24623281e-03\n",
      "Epoch: 2482 mean train loss:  1.28495910e-03, mean val. rec. loss:  1.24676635e-03\n",
      "Epoch: 2483 mean train loss:  1.28279890e-03, mean val. rec. loss:  1.24349303e-03\n",
      "Epoch: 2484 mean train loss:  1.28260593e-03, mean val. rec. loss:  1.23926634e-03\n",
      "Epoch: 2485 mean train loss:  1.28127509e-03, mean val. rec. loss:  1.23779032e-03\n",
      "Epoch: 2486 mean train loss:  1.27765604e-03, mean val. rec. loss:  1.23309362e-03\n",
      "Epoch: 2487 mean train loss:  1.27618482e-03, mean val. rec. loss:  1.23982866e-03\n",
      "Epoch: 2488 mean train loss:  1.27297137e-03, mean val. rec. loss:  1.22905126e-03\n",
      "Epoch: 2489 mean train loss:  1.27243527e-03, mean val. rec. loss:  1.22979269e-03\n",
      "Epoch: 2490 mean train loss:  1.27072250e-03, mean val. rec. loss:  1.23290957e-03\n",
      "Epoch: 2491 mean train loss:  1.26751047e-03, mean val. rec. loss:  1.22853459e-03\n",
      "Epoch: 2492 mean train loss:  1.26584022e-03, mean val. rec. loss:  1.22603905e-03\n",
      "Epoch: 2493 mean train loss:  1.26549679e-03, mean val. rec. loss:  1.22746433e-03\n",
      "Epoch: 2494 mean train loss:  1.26231374e-03, mean val. rec. loss:  1.22373657e-03\n",
      "Epoch: 2495 mean train loss:  1.26018371e-03, mean val. rec. loss:  1.22272532e-03\n",
      "Epoch: 2496 mean train loss:  1.25931580e-03, mean val. rec. loss:  1.21790607e-03\n",
      "Epoch: 2497 mean train loss:  1.25609748e-03, mean val. rec. loss:  1.21616619e-03\n",
      "Epoch: 2498 mean train loss:  1.25774159e-03, mean val. rec. loss:  1.21682810e-03\n",
      "Epoch: 2499 mean train loss:  1.25422479e-03, mean val. rec. loss:  1.21604255e-03\n",
      "Epoch: 2500 mean train loss:  1.25301488e-03, mean val. rec. loss:  1.21527867e-03\n",
      "Epoch: 2501 mean train loss:  1.25010275e-03, mean val. rec. loss:  1.20888142e-03\n",
      "Epoch: 2502 mean train loss:  1.24900565e-03, mean val. rec. loss:  1.21176228e-03\n",
      "Epoch: 2503 mean train loss:  1.24787680e-03, mean val. rec. loss:  1.20716822e-03\n",
      "Epoch: 2504 mean train loss:  1.24452865e-03, mean val. rec. loss:  1.20666216e-03\n",
      "Epoch: 2505 mean train loss:  1.24294504e-03, mean val. rec. loss:  1.20416320e-03\n",
      "Epoch: 2506 mean train loss:  1.24210491e-03, mean val. rec. loss:  1.20428561e-03\n",
      "Epoch: 2507 mean train loss:  1.23897845e-03, mean val. rec. loss:  1.20151494e-03\n",
      "Epoch: 2508 mean train loss:  1.23908720e-03, mean val. rec. loss:  1.19930506e-03\n",
      "Epoch: 2509 mean train loss:  1.23703176e-03, mean val. rec. loss:  1.19945814e-03\n",
      "Epoch: 2510 mean train loss:  1.23628626e-03, mean val. rec. loss:  1.19862506e-03\n",
      "Epoch: 2511 mean train loss:  1.23268284e-03, mean val. rec. loss:  1.19397902e-03\n",
      "Epoch: 2512 mean train loss:  1.23131672e-03, mean val. rec. loss:  1.19231743e-03\n",
      "Epoch: 2513 mean train loss:  1.22823591e-03, mean val. rec. loss:  1.19172008e-03\n",
      "Epoch: 2514 mean train loss:  1.22784414e-03, mean val. rec. loss:  1.18984740e-03\n",
      "Epoch: 2515 mean train loss:  1.22682346e-03, mean val. rec. loss:  1.19115463e-03\n",
      "Epoch: 2516 mean train loss:  1.22378017e-03, mean val. rec. loss:  1.18312109e-03\n",
      "Epoch: 2517 mean train loss:  1.22183069e-03, mean val. rec. loss:  1.18341431e-03\n",
      "Epoch: 2518 mean train loss:  1.22159376e-03, mean val. rec. loss:  1.18465031e-03\n",
      "Epoch: 2519 mean train loss:  1.21753058e-03, mean val. rec. loss:  1.17696167e-03\n",
      "Epoch: 2520 mean train loss:  1.21716118e-03, mean val. rec. loss:  1.17863424e-03\n",
      "Epoch: 2521 mean train loss:  1.21490516e-03, mean val. rec. loss:  1.17821490e-03\n",
      "Epoch: 2522 mean train loss:  1.21466082e-03, mean val. rec. loss:  1.18120640e-03\n",
      "Epoch: 2523 mean train loss:  1.21210163e-03, mean val. rec. loss:  1.17323181e-03\n",
      "Epoch: 2524 mean train loss:  1.21081222e-03, mean val. rec. loss:  1.17522740e-03\n",
      "Epoch: 2525 mean train loss:  1.20975680e-03, mean val. rec. loss:  1.17099838e-03\n",
      "Epoch: 2526 mean train loss:  1.20839182e-03, mean val. rec. loss:  1.17102403e-03\n",
      "Epoch: 2527 mean train loss:  1.20528591e-03, mean val. rec. loss:  1.17025746e-03\n",
      "Epoch: 2528 mean train loss:  1.20593612e-03, mean val. rec. loss:  1.16521789e-03\n",
      "Epoch: 2529 mean train loss:  1.20282356e-03, mean val. rec. loss:  1.16845667e-03\n",
      "Epoch: 2530 mean train loss:  1.20789284e-03, mean val. rec. loss:  1.16872359e-03\n",
      "Epoch: 2531 mean train loss:  1.20164444e-03, mean val. rec. loss:  1.15769375e-03\n",
      "Epoch: 2532 mean train loss:  1.20022677e-03, mean val. rec. loss:  1.16704426e-03\n",
      "Epoch: 2533 mean train loss:  1.19506562e-03, mean val. rec. loss:  1.16354263e-03\n",
      "Epoch: 2534 mean train loss:  1.19441398e-03, mean val. rec. loss:  1.15739311e-03\n",
      "Epoch: 2535 mean train loss:  1.19357371e-03, mean val. rec. loss:  1.15405343e-03\n",
      "Epoch: 2536 mean train loss:  1.19042726e-03, mean val. rec. loss:  1.15458558e-03\n",
      "Epoch: 2537 mean train loss:  1.18838109e-03, mean val. rec. loss:  1.16036622e-03\n",
      "Epoch: 2538 mean train loss:  1.18749742e-03, mean val. rec. loss:  1.14767268e-03\n",
      "Epoch: 2539 mean train loss:  1.18490888e-03, mean val. rec. loss:  1.14747140e-03\n",
      "Epoch: 2540 mean train loss:  1.18292010e-03, mean val. rec. loss:  1.15546126e-03\n",
      "Epoch: 2541 mean train loss:  1.18199986e-03, mean val. rec. loss:  1.14924450e-03\n",
      "Epoch: 2542 mean train loss:  1.18019669e-03, mean val. rec. loss:  1.14660699e-03\n",
      "Epoch: 2543 mean train loss:  1.17774709e-03, mean val. rec. loss:  1.14338035e-03\n",
      "Epoch: 2544 mean train loss:  1.17755152e-03, mean val. rec. loss:  1.14093721e-03\n",
      "Epoch: 2545 mean train loss:  1.17478714e-03, mean val. rec. loss:  1.14642687e-03\n",
      "Epoch: 2546 mean train loss:  1.17420202e-03, mean val. rec. loss:  1.13973356e-03\n",
      "Epoch: 2547 mean train loss:  1.17080239e-03, mean val. rec. loss:  1.13774335e-03\n",
      "Epoch: 2548 mean train loss:  1.17364850e-03, mean val. rec. loss:  1.13623259e-03\n",
      "Epoch: 2549 mean train loss:  1.16839541e-03, mean val. rec. loss:  1.13593965e-03\n",
      "Epoch: 2550 mean train loss:  1.16679273e-03, mean val. rec. loss:  1.13408689e-03\n",
      "Epoch: 2551 mean train loss:  1.16521304e-03, mean val. rec. loss:  1.13390074e-03\n",
      "Epoch: 2552 mean train loss:  1.16347351e-03, mean val. rec. loss:  1.12822485e-03\n",
      "Epoch: 2553 mean train loss:  1.16224599e-03, mean val. rec. loss:  1.13098898e-03\n",
      "Epoch: 2554 mean train loss:  1.16182447e-03, mean val. rec. loss:  1.12956617e-03\n",
      "Epoch: 2555 mean train loss:  1.15834341e-03, mean val. rec. loss:  1.12592447e-03\n",
      "Epoch: 2556 mean train loss:  1.15538670e-03, mean val. rec. loss:  1.12411969e-03\n",
      "Epoch: 2557 mean train loss:  1.15596568e-03, mean val. rec. loss:  1.12149999e-03\n",
      "Epoch: 2558 mean train loss:  1.15253875e-03, mean val. rec. loss:  1.12517113e-03\n",
      "Epoch: 2559 mean train loss:  1.15485102e-03, mean val. rec. loss:  1.11974129e-03\n",
      "Epoch: 2560 mean train loss:  1.15220848e-03, mean val. rec. loss:  1.11894186e-03\n",
      "Epoch: 2561 mean train loss:  1.14977960e-03, mean val. rec. loss:  1.12136297e-03\n",
      "Epoch: 2562 mean train loss:  1.14735162e-03, mean val. rec. loss:  1.11407492e-03\n",
      "Epoch: 2563 mean train loss:  1.14617837e-03, mean val. rec. loss:  1.11695819e-03\n",
      "Epoch: 2564 mean train loss:  1.14448085e-03, mean val. rec. loss:  1.11246850e-03\n",
      "Epoch: 2565 mean train loss:  1.14266723e-03, mean val. rec. loss:  1.11201478e-03\n",
      "Epoch: 2566 mean train loss:  1.14044212e-03, mean val. rec. loss:  1.11316370e-03\n",
      "Epoch: 2567 mean train loss:  1.13858623e-03, mean val. rec. loss:  1.10463718e-03\n",
      "Epoch: 2568 mean train loss:  1.13727751e-03, mean val. rec. loss:  1.10838769e-03\n",
      "Epoch: 2569 mean train loss:  1.13632812e-03, mean val. rec. loss:  1.10273179e-03\n",
      "Epoch: 2570 mean train loss:  1.13634099e-03, mean val. rec. loss:  1.10800695e-03\n",
      "Epoch: 2571 mean train loss:  1.13230554e-03, mean val. rec. loss:  1.10168072e-03\n",
      "Epoch: 2572 mean train loss:  1.13426015e-03, mean val. rec. loss:  1.10004661e-03\n",
      "Epoch: 2573 mean train loss:  1.13021508e-03, mean val. rec. loss:  1.10281960e-03\n",
      "Epoch: 2574 mean train loss:  1.12975264e-03, mean val. rec. loss:  1.09535645e-03\n",
      "Epoch: 2575 mean train loss:  1.12921766e-03, mean val. rec. loss:  1.09861013e-03\n",
      "Epoch: 2576 mean train loss:  1.12704713e-03, mean val. rec. loss:  1.09058400e-03\n",
      "Epoch: 2577 mean train loss:  1.12415002e-03, mean val. rec. loss:  1.09840871e-03\n",
      "Epoch: 2578 mean train loss:  1.12416991e-03, mean val. rec. loss:  1.09288387e-03\n",
      "Epoch: 2579 mean train loss:  1.12485192e-03, mean val. rec. loss:  1.08678130e-03\n",
      "Epoch: 2580 mean train loss:  1.12058117e-03, mean val. rec. loss:  1.09649235e-03\n",
      "Epoch: 2581 mean train loss:  1.11697269e-03, mean val. rec. loss:  1.08697814e-03\n",
      "Epoch: 2582 mean train loss:  1.11842277e-03, mean val. rec. loss:  1.08494933e-03\n",
      "Epoch: 2583 mean train loss:  1.11592383e-03, mean val. rec. loss:  1.09038048e-03\n",
      "Epoch: 2584 mean train loss:  1.11293603e-03, mean val. rec. loss:  1.08441420e-03\n",
      "Epoch: 2585 mean train loss:  1.11221163e-03, mean val. rec. loss:  1.07909986e-03\n",
      "Epoch: 2586 mean train loss:  1.10945500e-03, mean val. rec. loss:  1.08042650e-03\n",
      "Epoch: 2587 mean train loss:  1.10856457e-03, mean val. rec. loss:  1.08001726e-03\n",
      "Epoch: 2588 mean train loss:  1.10861568e-03, mean val. rec. loss:  1.08316861e-03\n",
      "Epoch: 2589 mean train loss:  1.10523683e-03, mean val. rec. loss:  1.07666181e-03\n",
      "Epoch: 2590 mean train loss:  1.10334163e-03, mean val. rec. loss:  1.07584538e-03\n",
      "Epoch: 2591 mean train loss:  1.10267561e-03, mean val. rec. loss:  1.07236897e-03\n",
      "Epoch: 2592 mean train loss:  1.09965699e-03, mean val. rec. loss:  1.07427261e-03\n",
      "Epoch: 2593 mean train loss:  1.10198221e-03, mean val. rec. loss:  1.06976686e-03\n",
      "Epoch: 2594 mean train loss:  1.09765169e-03, mean val. rec. loss:  1.07190754e-03\n",
      "Epoch: 2595 mean train loss:  1.09754247e-03, mean val. rec. loss:  1.06824469e-03\n",
      "Epoch: 2596 mean train loss:  1.09700450e-03, mean val. rec. loss:  1.06817884e-03\n",
      "Epoch: 2597 mean train loss:  1.09510664e-03, mean val. rec. loss:  1.06926291e-03\n",
      "Epoch: 2598 mean train loss:  1.09390858e-03, mean val. rec. loss:  1.06337492e-03\n",
      "Epoch: 2599 mean train loss:  1.09130269e-03, mean val. rec. loss:  1.05841305e-03\n",
      "Epoch: 2600 mean train loss:  1.09108129e-03, mean val. rec. loss:  1.06787958e-03\n",
      "Epoch: 2601 mean train loss:  1.08900351e-03, mean val. rec. loss:  1.06413335e-03\n",
      "Epoch: 2602 mean train loss:  1.08755475e-03, mean val. rec. loss:  1.05775188e-03\n",
      "Epoch: 2603 mean train loss:  1.08483884e-03, mean val. rec. loss:  1.06377863e-03\n",
      "Epoch: 2604 mean train loss:  1.08367285e-03, mean val. rec. loss:  1.05334448e-03\n",
      "Epoch: 2605 mean train loss:  1.08304371e-03, mean val. rec. loss:  1.05195518e-03\n",
      "Epoch: 2606 mean train loss:  1.08125293e-03, mean val. rec. loss:  1.05864805e-03\n",
      "Epoch: 2607 mean train loss:  1.07958726e-03, mean val. rec. loss:  1.04978521e-03\n",
      "Epoch: 2608 mean train loss:  1.07639366e-03, mean val. rec. loss:  1.05266280e-03\n",
      "Epoch: 2609 mean train loss:  1.07612316e-03, mean val. rec. loss:  1.04898621e-03\n",
      "Epoch: 2610 mean train loss:  1.07517529e-03, mean val. rec. loss:  1.04678680e-03\n",
      "Epoch: 2611 mean train loss:  1.07482843e-03, mean val. rec. loss:  1.04580696e-03\n",
      "Epoch: 2612 mean train loss:  1.07150492e-03, mean val. rec. loss:  1.04469664e-03\n",
      "Epoch: 2613 mean train loss:  1.07049526e-03, mean val. rec. loss:  1.04592246e-03\n",
      "Epoch: 2614 mean train loss:  1.07025680e-03, mean val. rec. loss:  1.04137906e-03\n",
      "Epoch: 2615 mean train loss:  1.06590557e-03, mean val. rec. loss:  1.04765354e-03\n",
      "Epoch: 2616 mean train loss:  1.06607969e-03, mean val. rec. loss:  1.04134148e-03\n",
      "Epoch: 2617 mean train loss:  1.06672737e-03, mean val. rec. loss:  1.03917071e-03\n",
      "Epoch: 2618 mean train loss:  1.06523318e-03, mean val. rec. loss:  1.03707314e-03\n",
      "Epoch: 2619 mean train loss:  1.06048435e-03, mean val. rec. loss:  1.03786922e-03\n",
      "Epoch: 2620 mean train loss:  1.05977125e-03, mean val. rec. loss:  1.03388952e-03\n",
      "Epoch: 2621 mean train loss:  1.05717671e-03, mean val. rec. loss:  1.03441869e-03\n",
      "Epoch: 2622 mean train loss:  1.05680113e-03, mean val. rec. loss:  1.03314454e-03\n",
      "Epoch: 2623 mean train loss:  1.05449780e-03, mean val. rec. loss:  1.03458195e-03\n",
      "Epoch: 2624 mean train loss:  1.05496060e-03, mean val. rec. loss:  1.02727268e-03\n",
      "Epoch: 2625 mean train loss:  1.05378282e-03, mean val. rec. loss:  1.03164054e-03\n",
      "Epoch: 2626 mean train loss:  1.05127455e-03, mean val. rec. loss:  1.02773062e-03\n",
      "Epoch: 2627 mean train loss:  1.04961296e-03, mean val. rec. loss:  1.02158123e-03\n",
      "Epoch: 2628 mean train loss:  1.04857167e-03, mean val. rec. loss:  1.02405882e-03\n",
      "Epoch: 2629 mean train loss:  1.04712670e-03, mean val. rec. loss:  1.02088640e-03\n",
      "Epoch: 2630 mean train loss:  1.04823170e-03, mean val. rec. loss:  1.02597322e-03\n",
      "Epoch: 2631 mean train loss:  1.04403193e-03, mean val. rec. loss:  1.01880686e-03\n",
      "Epoch: 2632 mean train loss:  1.04373427e-03, mean val. rec. loss:  1.01973967e-03\n",
      "Epoch: 2633 mean train loss:  1.04087186e-03, mean val. rec. loss:  1.02028098e-03\n",
      "Epoch: 2634 mean train loss:  1.03863110e-03, mean val. rec. loss:  1.01499485e-03\n",
      "Epoch: 2635 mean train loss:  1.03811758e-03, mean val. rec. loss:  1.01440185e-03\n",
      "Epoch: 2636 mean train loss:  1.03669431e-03, mean val. rec. loss:  1.01030127e-03\n",
      "Epoch: 2637 mean train loss:  1.03609392e-03, mean val. rec. loss:  1.01446502e-03\n",
      "Epoch: 2638 mean train loss:  1.03538590e-03, mean val. rec. loss:  1.01023331e-03\n",
      "Epoch: 2639 mean train loss:  1.03318657e-03, mean val. rec. loss:  1.01385713e-03\n",
      "Epoch: 2640 mean train loss:  1.03360717e-03, mean val. rec. loss:  1.00758207e-03\n",
      "Epoch: 2641 mean train loss:  1.02992957e-03, mean val. rec. loss:  1.00576020e-03\n",
      "Epoch: 2642 mean train loss:  1.02946560e-03, mean val. rec. loss:  1.00925637e-03\n",
      "Epoch: 2643 mean train loss:  1.02803665e-03, mean val. rec. loss:  1.00281246e-03\n",
      "Epoch: 2644 mean train loss:  1.02691484e-03, mean val. rec. loss:  1.00764996e-03\n",
      "Epoch: 2645 mean train loss:  1.02503031e-03, mean val. rec. loss:  1.00242532e-03\n",
      "Epoch: 2646 mean train loss:  1.02449224e-03, mean val. rec. loss:  9.99576726e-04\n",
      "Epoch: 2647 mean train loss:  1.02393118e-03, mean val. rec. loss:  1.00245156e-03\n",
      "Epoch: 2648 mean train loss:  1.02133806e-03, mean val. rec. loss:  1.00107978e-03\n",
      "Epoch: 2649 mean train loss:  1.02269165e-03, mean val. rec. loss:  9.97998729e-04\n",
      "Epoch: 2650 mean train loss:  1.01797451e-03, mean val. rec. loss:  9.98291228e-04\n",
      "Epoch: 2651 mean train loss:  1.01677581e-03, mean val. rec. loss:  9.95932265e-04\n",
      "Epoch: 2652 mean train loss:  1.01552791e-03, mean val. rec. loss:  9.94240075e-04\n",
      "Epoch: 2653 mean train loss:  1.01331139e-03, mean val. rec. loss:  9.94575242e-04\n",
      "Epoch: 2654 mean train loss:  1.01251044e-03, mean val. rec. loss:  9.88432620e-04\n",
      "Epoch: 2655 mean train loss:  1.01163291e-03, mean val. rec. loss:  9.90264590e-04\n",
      "Epoch: 2656 mean train loss:  1.00968534e-03, mean val. rec. loss:  9.91163530e-04\n",
      "Epoch: 2657 mean train loss:  1.00826843e-03, mean val. rec. loss:  9.83466462e-04\n",
      "Epoch: 2658 mean train loss:  1.00867971e-03, mean val. rec. loss:  9.84673965e-04\n",
      "Epoch: 2659 mean train loss:  1.00764467e-03, mean val. rec. loss:  9.86740211e-04\n",
      "Epoch: 2660 mean train loss:  1.00490294e-03, mean val. rec. loss:  9.82679899e-04\n",
      "Epoch: 2661 mean train loss:  1.00211888e-03, mean val. rec. loss:  9.84223005e-04\n",
      "Epoch: 2662 mean train loss:  1.00355691e-03, mean val. rec. loss:  9.78739959e-04\n",
      "Epoch: 2663 mean train loss:  1.00046767e-03, mean val. rec. loss:  9.79069311e-04\n",
      "Epoch: 2664 mean train loss:  1.00236963e-03, mean val. rec. loss:  9.81671636e-04\n",
      "Epoch: 2665 mean train loss:  9.99249818e-04, mean val. rec. loss:  9.80810276e-04\n",
      "Epoch: 2666 mean train loss:  9.96351697e-04, mean val. rec. loss:  9.73741747e-04\n",
      "Epoch: 2667 mean train loss:  9.95737638e-04, mean val. rec. loss:  9.76360862e-04\n",
      "Epoch: 2668 mean train loss:  9.94519172e-04, mean val. rec. loss:  9.75631286e-04\n",
      "Epoch: 2669 mean train loss:  9.93527199e-04, mean val. rec. loss:  9.71840431e-04\n",
      "Epoch: 2670 mean train loss:  9.90979992e-04, mean val. rec. loss:  9.70583355e-04\n",
      "Epoch: 2671 mean train loss:  9.89403462e-04, mean val. rec. loss:  9.68072182e-04\n",
      "Epoch: 2672 mean train loss:  9.90423646e-04, mean val. rec. loss:  9.67384984e-04\n",
      "Epoch: 2673 mean train loss:  9.88203432e-04, mean val. rec. loss:  9.72196532e-04\n",
      "Epoch: 2674 mean train loss:  9.85402373e-04, mean val. rec. loss:  9.66002592e-04\n",
      "Epoch: 2675 mean train loss:  9.86804756e-04, mean val. rec. loss:  9.64486599e-04\n",
      "Epoch: 2676 mean train loss:  9.86453387e-04, mean val. rec. loss:  9.65301147e-04\n",
      "Epoch: 2677 mean train loss:  9.81035559e-04, mean val. rec. loss:  9.62893847e-04\n",
      "Epoch: 2678 mean train loss:  9.81314833e-04, mean val. rec. loss:  9.59435033e-04\n",
      "Epoch: 2679 mean train loss:  9.79498256e-04, mean val. rec. loss:  9.58677690e-04\n",
      "Epoch: 2680 mean train loss:  9.77527832e-04, mean val. rec. loss:  9.62220241e-04\n",
      "Epoch: 2681 mean train loss:  9.77240375e-04, mean val. rec. loss:  9.61739915e-04\n",
      "Epoch: 2682 mean train loss:  9.75750337e-04, mean val. rec. loss:  9.57352359e-04\n",
      "Epoch: 2683 mean train loss:  9.76072972e-04, mean val. rec. loss:  9.51643615e-04\n",
      "Epoch: 2684 mean train loss:  9.77000674e-04, mean val. rec. loss:  9.54991288e-04\n",
      "Epoch: 2685 mean train loss:  9.75861354e-04, mean val. rec. loss:  9.57990783e-04\n",
      "Epoch: 2686 mean train loss:  9.70363594e-04, mean val. rec. loss:  9.53864034e-04\n",
      "Epoch: 2687 mean train loss:  9.72122063e-04, mean val. rec. loss:  9.50470785e-04\n",
      "Epoch: 2688 mean train loss:  9.68756434e-04, mean val. rec. loss:  9.51341449e-04\n",
      "Epoch: 2689 mean train loss:  9.68002975e-04, mean val. rec. loss:  9.47998646e-04\n",
      "Epoch: 2690 mean train loss:  9.67899249e-04, mean val. rec. loss:  9.49635666e-04\n",
      "Epoch: 2691 mean train loss:  9.65300797e-04, mean val. rec. loss:  9.48667381e-04\n",
      "Epoch: 2692 mean train loss:  9.62979905e-04, mean val. rec. loss:  9.45225794e-04\n",
      "Epoch: 2693 mean train loss:  9.63477766e-04, mean val. rec. loss:  9.41540628e-04\n",
      "Epoch: 2694 mean train loss:  9.61817032e-04, mean val. rec. loss:  9.43427260e-04\n",
      "Epoch: 2695 mean train loss:  9.61738842e-04, mean val. rec. loss:  9.38417417e-04\n",
      "Epoch: 2696 mean train loss:  9.57367002e-04, mean val. rec. loss:  9.43496169e-04\n",
      "Epoch: 2697 mean train loss:  9.59024588e-04, mean val. rec. loss:  9.41104424e-04\n",
      "Epoch: 2698 mean train loss:  9.56223857e-04, mean val. rec. loss:  9.39225932e-04\n",
      "Epoch: 2699 mean train loss:  9.54834023e-04, mean val. rec. loss:  9.33557313e-04\n",
      "Epoch: 2700 mean train loss:  9.53474424e-04, mean val. rec. loss:  9.39496843e-04\n",
      "Epoch: 2701 mean train loss:  9.52289050e-04, mean val. rec. loss:  9.33932967e-04\n",
      "Epoch: 2702 mean train loss:  9.50450667e-04, mean val. rec. loss:  9.32450411e-04\n",
      "Epoch: 2703 mean train loss:  9.50769354e-04, mean val. rec. loss:  9.32706565e-04\n",
      "Epoch: 2704 mean train loss:  9.48918599e-04, mean val. rec. loss:  9.32206686e-04\n",
      "Epoch: 2705 mean train loss:  9.47553719e-04, mean val. rec. loss:  9.33804817e-04\n",
      "Epoch: 2706 mean train loss:  9.46898068e-04, mean val. rec. loss:  9.32943821e-04\n",
      "Epoch: 2707 mean train loss:  9.45713849e-04, mean val. rec. loss:  9.25270741e-04\n",
      "Epoch: 2708 mean train loss:  9.44054716e-04, mean val. rec. loss:  9.26564743e-04\n",
      "Epoch: 2709 mean train loss:  9.44195795e-04, mean val. rec. loss:  9.27742443e-04\n",
      "Epoch: 2710 mean train loss:  9.42547325e-04, mean val. rec. loss:  9.28024839e-04\n",
      "Epoch: 2711 mean train loss:  9.40290035e-04, mean val. rec. loss:  9.23265626e-04\n",
      "Epoch: 2712 mean train loss:  9.38901157e-04, mean val. rec. loss:  9.23131007e-04\n",
      "Epoch: 2713 mean train loss:  9.37959636e-04, mean val. rec. loss:  9.19691600e-04\n",
      "Epoch: 2714 mean train loss:  9.37593707e-04, mean val. rec. loss:  9.22236355e-04\n",
      "Epoch: 2715 mean train loss:  9.35976076e-04, mean val. rec. loss:  9.21452554e-04\n",
      "Epoch: 2716 mean train loss:  9.35897277e-04, mean val. rec. loss:  9.17568803e-04\n",
      "Epoch: 2717 mean train loss:  9.32738694e-04, mean val. rec. loss:  9.18711322e-04\n",
      "Epoch: 2718 mean train loss:  9.33067680e-04, mean val. rec. loss:  9.13487266e-04\n",
      "Epoch: 2719 mean train loss:  9.29280183e-04, mean val. rec. loss:  9.16597901e-04\n",
      "Epoch: 2720 mean train loss:  9.28974791e-04, mean val. rec. loss:  9.12161280e-04\n",
      "Epoch: 2721 mean train loss:  9.29485338e-04, mean val. rec. loss:  9.12338641e-04\n",
      "Epoch: 2722 mean train loss:  9.27570220e-04, mean val. rec. loss:  9.13423227e-04\n",
      "Epoch: 2723 mean train loss:  9.26366328e-04, mean val. rec. loss:  9.10158419e-04\n",
      "Epoch: 2724 mean train loss:  9.27855053e-04, mean val. rec. loss:  9.14385551e-04\n",
      "Epoch: 2725 mean train loss:  9.25719520e-04, mean val. rec. loss:  9.06907276e-04\n",
      "Epoch: 2726 mean train loss:  9.24863745e-04, mean val. rec. loss:  9.08838685e-04\n",
      "Epoch: 2727 mean train loss:  9.21639358e-04, mean val. rec. loss:  9.06093455e-04\n",
      "Epoch: 2728 mean train loss:  9.21913801e-04, mean val. rec. loss:  9.03388494e-04\n",
      "Epoch: 2729 mean train loss:  9.18443000e-04, mean val. rec. loss:  9.06763716e-04\n",
      "Epoch: 2730 mean train loss:  9.17201023e-04, mean val. rec. loss:  9.00762692e-04\n",
      "Epoch: 2731 mean train loss:  9.17478272e-04, mean val. rec. loss:  9.02723539e-04\n",
      "Epoch: 2732 mean train loss:  9.14861640e-04, mean val. rec. loss:  9.00760002e-04\n",
      "Epoch: 2733 mean train loss:  9.13690580e-04, mean val. rec. loss:  9.01497283e-04\n",
      "Epoch: 2734 mean train loss:  9.13687050e-04, mean val. rec. loss:  8.99104665e-04\n",
      "Epoch: 2735 mean train loss:  9.12044902e-04, mean val. rec. loss:  8.98607911e-04\n",
      "Epoch: 2736 mean train loss:  9.10919019e-04, mean val. rec. loss:  8.92816448e-04\n",
      "Epoch: 2737 mean train loss:  9.09875901e-04, mean val. rec. loss:  8.94633735e-04\n",
      "Epoch: 2738 mean train loss:  9.08721625e-04, mean val. rec. loss:  8.96348895e-04\n",
      "Epoch: 2739 mean train loss:  9.07711607e-04, mean val. rec. loss:  8.93499938e-04\n",
      "Epoch: 2740 mean train loss:  9.06104124e-04, mean val. rec. loss:  8.92391656e-04\n",
      "Epoch: 2741 mean train loss:  9.06156160e-04, mean val. rec. loss:  8.90701355e-04\n",
      "Epoch: 2742 mean train loss:  9.04562354e-04, mean val. rec. loss:  8.90666246e-04\n",
      "Epoch: 2743 mean train loss:  9.04528627e-04, mean val. rec. loss:  8.89325578e-04\n",
      "Epoch: 2744 mean train loss:  9.02519804e-04, mean val. rec. loss:  8.87302073e-04\n",
      "Epoch: 2745 mean train loss:  9.03544668e-04, mean val. rec. loss:  8.91715797e-04\n",
      "Epoch: 2746 mean train loss:  8.99434654e-04, mean val. rec. loss:  8.86142981e-04\n",
      "Epoch: 2747 mean train loss:  8.99345993e-04, mean val. rec. loss:  8.86021445e-04\n",
      "Epoch: 2748 mean train loss:  8.97403766e-04, mean val. rec. loss:  8.83485922e-04\n",
      "Epoch: 2749 mean train loss:  8.97932265e-04, mean val. rec. loss:  8.84280699e-04\n",
      "Epoch: 2750 mean train loss:  8.97166611e-04, mean val. rec. loss:  8.82332790e-04\n",
      "Epoch: 2751 mean train loss:  8.93437673e-04, mean val. rec. loss:  8.78904069e-04\n",
      "Epoch: 2752 mean train loss:  8.92676672e-04, mean val. rec. loss:  8.79488195e-04\n",
      "Epoch: 2753 mean train loss:  8.91205196e-04, mean val. rec. loss:  8.78398593e-04\n",
      "Epoch: 2754 mean train loss:  8.89598022e-04, mean val. rec. loss:  8.78298719e-04\n",
      "Epoch: 2755 mean train loss:  8.89353099e-04, mean val. rec. loss:  8.78579297e-04\n",
      "Epoch: 2756 mean train loss:  8.89046106e-04, mean val. rec. loss:  8.75471496e-04\n",
      "Epoch: 2757 mean train loss:  8.89006170e-04, mean val. rec. loss:  8.74466576e-04\n",
      "Epoch: 2758 mean train loss:  8.86319530e-04, mean val. rec. loss:  8.76422917e-04\n",
      "Epoch: 2759 mean train loss:  8.86992452e-04, mean val. rec. loss:  8.75815822e-04\n",
      "Epoch: 2760 mean train loss:  8.84770847e-04, mean val. rec. loss:  8.70959788e-04\n",
      "Epoch: 2761 mean train loss:  8.83804364e-04, mean val. rec. loss:  8.67417964e-04\n",
      "Epoch: 2762 mean train loss:  8.81896214e-04, mean val. rec. loss:  8.71715604e-04\n",
      "Epoch: 2763 mean train loss:  8.80254612e-04, mean val. rec. loss:  8.71304477e-04\n",
      "Epoch: 2764 mean train loss:  8.79976280e-04, mean val. rec. loss:  8.65232218e-04\n",
      "Epoch: 2765 mean train loss:  8.77862776e-04, mean val. rec. loss:  8.66248041e-04\n",
      "Epoch: 2766 mean train loss:  8.77557602e-04, mean val. rec. loss:  8.68100291e-04\n",
      "Epoch: 2767 mean train loss:  8.77480281e-04, mean val. rec. loss:  8.65622410e-04\n",
      "Epoch: 2768 mean train loss:  8.76704133e-04, mean val. rec. loss:  8.61217118e-04\n",
      "Epoch: 2769 mean train loss:  8.74467139e-04, mean val. rec. loss:  8.63936907e-04\n",
      "Epoch: 2770 mean train loss:  8.72570953e-04, mean val. rec. loss:  8.61625410e-04\n",
      "Epoch: 2771 mean train loss:  8.71387684e-04, mean val. rec. loss:  8.60318469e-04\n",
      "Epoch: 2772 mean train loss:  8.71283472e-04, mean val. rec. loss:  8.64761632e-04\n",
      "Epoch: 2773 mean train loss:  8.70593943e-04, mean val. rec. loss:  8.56876519e-04\n",
      "Epoch: 2774 mean train loss:  8.69061452e-04, mean val. rec. loss:  8.59121578e-04\n",
      "Epoch: 2775 mean train loss:  8.67094526e-04, mean val. rec. loss:  8.57141105e-04\n",
      "Epoch: 2776 mean train loss:  8.67065593e-04, mean val. rec. loss:  8.55246986e-04\n",
      "Epoch: 2777 mean train loss:  8.65314574e-04, mean val. rec. loss:  8.56656054e-04\n",
      "Epoch: 2778 mean train loss:  8.65875829e-04, mean val. rec. loss:  8.50764353e-04\n",
      "Epoch: 2779 mean train loss:  8.63651171e-04, mean val. rec. loss:  8.53302784e-04\n",
      "Epoch: 2780 mean train loss:  8.62292122e-04, mean val. rec. loss:  8.48813028e-04\n",
      "Epoch: 2781 mean train loss:  8.61272220e-04, mean val. rec. loss:  8.52922986e-04\n",
      "Epoch: 2782 mean train loss:  8.60529196e-04, mean val. rec. loss:  8.48342587e-04\n",
      "Epoch: 2783 mean train loss:  8.59459419e-04, mean val. rec. loss:  8.48933109e-04\n",
      "Epoch: 2784 mean train loss:  8.58102926e-04, mean val. rec. loss:  8.50960976e-04\n",
      "Epoch: 2785 mean train loss:  8.57458137e-04, mean val. rec. loss:  8.45061497e-04\n",
      "Epoch: 2786 mean train loss:  8.56934959e-04, mean val. rec. loss:  8.46805369e-04\n",
      "Epoch: 2787 mean train loss:  8.54862006e-04, mean val. rec. loss:  8.45827271e-04\n",
      "Epoch: 2788 mean train loss:  8.57584106e-04, mean val. rec. loss:  8.42404656e-04\n",
      "Epoch: 2789 mean train loss:  8.53266858e-04, mean val. rec. loss:  8.45087592e-04\n",
      "Epoch: 2790 mean train loss:  8.52408909e-04, mean val. rec. loss:  8.42604695e-04\n",
      "Epoch: 2791 mean train loss:  8.50527005e-04, mean val. rec. loss:  8.39795717e-04\n",
      "Epoch: 2792 mean train loss:  8.50474196e-04, mean val. rec. loss:  8.40515044e-04\n",
      "Epoch: 2793 mean train loss:  8.48963093e-04, mean val. rec. loss:  8.37694726e-04\n",
      "Epoch: 2794 mean train loss:  8.49786159e-04, mean val. rec. loss:  8.40244642e-04\n",
      "Epoch: 2795 mean train loss:  8.46129707e-04, mean val. rec. loss:  8.39438671e-04\n",
      "Epoch: 2796 mean train loss:  8.46277195e-04, mean val. rec. loss:  8.36062722e-04\n",
      "Epoch: 2797 mean train loss:  8.46743875e-04, mean val. rec. loss:  8.36750211e-04\n",
      "Epoch: 2798 mean train loss:  8.44033042e-04, mean val. rec. loss:  8.36785537e-04\n",
      "Epoch: 2799 mean train loss:  8.42107908e-04, mean val. rec. loss:  8.34435661e-04\n",
      "Epoch: 2800 mean train loss:  8.42537927e-04, mean val. rec. loss:  8.30181052e-04\n",
      "Epoch: 2801 mean train loss:  8.40872080e-04, mean val. rec. loss:  8.32324929e-04\n",
      "Epoch: 2802 mean train loss:  8.40857316e-04, mean val. rec. loss:  8.29504321e-04\n",
      "Epoch: 2803 mean train loss:  8.42588871e-04, mean val. rec. loss:  8.27415251e-04\n",
      "Epoch: 2804 mean train loss:  8.37994787e-04, mean val. rec. loss:  8.27499134e-04\n",
      "Epoch: 2805 mean train loss:  8.37278390e-04, mean val. rec. loss:  8.29897857e-04\n",
      "Epoch: 2806 mean train loss:  8.35564920e-04, mean val. rec. loss:  8.26564649e-04\n",
      "Epoch: 2807 mean train loss:  8.34742113e-04, mean val. rec. loss:  8.24137868e-04\n",
      "Epoch: 2808 mean train loss:  8.34955542e-04, mean val. rec. loss:  8.24852469e-04\n",
      "Epoch: 2809 mean train loss:  8.32976576e-04, mean val. rec. loss:  8.25433542e-04\n",
      "Epoch: 2810 mean train loss:  8.31441274e-04, mean val. rec. loss:  8.22286635e-04\n",
      "Epoch: 2811 mean train loss:  8.32743165e-04, mean val. rec. loss:  8.22691220e-04\n",
      "Epoch: 2812 mean train loss:  8.30463014e-04, mean val. rec. loss:  8.22712735e-04\n",
      "Epoch: 2813 mean train loss:  8.26909447e-04, mean val. rec. loss:  8.20337854e-04\n",
      "Epoch: 2814 mean train loss:  8.27533253e-04, mean val. rec. loss:  8.21106027e-04\n",
      "Epoch: 2815 mean train loss:  8.27730011e-04, mean val. rec. loss:  8.16799737e-04\n",
      "Epoch: 2816 mean train loss:  8.26451159e-04, mean val. rec. loss:  8.16564008e-04\n",
      "Epoch: 2817 mean train loss:  8.25316105e-04, mean val. rec. loss:  8.17459240e-04\n",
      "Epoch: 2818 mean train loss:  8.25394641e-04, mean val. rec. loss:  8.16507238e-04\n",
      "Epoch: 2819 mean train loss:  8.23154645e-04, mean val. rec. loss:  8.14235501e-04\n",
      "Epoch: 2820 mean train loss:  8.22328377e-04, mean val. rec. loss:  8.16783527e-04\n",
      "Epoch: 2821 mean train loss:  8.20551172e-04, mean val. rec. loss:  8.10874526e-04\n",
      "Epoch: 2822 mean train loss:  8.24503482e-04, mean val. rec. loss:  8.13225857e-04\n",
      "Epoch: 2823 mean train loss:  8.24108793e-04, mean val. rec. loss:  8.08477765e-04\n",
      "Epoch: 2824 mean train loss:  8.17890915e-04, mean val. rec. loss:  8.13471399e-04\n",
      "Epoch: 2825 mean train loss:  8.17577726e-04, mean val. rec. loss:  8.09544543e-04\n",
      "Epoch: 2826 mean train loss:  8.17483280e-04, mean val. rec. loss:  8.08069619e-04\n",
      "Epoch: 2827 mean train loss:  8.15870111e-04, mean val. rec. loss:  8.11281582e-04\n",
      "Epoch: 2828 mean train loss:  8.16357933e-04, mean val. rec. loss:  8.04591906e-04\n",
      "Epoch: 2829 mean train loss:  8.17265748e-04, mean val. rec. loss:  8.10663148e-04\n",
      "Epoch: 2830 mean train loss:  8.15661608e-04, mean val. rec. loss:  8.03302701e-04\n",
      "Epoch: 2831 mean train loss:  8.11911819e-04, mean val. rec. loss:  8.04968869e-04\n",
      "Epoch: 2832 mean train loss:  8.09327282e-04, mean val. rec. loss:  8.02530603e-04\n",
      "Epoch: 2833 mean train loss:  8.08846101e-04, mean val. rec. loss:  8.01370929e-04\n",
      "Epoch: 2834 mean train loss:  8.09951816e-04, mean val. rec. loss:  8.00964164e-04\n",
      "Epoch: 2835 mean train loss:  8.08856926e-04, mean val. rec. loss:  8.00586038e-04\n",
      "Epoch: 2836 mean train loss:  8.06693602e-04, mean val. rec. loss:  7.98482866e-04\n",
      "Epoch: 2837 mean train loss:  8.06140030e-04, mean val. rec. loss:  8.01689959e-04\n",
      "Epoch: 2838 mean train loss:  8.05574809e-04, mean val. rec. loss:  7.97103891e-04\n",
      "Epoch: 2839 mean train loss:  8.03341218e-04, mean val. rec. loss:  7.94907241e-04\n",
      "Epoch: 2840 mean train loss:  8.03346308e-04, mean val. rec. loss:  7.97871191e-04\n",
      "Epoch: 2841 mean train loss:  8.01859922e-04, mean val. rec. loss:  7.94304435e-04\n",
      "Epoch: 2842 mean train loss:  8.01593711e-04, mean val. rec. loss:  7.95120582e-04\n",
      "Epoch: 2843 mean train loss:  8.00162909e-04, mean val. rec. loss:  7.95388367e-04\n",
      "Epoch: 2844 mean train loss:  7.99162084e-04, mean val. rec. loss:  7.91319114e-04\n",
      "Epoch: 2845 mean train loss:  7.98601029e-04, mean val. rec. loss:  7.94974696e-04\n",
      "Epoch: 2846 mean train loss:  7.97409701e-04, mean val. rec. loss:  7.87789938e-04\n",
      "Epoch: 2847 mean train loss:  7.95670763e-04, mean val. rec. loss:  7.89052248e-04\n",
      "Epoch: 2848 mean train loss:  7.94896895e-04, mean val. rec. loss:  7.90762901e-04\n",
      "Epoch: 2849 mean train loss:  7.95143410e-04, mean val. rec. loss:  7.88240535e-04\n",
      "Epoch: 2850 mean train loss:  7.92950137e-04, mean val. rec. loss:  7.88606812e-04\n",
      "Epoch: 2851 mean train loss:  7.93190029e-04, mean val. rec. loss:  7.83522972e-04\n",
      "Epoch: 2852 mean train loss:  7.91096644e-04, mean val. rec. loss:  7.84826206e-04\n",
      "Epoch: 2853 mean train loss:  7.90680953e-04, mean val. rec. loss:  7.85321361e-04\n",
      "Epoch: 2854 mean train loss:  7.91284100e-04, mean val. rec. loss:  7.84033682e-04\n",
      "Epoch: 2855 mean train loss:  7.88818463e-04, mean val. rec. loss:  7.82245543e-04\n",
      "Epoch: 2856 mean train loss:  7.87231876e-04, mean val. rec. loss:  7.84359691e-04\n",
      "Epoch: 2857 mean train loss:  7.89762562e-04, mean val. rec. loss:  7.78558923e-04\n",
      "Epoch: 2858 mean train loss:  7.87021217e-04, mean val. rec. loss:  7.80734274e-04\n",
      "Epoch: 2859 mean train loss:  7.85034673e-04, mean val. rec. loss:  7.81253198e-04\n",
      "Epoch: 2860 mean train loss:  7.84139393e-04, mean val. rec. loss:  7.78324430e-04\n",
      "Epoch: 2861 mean train loss:  7.83851755e-04, mean val. rec. loss:  7.77254090e-04\n",
      "Epoch: 2862 mean train loss:  7.83050453e-04, mean val. rec. loss:  7.79509326e-04\n",
      "Epoch: 2863 mean train loss:  7.83619395e-04, mean val. rec. loss:  7.76515356e-04\n",
      "Epoch: 2864 mean train loss:  7.81481874e-04, mean val. rec. loss:  7.80521806e-04\n",
      "Epoch: 2865 mean train loss:  7.80473740e-04, mean val. rec. loss:  7.69279497e-04\n",
      "Epoch: 2866 mean train loss:  7.77807100e-04, mean val. rec. loss:  7.76675344e-04\n",
      "Epoch: 2867 mean train loss:  7.78187990e-04, mean val. rec. loss:  7.75243669e-04\n",
      "Epoch: 2868 mean train loss:  7.79407242e-04, mean val. rec. loss:  7.72782070e-04\n",
      "Epoch: 2869 mean train loss:  7.78404257e-04, mean val. rec. loss:  7.70142020e-04\n",
      "Epoch: 2870 mean train loss:  7.75600751e-04, mean val. rec. loss:  7.67312035e-04\n",
      "Epoch: 2871 mean train loss:  7.74095383e-04, mean val. rec. loss:  7.72968443e-04\n",
      "Epoch: 2872 mean train loss:  7.74613276e-04, mean val. rec. loss:  7.68867208e-04\n",
      "Epoch: 2873 mean train loss:  7.74331546e-04, mean val. rec. loss:  7.68004540e-04\n",
      "Epoch: 2874 mean train loss:  7.73583717e-04, mean val. rec. loss:  7.69430544e-04\n",
      "Epoch: 2875 mean train loss:  7.71995961e-04, mean val. rec. loss:  7.65409411e-04\n",
      "Epoch: 2876 mean train loss:  7.70489702e-04, mean val. rec. loss:  7.65353514e-04\n",
      "Epoch: 2877 mean train loss:  7.70210983e-04, mean val. rec. loss:  7.66471246e-04\n",
      "Epoch: 2878 mean train loss:  7.69284082e-04, mean val. rec. loss:  7.60612837e-04\n",
      "Epoch: 2879 mean train loss:  7.67202049e-04, mean val. rec. loss:  7.65140173e-04\n",
      "Epoch: 2880 mean train loss:  7.67012064e-04, mean val. rec. loss:  7.64249156e-04\n",
      "Epoch: 2881 mean train loss:  7.65614398e-04, mean val. rec. loss:  7.60313796e-04\n",
      "Epoch: 2882 mean train loss:  7.64812815e-04, mean val. rec. loss:  7.59802795e-04\n",
      "Epoch: 2883 mean train loss:  7.63373998e-04, mean val. rec. loss:  7.61511413e-04\n",
      "Epoch: 2884 mean train loss:  7.62980973e-04, mean val. rec. loss:  7.56989383e-04\n",
      "Epoch: 2885 mean train loss:  7.63777316e-04, mean val. rec. loss:  7.58296179e-04\n",
      "Epoch: 2886 mean train loss:  7.61913961e-04, mean val. rec. loss:  7.57244884e-04\n",
      "Epoch: 2887 mean train loss:  7.61459116e-04, mean val. rec. loss:  7.54392511e-04\n",
      "Epoch: 2888 mean train loss:  7.59886525e-04, mean val. rec. loss:  7.56139581e-04\n",
      "Epoch: 2889 mean train loss:  7.59297829e-04, mean val. rec. loss:  7.57701949e-04\n",
      "Epoch: 2890 mean train loss:  7.57175456e-04, mean val. rec. loss:  7.53193367e-04\n",
      "Epoch: 2891 mean train loss:  7.57782068e-04, mean val. rec. loss:  7.53934064e-04\n",
      "Epoch: 2892 mean train loss:  7.57861273e-04, mean val. rec. loss:  7.50588353e-04\n",
      "Epoch: 2893 mean train loss:  7.54044656e-04, mean val. rec. loss:  7.53482958e-04\n",
      "Epoch: 2894 mean train loss:  7.54078384e-04, mean val. rec. loss:  7.48114760e-04\n",
      "Epoch: 2895 mean train loss:  7.53452108e-04, mean val. rec. loss:  7.49526664e-04\n",
      "Epoch: 2896 mean train loss:  7.50983864e-04, mean val. rec. loss:  7.51782627e-04\n",
      "Epoch: 2897 mean train loss:  7.51690718e-04, mean val. rec. loss:  7.48467227e-04\n",
      "Epoch: 2898 mean train loss:  7.50828744e-04, mean val. rec. loss:  7.47048855e-04\n",
      "Epoch: 2899 mean train loss:  7.50769039e-04, mean val. rec. loss:  7.45620015e-04\n",
      "Epoch: 2900 mean train loss:  7.49334047e-04, mean val. rec. loss:  7.45408273e-04\n",
      "Epoch: 2901 mean train loss:  7.48415006e-04, mean val. rec. loss:  7.42841421e-04\n",
      "Epoch: 2902 mean train loss:  7.48086525e-04, mean val. rec. loss:  7.45948786e-04\n",
      "Epoch: 2903 mean train loss:  7.47242908e-04, mean val. rec. loss:  7.42879365e-04\n",
      "Epoch: 2904 mean train loss:  7.44894474e-04, mean val. rec. loss:  7.43636707e-04\n",
      "Epoch: 2905 mean train loss:  7.45495870e-04, mean val. rec. loss:  7.41617999e-04\n",
      "Epoch: 2906 mean train loss:  7.43221387e-04, mean val. rec. loss:  7.38269381e-04\n",
      "Epoch: 2907 mean train loss:  7.42207508e-04, mean val. rec. loss:  7.40875704e-04\n",
      "Epoch: 2908 mean train loss:  7.41210808e-04, mean val. rec. loss:  7.37768920e-04\n",
      "Epoch: 2909 mean train loss:  7.43499624e-04, mean val. rec. loss:  7.40324433e-04\n",
      "Epoch: 2910 mean train loss:  7.41572302e-04, mean val. rec. loss:  7.36168681e-04\n",
      "Epoch: 2911 mean train loss:  7.40689581e-04, mean val. rec. loss:  7.41331316e-04\n",
      "Epoch: 2912 mean train loss:  7.40014408e-04, mean val. rec. loss:  7.37223974e-04\n",
      "Epoch: 2913 mean train loss:  7.38376117e-04, mean val. rec. loss:  7.35227437e-04\n",
      "Epoch: 2914 mean train loss:  7.38012212e-04, mean val. rec. loss:  7.33819531e-04\n",
      "Epoch: 2915 mean train loss:  7.39191956e-04, mean val. rec. loss:  7.32905836e-04\n",
      "Epoch: 2916 mean train loss:  7.36099105e-04, mean val. rec. loss:  7.35816287e-04\n",
      "Epoch: 2917 mean train loss:  7.34912188e-04, mean val. rec. loss:  7.31630950e-04\n",
      "Epoch: 2918 mean train loss:  7.34114435e-04, mean val. rec. loss:  7.31464130e-04\n",
      "Epoch: 2919 mean train loss:  7.32352495e-04, mean val. rec. loss:  7.30793651e-04\n",
      "Epoch: 2920 mean train loss:  7.32354510e-04, mean val. rec. loss:  7.29719168e-04\n",
      "Epoch: 2921 mean train loss:  7.30984914e-04, mean val. rec. loss:  7.28969894e-04\n",
      "Epoch: 2922 mean train loss:  7.30461599e-04, mean val. rec. loss:  7.29273732e-04\n",
      "Epoch: 2923 mean train loss:  7.29788199e-04, mean val. rec. loss:  7.27575945e-04\n",
      "Epoch: 2924 mean train loss:  7.28981845e-04, mean val. rec. loss:  7.26398462e-04\n",
      "Epoch: 2925 mean train loss:  7.28035903e-04, mean val. rec. loss:  7.26611294e-04\n",
      "Epoch: 2926 mean train loss:  7.28831223e-04, mean val. rec. loss:  7.21995423e-04\n",
      "Epoch: 2927 mean train loss:  7.27139773e-04, mean val. rec. loss:  7.26144343e-04\n",
      "Epoch: 2928 mean train loss:  7.27650360e-04, mean val. rec. loss:  7.26562956e-04\n",
      "Epoch: 2929 mean train loss:  7.24814091e-04, mean val. rec. loss:  7.21602324e-04\n",
      "Epoch: 2930 mean train loss:  7.26406250e-04, mean val. rec. loss:  7.21912704e-04\n",
      "Epoch: 2931 mean train loss:  7.26781922e-04, mean val. rec. loss:  7.24112115e-04\n",
      "Epoch: 2932 mean train loss:  7.24938572e-04, mean val. rec. loss:  7.18355978e-04\n",
      "Epoch: 2933 mean train loss:  7.25366093e-04, mean val. rec. loss:  7.20065251e-04\n",
      "Epoch: 2934 mean train loss:  7.21440429e-04, mean val. rec. loss:  7.19264077e-04\n",
      "Epoch: 2935 mean train loss:  7.21660157e-04, mean val. rec. loss:  7.19224098e-04\n",
      "Epoch: 2936 mean train loss:  7.20002385e-04, mean val. rec. loss:  7.19939935e-04\n",
      "Epoch: 2937 mean train loss:  7.19493658e-04, mean val. rec. loss:  7.17745830e-04\n",
      "Epoch: 2938 mean train loss:  7.20107853e-04, mean val. rec. loss:  7.12934354e-04\n",
      "Epoch: 2939 mean train loss:  7.19126419e-04, mean val. rec. loss:  7.18157757e-04\n",
      "Epoch: 2940 mean train loss:  7.16434348e-04, mean val. rec. loss:  7.14622692e-04\n",
      "Epoch: 2941 mean train loss:  7.17337342e-04, mean val. rec. loss:  7.16231800e-04\n",
      "Epoch: 2942 mean train loss:  7.14990482e-04, mean val. rec. loss:  7.13143625e-04\n",
      "Epoch: 2943 mean train loss:  7.15244484e-04, mean val. rec. loss:  7.11824254e-04\n",
      "Epoch: 2944 mean train loss:  7.13065740e-04, mean val. rec. loss:  7.14807176e-04\n",
      "Epoch: 2945 mean train loss:  7.13704079e-04, mean val. rec. loss:  7.10699180e-04\n",
      "Epoch: 2946 mean train loss:  7.13920687e-04, mean val. rec. loss:  7.10793675e-04\n",
      "Epoch: 2947 mean train loss:  7.12064305e-04, mean val. rec. loss:  7.11271748e-04\n",
      "Epoch: 2948 mean train loss:  7.10415662e-04, mean val. rec. loss:  7.09065722e-04\n",
      "Epoch: 2949 mean train loss:  7.08655514e-04, mean val. rec. loss:  7.10104588e-04\n",
      "Epoch: 2950 mean train loss:  7.08495268e-04, mean val. rec. loss:  7.05900498e-04\n",
      "Epoch: 2951 mean train loss:  7.08436537e-04, mean val. rec. loss:  7.08003233e-04\n",
      "Epoch: 2952 mean train loss:  7.08513899e-04, mean val. rec. loss:  7.04206999e-04\n",
      "Epoch: 2953 mean train loss:  7.06435278e-04, mean val. rec. loss:  7.09093780e-04\n",
      "Epoch: 2954 mean train loss:  7.05389768e-04, mean val. rec. loss:  7.03492106e-04\n",
      "Epoch: 2955 mean train loss:  7.04864743e-04, mean val. rec. loss:  7.02861751e-04\n",
      "Epoch: 2956 mean train loss:  7.03976841e-04, mean val. rec. loss:  7.06650280e-04\n",
      "Epoch: 2957 mean train loss:  7.02119677e-04, mean val. rec. loss:  7.02975218e-04\n",
      "Epoch: 2958 mean train loss:  7.02616132e-04, mean val. rec. loss:  7.00278108e-04\n",
      "Epoch: 2959 mean train loss:  7.00731752e-04, mean val. rec. loss:  7.00634427e-04\n",
      "Epoch: 2960 mean train loss:  7.02435635e-04, mean val. rec. loss:  7.00477129e-04\n",
      "Epoch: 2961 mean train loss:  6.99533848e-04, mean val. rec. loss:  6.97784308e-04\n",
      "Epoch: 2962 mean train loss:  6.99715887e-04, mean val. rec. loss:  6.98443012e-04\n",
      "Epoch: 2963 mean train loss:  6.99463076e-04, mean val. rec. loss:  7.01565132e-04\n",
      "Epoch: 2964 mean train loss:  7.00601264e-04, mean val. rec. loss:  6.98192817e-04\n",
      "Epoch: 2965 mean train loss:  6.97480380e-04, mean val. rec. loss:  6.93715491e-04\n",
      "Epoch: 2966 mean train loss:  6.96179758e-04, mean val. rec. loss:  7.02234885e-04\n",
      "Epoch: 2967 mean train loss:  6.95713720e-04, mean val. rec. loss:  6.93291280e-04\n",
      "Epoch: 2968 mean train loss:  6.96260819e-04, mean val. rec. loss:  6.94519427e-04\n",
      "Epoch: 2969 mean train loss:  6.95612905e-04, mean val. rec. loss:  6.92655691e-04\n",
      "Epoch: 2970 mean train loss:  6.94424332e-04, mean val. rec. loss:  6.92511114e-04\n",
      "Epoch: 2971 mean train loss:  6.91929934e-04, mean val. rec. loss:  6.92880299e-04\n",
      "Epoch: 2972 mean train loss:  6.92652650e-04, mean val. rec. loss:  6.92347201e-04\n",
      "Epoch: 2973 mean train loss:  6.90437931e-04, mean val. rec. loss:  6.91782556e-04\n",
      "Epoch: 2974 mean train loss:  6.90962419e-04, mean val. rec. loss:  6.91917829e-04\n",
      "Epoch: 2975 mean train loss:  6.89767438e-04, mean val. rec. loss:  6.90744854e-04\n",
      "Epoch: 2976 mean train loss:  6.89482532e-04, mean val. rec. loss:  6.88316328e-04\n",
      "Epoch: 2977 mean train loss:  6.87431235e-04, mean val. rec. loss:  6.88072094e-04\n",
      "Epoch: 2978 mean train loss:  6.86986229e-04, mean val. rec. loss:  6.85525667e-04\n",
      "Epoch: 2979 mean train loss:  6.86117918e-04, mean val. rec. loss:  6.87196923e-04\n",
      "Epoch: 2980 mean train loss:  6.85527097e-04, mean val. rec. loss:  6.88561361e-04\n",
      "Epoch: 2981 mean train loss:  6.86050903e-04, mean val. rec. loss:  6.84882882e-04\n",
      "Epoch: 2982 mean train loss:  6.83690725e-04, mean val. rec. loss:  6.87311699e-04\n",
      "Epoch: 2983 mean train loss:  6.84737882e-04, mean val. rec. loss:  6.83776344e-04\n",
      "Epoch: 2984 mean train loss:  6.85755373e-04, mean val. rec. loss:  6.83983506e-04\n",
      "Epoch: 2985 mean train loss:  6.83209062e-04, mean val. rec. loss:  6.83600292e-04\n",
      "Epoch: 2986 mean train loss:  6.82568648e-04, mean val. rec. loss:  6.83380627e-04\n",
      "Epoch: 2987 mean train loss:  6.81161835e-04, mean val. rec. loss:  6.83890610e-04\n",
      "Epoch: 2988 mean train loss:  6.79356284e-04, mean val. rec. loss:  6.79123838e-04\n",
      "Epoch: 2989 mean train loss:  6.80037079e-04, mean val. rec. loss:  6.81030169e-04\n",
      "Epoch: 2990 mean train loss:  6.80480648e-04, mean val. rec. loss:  6.77156012e-04\n",
      "Epoch: 2991 mean train loss:  6.79313336e-04, mean val. rec. loss:  6.80027357e-04\n",
      "Epoch: 2992 mean train loss:  6.79480442e-04, mean val. rec. loss:  6.77837032e-04\n",
      "Epoch: 2993 mean train loss:  6.77667999e-04, mean val. rec. loss:  6.77379602e-04\n",
      "Epoch: 2994 mean train loss:  6.77489504e-04, mean val. rec. loss:  6.77801123e-04\n",
      "Epoch: 2995 mean train loss:  6.75248394e-04, mean val. rec. loss:  6.73410006e-04\n",
      "Epoch: 2996 mean train loss:  6.73833811e-04, mean val. rec. loss:  6.78345561e-04\n",
      "Epoch: 2997 mean train loss:  6.75450305e-04, mean val. rec. loss:  6.74415362e-04\n",
      "Epoch: 2998 mean train loss:  6.74403644e-04, mean val. rec. loss:  6.75778781e-04\n",
      "Epoch: 2999 mean train loss:  6.72115156e-04, mean val. rec. loss:  6.74784474e-04\n",
      "Epoch: 3000 mean train loss:  6.71026257e-04, mean val. rec. loss:  6.70115541e-04\n",
      "Epoch: 3001 mean train loss:  6.71574534e-04, mean val. rec. loss:  6.75052768e-04\n",
      "Epoch: 3002 mean train loss:  6.69121442e-04, mean val. rec. loss:  6.73850280e-04\n",
      "Epoch: 3003 mean train loss:  6.69970308e-04, mean val. rec. loss:  6.68571926e-04\n",
      "Epoch: 3004 mean train loss:  6.70365782e-04, mean val. rec. loss:  6.70835594e-04\n",
      "Epoch: 3005 mean train loss:  6.70225738e-04, mean val. rec. loss:  6.68232034e-04\n",
      "Epoch: 3006 mean train loss:  6.66993596e-04, mean val. rec. loss:  6.71140304e-04\n",
      "Epoch: 3007 mean train loss:  6.67866479e-04, mean val. rec. loss:  6.65591112e-04\n",
      "Epoch: 3008 mean train loss:  6.65596308e-04, mean val. rec. loss:  6.64869242e-04\n",
      "Epoch: 3009 mean train loss:  6.66340811e-04, mean val. rec. loss:  6.69133518e-04\n",
      "Epoch: 3010 mean train loss:  6.63779708e-04, mean val. rec. loss:  6.66709353e-04\n",
      "Epoch: 3011 mean train loss:  6.63506943e-04, mean val. rec. loss:  6.66185196e-04\n",
      "Epoch: 3012 mean train loss:  6.63180905e-04, mean val. rec. loss:  6.62819787e-04\n",
      "Epoch: 3013 mean train loss:  6.64421231e-04, mean val. rec. loss:  6.61331415e-04\n",
      "Epoch: 3014 mean train loss:  6.63747104e-04, mean val. rec. loss:  6.63616381e-04\n",
      "Epoch: 3015 mean train loss:  6.60219529e-04, mean val. rec. loss:  6.64747488e-04\n",
      "Epoch: 3016 mean train loss:  6.59536256e-04, mean val. rec. loss:  6.60709637e-04\n",
      "Epoch: 3017 mean train loss:  6.59591917e-04, mean val. rec. loss:  6.59952803e-04\n",
      "Epoch: 3018 mean train loss:  6.59321309e-04, mean val. rec. loss:  6.60361604e-04\n",
      "Epoch: 3019 mean train loss:  6.57853258e-04, mean val. rec. loss:  6.60237743e-04\n",
      "Epoch: 3020 mean train loss:  6.57765556e-04, mean val. rec. loss:  6.57439086e-04\n",
      "Epoch: 3021 mean train loss:  6.58800568e-04, mean val. rec. loss:  6.60058638e-04\n",
      "Epoch: 3022 mean train loss:  6.56506449e-04, mean val. rec. loss:  6.56864919e-04\n",
      "Epoch: 3023 mean train loss:  6.57377353e-04, mean val. rec. loss:  6.56841005e-04\n",
      "Epoch: 3024 mean train loss:  6.56116813e-04, mean val. rec. loss:  6.56820943e-04\n",
      "Epoch: 3025 mean train loss:  6.54285398e-04, mean val. rec. loss:  6.55501063e-04\n",
      "Epoch: 3026 mean train loss:  6.52318199e-04, mean val. rec. loss:  6.57231197e-04\n",
      "Epoch: 3027 mean train loss:  6.52676669e-04, mean val. rec. loss:  6.53901260e-04\n",
      "Epoch: 3028 mean train loss:  6.52105735e-04, mean val. rec. loss:  6.52346088e-04\n",
      "Epoch: 3029 mean train loss:  6.51755853e-04, mean val. rec. loss:  6.56309360e-04\n",
      "Epoch: 3030 mean train loss:  6.51357102e-04, mean val. rec. loss:  6.50522331e-04\n",
      "Epoch: 3031 mean train loss:  6.49645961e-04, mean val. rec. loss:  6.52182756e-04\n",
      "Epoch: 3032 mean train loss:  6.49097556e-04, mean val. rec. loss:  6.51058627e-04\n",
      "Epoch: 3033 mean train loss:  6.50849530e-04, mean val. rec. loss:  6.51100932e-04\n",
      "Epoch: 3034 mean train loss:  6.47595450e-04, mean val. rec. loss:  6.54299012e-04\n",
      "Epoch: 3035 mean train loss:  6.47631798e-04, mean val. rec. loss:  6.47571465e-04\n",
      "Epoch: 3036 mean train loss:  6.47993979e-04, mean val. rec. loss:  6.47874722e-04\n",
      "Epoch: 3037 mean train loss:  6.46290807e-04, mean val. rec. loss:  6.50300340e-04\n",
      "Epoch: 3038 mean train loss:  6.44823029e-04, mean val. rec. loss:  6.47549149e-04\n",
      "Epoch: 3039 mean train loss:  6.43431911e-04, mean val. rec. loss:  6.47097172e-04\n",
      "Epoch: 3040 mean train loss:  6.43551143e-04, mean val. rec. loss:  6.44582946e-04\n",
      "Epoch: 3041 mean train loss:  6.43320686e-04, mean val. rec. loss:  6.45780709e-04\n",
      "Epoch: 3042 mean train loss:  6.42848618e-04, mean val. rec. loss:  6.43818117e-04\n",
      "Epoch: 3043 mean train loss:  6.41989561e-04, mean val. rec. loss:  6.44079360e-04\n",
      "Epoch: 3044 mean train loss:  6.41145999e-04, mean val. rec. loss:  6.43618877e-04\n",
      "Epoch: 3045 mean train loss:  6.41028814e-04, mean val. rec. loss:  6.40520090e-04\n",
      "Epoch: 3046 mean train loss:  6.41834214e-04, mean val. rec. loss:  6.44986877e-04\n",
      "Epoch: 3047 mean train loss:  6.39645644e-04, mean val. rec. loss:  6.40732704e-04\n",
      "Epoch: 3048 mean train loss:  6.40231224e-04, mean val. rec. loss:  6.43649843e-04\n",
      "Epoch: 3049 mean train loss:  6.38382111e-04, mean val. rec. loss:  6.39835363e-04\n",
      "Epoch: 3050 mean train loss:  6.37184296e-04, mean val. rec. loss:  6.39861241e-04\n",
      "Epoch: 3051 mean train loss:  6.36965232e-04, mean val. rec. loss:  6.40466809e-04\n",
      "Epoch: 3052 mean train loss:  6.36299733e-04, mean val. rec. loss:  6.35718718e-04\n",
      "Epoch: 3053 mean train loss:  6.34718527e-04, mean val. rec. loss:  6.39993752e-04\n",
      "Epoch: 3054 mean train loss:  6.34443374e-04, mean val. rec. loss:  6.37814403e-04\n",
      "Epoch: 3055 mean train loss:  6.34344447e-04, mean val. rec. loss:  6.35319657e-04\n",
      "Epoch: 3056 mean train loss:  6.34985675e-04, mean val. rec. loss:  6.37078358e-04\n",
      "Epoch: 3057 mean train loss:  6.33822374e-04, mean val. rec. loss:  6.36609153e-04\n",
      "Epoch: 3058 mean train loss:  6.32237695e-04, mean val. rec. loss:  6.34635512e-04\n",
      "Epoch: 3059 mean train loss:  6.31695456e-04, mean val. rec. loss:  6.34084678e-04\n",
      "Epoch: 3060 mean train loss:  6.31570429e-04, mean val. rec. loss:  6.35742487e-04\n",
      "Epoch: 3061 mean train loss:  6.30817343e-04, mean val. rec. loss:  6.34336035e-04\n",
      "Epoch: 3062 mean train loss:  6.32151429e-04, mean val. rec. loss:  6.31509612e-04\n",
      "Epoch: 3063 mean train loss:  6.28881259e-04, mean val. rec. loss:  6.32449112e-04\n",
      "Epoch: 3064 mean train loss:  6.31148012e-04, mean val. rec. loss:  6.31764022e-04\n",
      "Epoch: 3065 mean train loss:  6.29952721e-04, mean val. rec. loss:  6.30485285e-04\n",
      "Epoch: 3066 mean train loss:  6.27228952e-04, mean val. rec. loss:  6.32124994e-04\n",
      "Epoch: 3067 mean train loss:  6.27660908e-04, mean val. rec. loss:  6.32453982e-04\n",
      "Epoch: 3068 mean train loss:  6.25787323e-04, mean val. rec. loss:  6.26192296e-04\n",
      "Epoch: 3069 mean train loss:  6.24799753e-04, mean val. rec. loss:  6.26240126e-04\n",
      "Epoch: 3070 mean train loss:  6.22826695e-04, mean val. rec. loss:  6.28421292e-04\n",
      "Epoch: 3071 mean train loss:  6.23600451e-04, mean val. rec. loss:  6.27524606e-04\n",
      "Epoch: 3072 mean train loss:  6.21928427e-04, mean val. rec. loss:  6.28018670e-04\n",
      "Epoch: 3073 mean train loss:  6.21277416e-04, mean val. rec. loss:  6.22723379e-04\n",
      "Epoch: 3074 mean train loss:  6.20786032e-04, mean val. rec. loss:  6.25187885e-04\n",
      "Epoch: 3075 mean train loss:  6.20501223e-04, mean val. rec. loss:  6.23628352e-04\n",
      "Epoch: 3076 mean train loss:  6.21003737e-04, mean val. rec. loss:  6.22770699e-04\n",
      "Epoch: 3077 mean train loss:  6.19649637e-04, mean val. rec. loss:  6.24909924e-04\n",
      "Epoch: 3078 mean train loss:  6.19864089e-04, mean val. rec. loss:  6.24516243e-04\n",
      "Epoch: 3079 mean train loss:  6.19637347e-04, mean val. rec. loss:  6.19461624e-04\n",
      "Epoch: 3080 mean train loss:  6.17356673e-04, mean val. rec. loss:  6.22188173e-04\n",
      "Epoch: 3081 mean train loss:  6.18099620e-04, mean val. rec. loss:  6.21357051e-04\n",
      "Epoch: 3082 mean train loss:  6.17841043e-04, mean val. rec. loss:  6.22019462e-04\n",
      "Epoch: 3083 mean train loss:  6.18659797e-04, mean val. rec. loss:  6.21075746e-04\n",
      "Epoch: 3084 mean train loss:  6.16600371e-04, mean val. rec. loss:  6.19106831e-04\n",
      "Epoch: 3085 mean train loss:  6.14812818e-04, mean val. rec. loss:  6.20625295e-04\n",
      "Epoch: 3086 mean train loss:  6.17697487e-04, mean val. rec. loss:  6.18160280e-04\n",
      "Epoch: 3087 mean train loss:  6.14428217e-04, mean val. rec. loss:  6.19475507e-04\n",
      "Epoch: 3088 mean train loss:  6.12843673e-04, mean val. rec. loss:  6.14331990e-04\n",
      "Epoch: 3089 mean train loss:  6.13506590e-04, mean val. rec. loss:  6.19288625e-04\n",
      "Epoch: 3090 mean train loss:  6.10930620e-04, mean val. rec. loss:  6.15737206e-04\n",
      "Epoch: 3091 mean train loss:  6.11329299e-04, mean val. rec. loss:  6.15269891e-04\n",
      "Epoch: 3092 mean train loss:  6.09744938e-04, mean val. rec. loss:  6.14775536e-04\n",
      "Epoch: 3093 mean train loss:  6.10500447e-04, mean val. rec. loss:  6.13473828e-04\n",
      "Epoch: 3094 mean train loss:  6.09697976e-04, mean val. rec. loss:  6.12091873e-04\n",
      "Epoch: 3095 mean train loss:  6.08686035e-04, mean val. rec. loss:  6.11908698e-04\n",
      "Epoch: 3096 mean train loss:  6.07636242e-04, mean val. rec. loss:  6.14617002e-04\n",
      "Epoch: 3097 mean train loss:  6.07720668e-04, mean val. rec. loss:  6.10872449e-04\n",
      "Epoch: 3098 mean train loss:  6.06413203e-04, mean val. rec. loss:  6.08603656e-04\n",
      "Epoch: 3099 mean train loss:  6.07484413e-04, mean val. rec. loss:  6.10422361e-04\n",
      "Epoch: 3100 mean train loss:  6.06480758e-04, mean val. rec. loss:  6.09883884e-04\n",
      "Epoch: 3101 mean train loss:  6.07219749e-04, mean val. rec. loss:  6.07653289e-04\n",
      "Epoch: 3102 mean train loss:  6.04904144e-04, mean val. rec. loss:  6.08048497e-04\n",
      "Epoch: 3103 mean train loss:  6.04418153e-04, mean val. rec. loss:  6.08562769e-04\n",
      "Epoch: 3104 mean train loss:  6.03677400e-04, mean val. rec. loss:  6.09590150e-04\n",
      "Epoch: 3105 mean train loss:  6.03811815e-04, mean val. rec. loss:  6.10668776e-04\n",
      "Epoch: 3106 mean train loss:  6.02970989e-04, mean val. rec. loss:  6.03753474e-04\n",
      "Epoch: 3107 mean train loss:  6.01321950e-04, mean val. rec. loss:  6.07028167e-04\n",
      "Epoch: 3108 mean train loss:  6.01609571e-04, mean val. rec. loss:  6.04850853e-04\n",
      "Epoch: 3109 mean train loss:  6.02465082e-04, mean val. rec. loss:  6.03933887e-04\n",
      "Epoch: 3110 mean train loss:  6.01332066e-04, mean val. rec. loss:  6.03833031e-04\n",
      "Epoch: 3111 mean train loss:  5.99851015e-04, mean val. rec. loss:  6.04447504e-04\n",
      "Epoch: 3112 mean train loss:  5.98554760e-04, mean val. rec. loss:  6.03348489e-04\n",
      "Epoch: 3113 mean train loss:  5.98077313e-04, mean val. rec. loss:  6.03907828e-04\n",
      "Epoch: 3114 mean train loss:  5.97206905e-04, mean val. rec. loss:  6.01179026e-04\n",
      "Epoch: 3115 mean train loss:  5.97965955e-04, mean val. rec. loss:  6.01172084e-04\n",
      "Epoch: 3116 mean train loss:  5.95427431e-04, mean val. rec. loss:  6.00316757e-04\n",
      "Epoch: 3117 mean train loss:  5.96712547e-04, mean val. rec. loss:  6.00488811e-04\n",
      "Epoch: 3118 mean train loss:  5.95214701e-04, mean val. rec. loss:  6.02882446e-04\n",
      "Epoch: 3119 mean train loss:  5.93999853e-04, mean val. rec. loss:  5.98018489e-04\n",
      "Epoch: 3120 mean train loss:  5.94150061e-04, mean val. rec. loss:  5.98373355e-04\n",
      "Epoch: 3121 mean train loss:  5.95109044e-04, mean val. rec. loss:  5.97492442e-04\n",
      "Epoch: 3122 mean train loss:  5.93809773e-04, mean val. rec. loss:  5.97418518e-04\n",
      "Epoch: 3123 mean train loss:  5.94479201e-04, mean val. rec. loss:  5.99167841e-04\n",
      "Epoch: 3124 mean train loss:  5.93012942e-04, mean val. rec. loss:  5.96772898e-04\n",
      "Epoch: 3125 mean train loss:  5.92657384e-04, mean val. rec. loss:  5.92161243e-04\n",
      "Epoch: 3126 mean train loss:  5.90576544e-04, mean val. rec. loss:  6.00794576e-04\n",
      "Epoch: 3127 mean train loss:  5.90694939e-04, mean val. rec. loss:  5.93424498e-04\n",
      "Epoch: 3128 mean train loss:  5.89871777e-04, mean val. rec. loss:  5.93991469e-04\n",
      "Epoch: 3129 mean train loss:  5.88521050e-04, mean val. rec. loss:  5.95272859e-04\n",
      "Epoch: 3130 mean train loss:  5.89388349e-04, mean val. rec. loss:  5.89279322e-04\n",
      "Epoch: 3131 mean train loss:  5.86789638e-04, mean val. rec. loss:  5.89985164e-04\n",
      "Epoch: 3132 mean train loss:  5.86356113e-04, mean val. rec. loss:  5.96901084e-04\n",
      "Epoch: 3133 mean train loss:  5.86578306e-04, mean val. rec. loss:  5.89317592e-04\n",
      "Epoch: 3134 mean train loss:  5.86378009e-04, mean val. rec. loss:  5.90480973e-04\n",
      "Epoch: 3135 mean train loss:  5.84798345e-04, mean val. rec. loss:  5.91070005e-04\n",
      "Epoch: 3136 mean train loss:  5.83653958e-04, mean val. rec. loss:  5.91226867e-04\n",
      "Epoch: 3137 mean train loss:  5.83383981e-04, mean val. rec. loss:  5.87887081e-04\n",
      "Epoch: 3138 mean train loss:  5.83953018e-04, mean val. rec. loss:  5.87689913e-04\n",
      "Epoch: 3139 mean train loss:  5.82409884e-04, mean val. rec. loss:  5.88746914e-04\n",
      "Epoch: 3140 mean train loss:  5.81867818e-04, mean val. rec. loss:  5.86504943e-04\n",
      "Epoch: 3141 mean train loss:  5.83079922e-04, mean val. rec. loss:  5.86275284e-04\n",
      "Epoch: 3142 mean train loss:  5.80863054e-04, mean val. rec. loss:  5.85241361e-04\n",
      "Epoch: 3143 mean train loss:  5.82120098e-04, mean val. rec. loss:  5.86315626e-04\n",
      "Epoch: 3144 mean train loss:  5.81787790e-04, mean val. rec. loss:  5.85460554e-04\n",
      "Epoch: 3145 mean train loss:  5.78393225e-04, mean val. rec. loss:  5.81331042e-04\n",
      "Epoch: 3146 mean train loss:  5.79394223e-04, mean val. rec. loss:  5.88877463e-04\n",
      "Epoch: 3147 mean train loss:  5.80572690e-04, mean val. rec. loss:  5.82555700e-04\n",
      "Epoch: 3148 mean train loss:  5.78395681e-04, mean val. rec. loss:  5.83904255e-04\n",
      "Epoch: 3149 mean train loss:  5.77003214e-04, mean val. rec. loss:  5.83236101e-04\n",
      "Epoch: 3150 mean train loss:  5.76549970e-04, mean val. rec. loss:  5.83943107e-04\n",
      "Epoch: 3151 mean train loss:  5.76062126e-04, mean val. rec. loss:  5.81397371e-04\n",
      "Epoch: 3152 mean train loss:  5.76757470e-04, mean val. rec. loss:  5.82420281e-04\n",
      "Epoch: 3153 mean train loss:  5.75870730e-04, mean val. rec. loss:  5.79989429e-04\n",
      "Epoch: 3154 mean train loss:  5.74014314e-04, mean val. rec. loss:  5.78548741e-04\n",
      "Epoch: 3155 mean train loss:  5.73600334e-04, mean val. rec. loss:  5.81249740e-04\n",
      "Epoch: 3156 mean train loss:  5.73670808e-04, mean val. rec. loss:  5.76103788e-04\n",
      "Epoch: 3157 mean train loss:  5.72498515e-04, mean val. rec. loss:  5.78032143e-04\n",
      "Epoch: 3158 mean train loss:  5.72449490e-04, mean val. rec. loss:  5.78290369e-04\n",
      "Epoch: 3159 mean train loss:  5.70982142e-04, mean val. rec. loss:  5.78935517e-04\n",
      "Epoch: 3160 mean train loss:  5.71851765e-04, mean val. rec. loss:  5.76349293e-04\n",
      "Epoch: 3161 mean train loss:  5.69810133e-04, mean val. rec. loss:  5.76718333e-04\n",
      "Epoch: 3162 mean train loss:  5.71018669e-04, mean val. rec. loss:  5.76662545e-04\n",
      "Epoch: 3163 mean train loss:  5.69702225e-04, mean val. rec. loss:  5.75126962e-04\n",
      "Epoch: 3164 mean train loss:  5.68901995e-04, mean val. rec. loss:  5.73781169e-04\n",
      "Epoch: 3165 mean train loss:  5.67822368e-04, mean val. rec. loss:  5.73292411e-04\n",
      "Epoch: 3166 mean train loss:  5.66887823e-04, mean val. rec. loss:  5.75504434e-04\n",
      "Epoch: 3167 mean train loss:  5.67773528e-04, mean val. rec. loss:  5.71742472e-04\n",
      "Epoch: 3168 mean train loss:  5.67531120e-04, mean val. rec. loss:  5.70805298e-04\n",
      "Epoch: 3169 mean train loss:  5.66214930e-04, mean val. rec. loss:  5.70910806e-04\n",
      "Epoch: 3170 mean train loss:  5.65835008e-04, mean val. rec. loss:  5.70976589e-04\n",
      "Epoch: 3171 mean train loss:  5.65233334e-04, mean val. rec. loss:  5.71313355e-04\n",
      "Epoch: 3172 mean train loss:  5.66021866e-04, mean val. rec. loss:  5.69837159e-04\n",
      "Epoch: 3173 mean train loss:  5.66088666e-04, mean val. rec. loss:  5.69986352e-04\n",
      "Epoch: 3174 mean train loss:  5.64939462e-04, mean val. rec. loss:  5.69411676e-04\n",
      "Epoch: 3175 mean train loss:  5.62358404e-04, mean val. rec. loss:  5.68296924e-04\n",
      "Epoch: 3176 mean train loss:  5.62261444e-04, mean val. rec. loss:  5.70535078e-04\n",
      "Epoch: 3177 mean train loss:  5.62036360e-04, mean val. rec. loss:  5.65907759e-04\n",
      "Epoch: 3178 mean train loss:  5.62672504e-04, mean val. rec. loss:  5.68709977e-04\n",
      "Epoch: 3179 mean train loss:  5.61393456e-04, mean val. rec. loss:  5.65811301e-04\n",
      "Epoch: 3180 mean train loss:  5.60233344e-04, mean val. rec. loss:  5.67172250e-04\n",
      "Epoch: 3181 mean train loss:  5.59187327e-04, mean val. rec. loss:  5.63293586e-04\n",
      "Epoch: 3182 mean train loss:  5.59602840e-04, mean val. rec. loss:  5.65869307e-04\n",
      "Epoch: 3183 mean train loss:  5.58649004e-04, mean val. rec. loss:  5.64201867e-04\n",
      "Epoch: 3184 mean train loss:  5.58642798e-04, mean val. rec. loss:  5.62736392e-04\n",
      "Epoch: 3185 mean train loss:  5.58493552e-04, mean val. rec. loss:  5.65726946e-04\n",
      "Epoch: 3186 mean train loss:  5.58350269e-04, mean val. rec. loss:  5.64689571e-04\n",
      "Epoch: 3187 mean train loss:  5.57031019e-04, mean val. rec. loss:  5.60725244e-04\n",
      "Epoch: 3188 mean train loss:  5.55668704e-04, mean val. rec. loss:  5.60801785e-04\n",
      "Epoch: 3189 mean train loss:  5.56323377e-04, mean val. rec. loss:  5.62311418e-04\n",
      "Epoch: 3190 mean train loss:  5.55705188e-04, mean val. rec. loss:  5.59310906e-04\n",
      "Epoch: 3191 mean train loss:  5.53618916e-04, mean val. rec. loss:  5.60306521e-04\n",
      "Epoch: 3192 mean train loss:  5.53036386e-04, mean val. rec. loss:  5.58186304e-04\n",
      "Epoch: 3193 mean train loss:  5.52291999e-04, mean val. rec. loss:  5.59947694e-04\n",
      "Epoch: 3194 mean train loss:  5.52874468e-04, mean val. rec. loss:  5.58580058e-04\n",
      "Epoch: 3195 mean train loss:  5.53072818e-04, mean val. rec. loss:  5.61530815e-04\n",
      "Epoch: 3196 mean train loss:  5.52976970e-04, mean val. rec. loss:  5.58789729e-04\n",
      "Epoch: 3197 mean train loss:  5.52219758e-04, mean val. rec. loss:  5.58317362e-04\n",
      "Epoch: 3198 mean train loss:  5.51518353e-04, mean val. rec. loss:  5.58220940e-04\n",
      "Epoch: 3199 mean train loss:  5.50507107e-04, mean val. rec. loss:  5.55576784e-04\n",
      "Epoch: 3200 mean train loss:  5.50454212e-04, mean val. rec. loss:  5.56475433e-04\n",
      "Epoch: 3201 mean train loss:  5.49624489e-04, mean val. rec. loss:  5.53456748e-04\n",
      "Epoch: 3202 mean train loss:  5.50523637e-04, mean val. rec. loss:  5.56180026e-04\n",
      "Epoch: 3203 mean train loss:  5.49711305e-04, mean val. rec. loss:  5.57083473e-04\n",
      "Epoch: 3204 mean train loss:  5.48917059e-04, mean val. rec. loss:  5.51442475e-04\n",
      "Epoch: 3205 mean train loss:  5.46833862e-04, mean val. rec. loss:  5.55927361e-04\n",
      "Epoch: 3206 mean train loss:  5.46534429e-04, mean val. rec. loss:  5.53161197e-04\n",
      "Epoch: 3207 mean train loss:  5.46032224e-04, mean val. rec. loss:  5.52994703e-04\n",
      "Epoch: 3208 mean train loss:  5.45303093e-04, mean val. rec. loss:  5.53010004e-04\n",
      "Epoch: 3209 mean train loss:  5.45748440e-04, mean val. rec. loss:  5.52664443e-04\n",
      "Epoch: 3210 mean train loss:  5.47357913e-04, mean val. rec. loss:  5.51854001e-04\n",
      "Epoch: 3211 mean train loss:  5.45154923e-04, mean val. rec. loss:  5.49859063e-04\n",
      "Epoch: 3212 mean train loss:  5.44197837e-04, mean val. rec. loss:  5.48898810e-04\n",
      "Epoch: 3213 mean train loss:  5.43670501e-04, mean val. rec. loss:  5.50879356e-04\n",
      "Epoch: 3214 mean train loss:  5.43461983e-04, mean val. rec. loss:  5.50612407e-04\n",
      "Epoch: 3215 mean train loss:  5.41136742e-04, mean val. rec. loss:  5.47485962e-04\n",
      "Epoch: 3216 mean train loss:  5.41192867e-04, mean val. rec. loss:  5.50148763e-04\n",
      "Epoch: 3217 mean train loss:  5.41359341e-04, mean val. rec. loss:  5.46487512e-04\n",
      "Epoch: 3218 mean train loss:  5.44604655e-04, mean val. rec. loss:  5.50182600e-04\n",
      "Epoch: 3219 mean train loss:  5.43554915e-04, mean val. rec. loss:  5.47018393e-04\n",
      "Epoch: 3220 mean train loss:  5.40732041e-04, mean val. rec. loss:  5.45744671e-04\n",
      "Epoch: 3221 mean train loss:  5.40884823e-04, mean val. rec. loss:  5.49545957e-04\n",
      "Epoch: 3222 mean train loss:  5.38604659e-04, mean val. rec. loss:  5.43505826e-04\n",
      "Epoch: 3223 mean train loss:  5.38505061e-04, mean val. rec. loss:  5.47749531e-04\n",
      "Epoch: 3224 mean train loss:  5.39312878e-04, mean val. rec. loss:  5.44198184e-04\n",
      "Epoch: 3225 mean train loss:  5.37776867e-04, mean val. rec. loss:  5.43649240e-04\n",
      "Epoch: 3226 mean train loss:  5.35900283e-04, mean val. rec. loss:  5.45439124e-04\n",
      "Epoch: 3227 mean train loss:  5.36096998e-04, mean val. rec. loss:  5.41917144e-04\n",
      "Epoch: 3228 mean train loss:  5.34763971e-04, mean val. rec. loss:  5.41860919e-04\n",
      "Epoch: 3229 mean train loss:  5.36220439e-04, mean val. rec. loss:  5.41067814e-04\n",
      "Epoch: 3230 mean train loss:  5.34327057e-04, mean val. rec. loss:  5.41839076e-04\n",
      "Epoch: 3231 mean train loss:  5.35592646e-04, mean val. rec. loss:  5.40141362e-04\n",
      "Epoch: 3232 mean train loss:  5.35073280e-04, mean val. rec. loss:  5.41965736e-04\n",
      "Epoch: 3233 mean train loss:  5.34381704e-04, mean val. rec. loss:  5.40378944e-04\n",
      "Epoch: 3234 mean train loss:  5.33314147e-04, mean val. rec. loss:  5.37931410e-04\n",
      "Epoch: 3235 mean train loss:  5.32722485e-04, mean val. rec. loss:  5.41084532e-04\n",
      "Epoch: 3236 mean train loss:  5.31143399e-04, mean val. rec. loss:  5.37435129e-04\n",
      "Epoch: 3237 mean train loss:  5.31053055e-04, mean val. rec. loss:  5.36531573e-04\n",
      "Epoch: 3238 mean train loss:  5.29018075e-04, mean val. rec. loss:  5.38039135e-04\n",
      "Epoch: 3239 mean train loss:  5.30394260e-04, mean val. rec. loss:  5.38164741e-04\n",
      "Epoch: 3240 mean train loss:  5.28781282e-04, mean val. rec. loss:  5.37130964e-04\n",
      "Epoch: 3241 mean train loss:  5.28929964e-04, mean val. rec. loss:  5.34275683e-04\n",
      "Epoch: 3242 mean train loss:  5.27753318e-04, mean val. rec. loss:  5.33529535e-04\n",
      "Epoch: 3243 mean train loss:  5.28244013e-04, mean val. rec. loss:  5.37238179e-04\n",
      "Epoch: 3244 mean train loss:  5.27458186e-04, mean val. rec. loss:  5.30855067e-04\n",
      "Epoch: 3245 mean train loss:  5.26866565e-04, mean val. rec. loss:  5.37100507e-04\n",
      "Epoch: 3246 mean train loss:  5.27080610e-04, mean val. rec. loss:  5.31317802e-04\n",
      "Epoch: 3247 mean train loss:  5.25797414e-04, mean val. rec. loss:  5.34130560e-04\n",
      "Epoch: 3248 mean train loss:  5.24832634e-04, mean val. rec. loss:  5.34827389e-04\n",
      "Epoch: 3249 mean train loss:  5.26332316e-04, mean val. rec. loss:  5.29777858e-04\n",
      "Epoch: 3250 mean train loss:  5.23997546e-04, mean val. rec. loss:  5.34641852e-04\n",
      "Epoch: 3251 mean train loss:  5.23598626e-04, mean val. rec. loss:  5.28863581e-04\n",
      "Epoch: 3252 mean train loss:  5.23415509e-04, mean val. rec. loss:  5.32112034e-04\n",
      "Epoch: 3253 mean train loss:  5.24225489e-04, mean val. rec. loss:  5.31000771e-04\n",
      "Epoch: 3254 mean train loss:  5.22457279e-04, mean val. rec. loss:  5.25600191e-04\n",
      "Epoch: 3255 mean train loss:  5.23455596e-04, mean val. rec. loss:  5.32607007e-04\n",
      "Epoch: 3256 mean train loss:  5.21454059e-04, mean val. rec. loss:  5.28293557e-04\n",
      "Epoch: 3257 mean train loss:  5.20441986e-04, mean val. rec. loss:  5.29185083e-04\n",
      "Epoch: 3258 mean train loss:  5.19891839e-04, mean val. rec. loss:  5.28762471e-04\n",
      "Epoch: 3259 mean train loss:  5.23270177e-04, mean val. rec. loss:  5.32863126e-04\n",
      "Epoch: 3260 mean train loss:  5.20927907e-04, mean val. rec. loss:  5.22281629e-04\n",
      "Epoch: 3261 mean train loss:  5.19802405e-04, mean val. rec. loss:  5.27470795e-04\n",
      "Epoch: 3262 mean train loss:  5.18608222e-04, mean val. rec. loss:  5.26535547e-04\n",
      "Epoch: 3263 mean train loss:  5.18095744e-04, mean val. rec. loss:  5.26167925e-04\n",
      "Epoch: 3264 mean train loss:  5.18096634e-04, mean val. rec. loss:  5.25109070e-04\n",
      "Epoch: 3265 mean train loss:  5.18212172e-04, mean val. rec. loss:  5.24256651e-04\n",
      "Epoch: 3266 mean train loss:  5.17242871e-04, mean val. rec. loss:  5.22668405e-04\n",
      "Epoch: 3267 mean train loss:  5.17957736e-04, mean val. rec. loss:  5.24278458e-04\n",
      "Epoch: 3268 mean train loss:  5.18394909e-04, mean val. rec. loss:  5.24222233e-04\n",
      "Epoch: 3269 mean train loss:  5.15453072e-04, mean val. rec. loss:  5.24583931e-04\n",
      "Epoch: 3270 mean train loss:  5.16471902e-04, mean val. rec. loss:  5.21610241e-04\n",
      "Epoch: 3271 mean train loss:  5.14105840e-04, mean val. rec. loss:  5.25219084e-04\n",
      "Epoch: 3272 mean train loss:  5.15707156e-04, mean val. rec. loss:  5.20608919e-04\n",
      "Epoch: 3273 mean train loss:  5.13748458e-04, mean val. rec. loss:  5.20578536e-04\n",
      "Epoch: 3274 mean train loss:  5.13054785e-04, mean val. rec. loss:  5.19866478e-04\n",
      "Epoch: 3275 mean train loss:  5.14589400e-04, mean val. rec. loss:  5.18400349e-04\n",
      "Epoch: 3276 mean train loss:  5.12120779e-04, mean val. rec. loss:  5.20931511e-04\n",
      "Epoch: 3277 mean train loss:  5.12748215e-04, mean val. rec. loss:  5.19410648e-04\n",
      "Epoch: 3278 mean train loss:  5.12285913e-04, mean val. rec. loss:  5.16647246e-04\n",
      "Epoch: 3279 mean train loss:  5.11682259e-04, mean val. rec. loss:  5.20669069e-04\n",
      "Epoch: 3280 mean train loss:  5.10798795e-04, mean val. rec. loss:  5.18110394e-04\n",
      "Epoch: 3281 mean train loss:  5.11690315e-04, mean val. rec. loss:  5.20458963e-04\n",
      "Epoch: 3282 mean train loss:  5.08900122e-04, mean val. rec. loss:  5.16655932e-04\n",
      "Epoch: 3283 mean train loss:  5.10113111e-04, mean val. rec. loss:  5.16967584e-04\n",
      "Epoch: 3284 mean train loss:  5.08991164e-04, mean val. rec. loss:  5.14657468e-04\n",
      "Epoch: 3285 mean train loss:  5.07671118e-04, mean val. rec. loss:  5.15922177e-04\n",
      "Epoch: 3286 mean train loss:  5.06829348e-04, mean val. rec. loss:  5.16636343e-04\n",
      "Epoch: 3287 mean train loss:  5.06462547e-04, mean val. rec. loss:  5.13583349e-04\n",
      "Epoch: 3288 mean train loss:  5.06877940e-04, mean val. rec. loss:  5.14793323e-04\n",
      "Epoch: 3289 mean train loss:  5.06336017e-04, mean val. rec. loss:  5.13215399e-04\n",
      "Epoch: 3290 mean train loss:  5.06980502e-04, mean val. rec. loss:  5.16009549e-04\n",
      "Epoch: 3291 mean train loss:  5.07206819e-04, mean val. rec. loss:  5.13080053e-04\n",
      "Epoch: 3292 mean train loss:  5.03258927e-04, mean val. rec. loss:  5.13482603e-04\n",
      "Epoch: 3293 mean train loss:  5.04808259e-04, mean val. rec. loss:  5.11812182e-04\n",
      "Epoch: 3294 mean train loss:  5.03283171e-04, mean val. rec. loss:  5.11640456e-04\n",
      "Epoch: 3295 mean train loss:  5.02857647e-04, mean val. rec. loss:  5.10693759e-04\n",
      "Epoch: 3296 mean train loss:  5.02577227e-04, mean val. rec. loss:  5.11440780e-04\n",
      "Epoch: 3297 mean train loss:  5.02711496e-04, mean val. rec. loss:  5.15454353e-04\n",
      "Epoch: 3298 mean train loss:  5.02365319e-04, mean val. rec. loss:  5.05732036e-04\n",
      "Epoch: 3299 mean train loss:  5.02719782e-04, mean val. rec. loss:  5.10771936e-04\n",
      "Epoch: 3300 mean train loss:  5.00897601e-04, mean val. rec. loss:  5.06340694e-04\n",
      "Epoch: 3301 mean train loss:  5.01618522e-04, mean val. rec. loss:  5.10159571e-04\n",
      "Epoch: 3302 mean train loss:  4.99883530e-04, mean val. rec. loss:  5.07328423e-04\n",
      "Epoch: 3303 mean train loss:  4.99169150e-04, mean val. rec. loss:  5.06387324e-04\n",
      "Epoch: 3304 mean train loss:  4.98083899e-04, mean val. rec. loss:  5.06324957e-04\n",
      "Epoch: 3305 mean train loss:  4.98595062e-04, mean val. rec. loss:  5.06304895e-04\n",
      "Epoch: 3306 mean train loss:  4.97747281e-04, mean val. rec. loss:  5.07259369e-04\n",
      "Epoch: 3307 mean train loss:  4.99356954e-04, mean val. rec. loss:  5.08559913e-04\n",
      "Epoch: 3308 mean train loss:  4.96555770e-04, mean val. rec. loss:  5.04456424e-04\n",
      "Epoch: 3309 mean train loss:  4.98298205e-04, mean val. rec. loss:  5.03990926e-04\n",
      "Epoch: 3310 mean train loss:  4.96524023e-04, mean val. rec. loss:  5.06526704e-04\n",
      "Epoch: 3311 mean train loss:  4.96322637e-04, mean val. rec. loss:  5.03417050e-04\n",
      "Epoch: 3312 mean train loss:  4.96711773e-04, mean val. rec. loss:  5.03707259e-04\n",
      "Epoch: 3313 mean train loss:  4.96549327e-04, mean val. rec. loss:  5.04406269e-04\n",
      "Epoch: 3314 mean train loss:  4.95159364e-04, mean val. rec. loss:  5.04268888e-04\n",
      "Epoch: 3315 mean train loss:  4.94690773e-04, mean val. rec. loss:  5.00736295e-04\n",
      "Epoch: 3316 mean train loss:  4.93769230e-04, mean val. rec. loss:  5.05674685e-04\n",
      "Epoch: 3317 mean train loss:  4.92959037e-04, mean val. rec. loss:  4.99121736e-04\n",
      "Epoch: 3318 mean train loss:  4.94459392e-04, mean val. rec. loss:  5.00362603e-04\n",
      "Epoch: 3319 mean train loss:  4.92725840e-04, mean val. rec. loss:  5.04740128e-04\n",
      "Epoch: 3320 mean train loss:  4.93025148e-04, mean val. rec. loss:  4.98316164e-04\n",
      "Epoch: 3321 mean train loss:  4.92291066e-04, mean val. rec. loss:  5.01370248e-04\n",
      "Epoch: 3322 mean train loss:  4.91008267e-04, mean val. rec. loss:  4.98968508e-04\n",
      "Epoch: 3323 mean train loss:  4.90861525e-04, mean val. rec. loss:  4.98704867e-04\n",
      "Epoch: 3324 mean train loss:  4.91010778e-04, mean val. rec. loss:  4.96868171e-04\n",
      "Epoch: 3325 mean train loss:  4.89859247e-04, mean val. rec. loss:  5.00797062e-04\n",
      "Epoch: 3326 mean train loss:  4.90131819e-04, mean val. rec. loss:  4.96439563e-04\n",
      "Epoch: 3327 mean train loss:  4.90289181e-04, mean val. rec. loss:  4.98124630e-04\n",
      "Epoch: 3328 mean train loss:  4.88542794e-04, mean val. rec. loss:  4.95855547e-04\n",
      "Epoch: 3329 mean train loss:  4.89161988e-04, mean val. rec. loss:  4.97280243e-04\n",
      "Epoch: 3330 mean train loss:  4.87773969e-04, mean val. rec. loss:  4.95744624e-04\n",
      "Epoch: 3331 mean train loss:  4.86822633e-04, mean val. rec. loss:  4.96289207e-04\n",
      "Epoch: 3332 mean train loss:  4.87472707e-04, mean val. rec. loss:  4.92549233e-04\n",
      "Epoch: 3333 mean train loss:  4.87076507e-04, mean val. rec. loss:  4.96208450e-04\n",
      "Epoch: 3334 mean train loss:  4.86071448e-04, mean val. rec. loss:  4.95380890e-04\n",
      "Epoch: 3335 mean train loss:  4.85066092e-04, mean val. rec. loss:  4.92894504e-04\n",
      "Epoch: 3336 mean train loss:  4.84690352e-04, mean val. rec. loss:  4.94550459e-04\n",
      "Epoch: 3337 mean train loss:  4.85007391e-04, mean val. rec. loss:  4.92290898e-04\n",
      "Epoch: 3338 mean train loss:  4.84751831e-04, mean val. rec. loss:  4.92282030e-04\n",
      "Epoch: 3339 mean train loss:  4.84349472e-04, mean val. rec. loss:  4.91091209e-04\n",
      "Epoch: 3340 mean train loss:  4.83537536e-04, mean val. rec. loss:  4.93799768e-04\n",
      "Epoch: 3341 mean train loss:  4.84169175e-04, mean val. rec. loss:  4.88372547e-04\n",
      "Epoch: 3342 mean train loss:  4.82724864e-04, mean val. rec. loss:  4.94197011e-04\n",
      "Epoch: 3343 mean train loss:  4.83804536e-04, mean val. rec. loss:  4.89896609e-04\n",
      "Epoch: 3344 mean train loss:  4.82939434e-04, mean val. rec. loss:  4.90679501e-04\n",
      "Epoch: 3345 mean train loss:  4.83393000e-04, mean val. rec. loss:  4.87225594e-04\n",
      "Epoch: 3346 mean train loss:  4.85397807e-04, mean val. rec. loss:  4.88948967e-04\n",
      "Epoch: 3347 mean train loss:  4.81641873e-04, mean val. rec. loss:  4.93229308e-04\n",
      "Epoch: 3348 mean train loss:  4.81368151e-04, mean val. rec. loss:  4.87401936e-04\n",
      "Epoch: 3349 mean train loss:  4.80769068e-04, mean val. rec. loss:  4.86295252e-04\n",
      "Epoch: 3350 mean train loss:  4.81486800e-04, mean val. rec. loss:  4.90000917e-04\n",
      "Epoch: 3351 mean train loss:  4.79490936e-04, mean val. rec. loss:  4.88402277e-04\n",
      "Epoch: 3352 mean train loss:  4.79912228e-04, mean val. rec. loss:  4.86999278e-04\n",
      "Epoch: 3353 mean train loss:  4.79141741e-04, mean val. rec. loss:  4.87557817e-04\n",
      "Epoch: 3354 mean train loss:  4.78617840e-04, mean val. rec. loss:  4.87301480e-04\n",
      "Epoch: 3355 mean train loss:  4.76899669e-04, mean val. rec. loss:  4.85008446e-04\n",
      "Epoch: 3356 mean train loss:  4.77548857e-04, mean val. rec. loss:  4.84860961e-04\n",
      "Epoch: 3357 mean train loss:  4.76341169e-04, mean val. rec. loss:  4.87180309e-04\n",
      "Epoch: 3358 mean train loss:  4.75920518e-04, mean val. rec. loss:  4.81014244e-04\n",
      "Epoch: 3359 mean train loss:  4.74954870e-04, mean val. rec. loss:  4.85526171e-04\n",
      "Epoch: 3360 mean train loss:  4.75702359e-04, mean val. rec. loss:  4.83641174e-04\n",
      "Epoch: 3361 mean train loss:  4.75694413e-04, mean val. rec. loss:  4.84280470e-04\n",
      "Epoch: 3362 mean train loss:  4.74137728e-04, mean val. rec. loss:  4.82235813e-04\n",
      "Epoch: 3363 mean train loss:  4.73193399e-04, mean val. rec. loss:  4.82313735e-04\n",
      "Epoch: 3364 mean train loss:  4.73610243e-04, mean val. rec. loss:  4.81242305e-04\n",
      "Epoch: 3365 mean train loss:  4.72535783e-04, mean val. rec. loss:  4.82947579e-04\n",
      "Epoch: 3366 mean train loss:  4.72760906e-04, mean val. rec. loss:  4.79847011e-04\n",
      "Epoch: 3367 mean train loss:  4.71937783e-04, mean val. rec. loss:  4.80945299e-04\n",
      "Epoch: 3368 mean train loss:  4.71667051e-04, mean val. rec. loss:  4.81280975e-04\n",
      "Epoch: 3369 mean train loss:  4.71011380e-04, mean val. rec. loss:  4.78363764e-04\n",
      "Epoch: 3370 mean train loss:  4.70764066e-04, mean val. rec. loss:  4.79121979e-04\n",
      "Epoch: 3371 mean train loss:  4.70328260e-04, mean val. rec. loss:  4.79879285e-04\n",
      "Epoch: 3372 mean train loss:  4.69463039e-04, mean val. rec. loss:  4.78306049e-04\n",
      "Epoch: 3373 mean train loss:  4.69698651e-04, mean val. rec. loss:  4.77047010e-04\n",
      "Epoch: 3374 mean train loss:  4.68812819e-04, mean val. rec. loss:  4.78986160e-04\n",
      "Epoch: 3375 mean train loss:  4.70470953e-04, mean val. rec. loss:  4.77752017e-04\n",
      "Epoch: 3376 mean train loss:  4.70832413e-04, mean val. rec. loss:  4.72669703e-04\n",
      "Epoch: 3377 mean train loss:  4.68633566e-04, mean val. rec. loss:  4.81203017e-04\n",
      "Epoch: 3378 mean train loss:  4.68922208e-04, mean val. rec. loss:  4.76439115e-04\n",
      "Epoch: 3379 mean train loss:  4.66835138e-04, mean val. rec. loss:  4.75184510e-04\n",
      "Epoch: 3380 mean train loss:  4.66267864e-04, mean val. rec. loss:  4.73244197e-04\n",
      "Epoch: 3381 mean train loss:  4.67569323e-04, mean val. rec. loss:  4.73782093e-04\n",
      "Epoch: 3382 mean train loss:  4.67178950e-04, mean val. rec. loss:  4.78077480e-04\n",
      "Epoch: 3383 mean train loss:  4.68110443e-04, mean val. rec. loss:  4.73484252e-04\n",
      "Epoch: 3384 mean train loss:  4.66127044e-04, mean val. rec. loss:  4.75892788e-04\n",
      "Epoch: 3385 mean train loss:  4.65119080e-04, mean val. rec. loss:  4.75632272e-04\n",
      "Epoch: 3386 mean train loss:  4.64328934e-04, mean val. rec. loss:  4.71604416e-04\n",
      "Epoch: 3387 mean train loss:  4.64416752e-04, mean val. rec. loss:  4.71201248e-04\n",
      "Epoch: 3388 mean train loss:  4.63969635e-04, mean val. rec. loss:  4.75501142e-04\n",
      "Epoch: 3389 mean train loss:  4.63961750e-04, mean val. rec. loss:  4.70978821e-04\n",
      "Epoch: 3390 mean train loss:  4.63277745e-04, mean val. rec. loss:  4.73166384e-04\n",
      "Epoch: 3391 mean train loss:  4.62199510e-04, mean val. rec. loss:  4.70319281e-04\n",
      "Epoch: 3392 mean train loss:  4.61621153e-04, mean val. rec. loss:  4.72744427e-04\n",
      "Epoch: 3393 mean train loss:  4.62454545e-04, mean val. rec. loss:  4.69647239e-04\n",
      "Epoch: 3394 mean train loss:  4.62078577e-04, mean val. rec. loss:  4.69494993e-04\n",
      "Epoch: 3395 mean train loss:  4.61189415e-04, mean val. rec. loss:  4.67045278e-04\n",
      "Epoch: 3396 mean train loss:  4.59789579e-04, mean val. rec. loss:  4.71657733e-04\n",
      "Epoch: 3397 mean train loss:  4.60885708e-04, mean val. rec. loss:  4.70020204e-04\n",
      "Epoch: 3398 mean train loss:  4.60159598e-04, mean val. rec. loss:  4.66632625e-04\n",
      "Epoch: 3399 mean train loss:  4.59483589e-04, mean val. rec. loss:  4.70703768e-04\n",
      "Epoch: 3400 mean train loss:  4.60074789e-04, mean val. rec. loss:  4.68204588e-04\n",
      "Epoch: 3401 mean train loss:  4.58882055e-04, mean val. rec. loss:  4.68252999e-04\n",
      "Epoch: 3402 mean train loss:  4.59242312e-04, mean val. rec. loss:  4.63462603e-04\n",
      "Epoch: 3403 mean train loss:  4.59952862e-04, mean val. rec. loss:  4.67940438e-04\n",
      "Epoch: 3404 mean train loss:  4.58377476e-04, mean val. rec. loss:  4.65333571e-04\n",
      "Epoch: 3405 mean train loss:  4.57584621e-04, mean val. rec. loss:  4.65200224e-04\n",
      "Epoch: 3406 mean train loss:  4.56462474e-04, mean val. rec. loss:  4.66327878e-04\n",
      "Epoch: 3407 mean train loss:  4.56087819e-04, mean val. rec. loss:  4.65377075e-04\n",
      "Epoch: 3408 mean train loss:  4.56255280e-04, mean val. rec. loss:  4.64629909e-04\n",
      "Epoch: 3409 mean train loss:  4.54803428e-04, mean val. rec. loss:  4.63983126e-04\n",
      "Epoch: 3410 mean train loss:  4.54180379e-04, mean val. rec. loss:  4.61487436e-04\n",
      "Epoch: 3411 mean train loss:  4.55117763e-04, mean val. rec. loss:  4.62162567e-04\n",
      "Epoch: 3412 mean train loss:  4.53407329e-04, mean val. rec. loss:  4.64720043e-04\n",
      "Epoch: 3413 mean train loss:  4.53072635e-04, mean val. rec. loss:  4.62258516e-04\n",
      "Epoch: 3414 mean train loss:  4.52850210e-04, mean val. rec. loss:  4.60616481e-04\n",
      "Epoch: 3415 mean train loss:  4.53016247e-04, mean val. rec. loss:  4.61573935e-04\n",
      "Epoch: 3416 mean train loss:  4.53185049e-04, mean val. rec. loss:  4.61575098e-04\n",
      "Epoch: 3417 mean train loss:  4.51273525e-04, mean val. rec. loss:  4.60065320e-04\n",
      "Epoch: 3418 mean train loss:  4.53521137e-04, mean val. rec. loss:  4.61261701e-04\n",
      "Epoch: 3419 mean train loss:  4.52746898e-04, mean val. rec. loss:  4.59098743e-04\n",
      "Epoch: 3420 mean train loss:  4.50918931e-04, mean val. rec. loss:  4.59404253e-04\n",
      "Epoch: 3421 mean train loss:  4.50550291e-04, mean val. rec. loss:  4.60477573e-04\n",
      "Epoch: 3422 mean train loss:  4.50755980e-04, mean val. rec. loss:  4.57131499e-04\n",
      "Epoch: 3423 mean train loss:  4.51241275e-04, mean val. rec. loss:  4.60256527e-04\n",
      "Epoch: 3424 mean train loss:  4.49748093e-04, mean val. rec. loss:  4.59288533e-04\n",
      "Epoch: 3425 mean train loss:  4.49917192e-04, mean val. rec. loss:  4.57323579e-04\n",
      "Epoch: 3426 mean train loss:  4.50390665e-04, mean val. rec. loss:  4.58198422e-04\n",
      "Epoch: 3427 mean train loss:  4.48305092e-04, mean val. rec. loss:  4.54287267e-04\n",
      "Epoch: 3428 mean train loss:  4.47828896e-04, mean val. rec. loss:  4.57957532e-04\n",
      "Epoch: 3429 mean train loss:  4.49277795e-04, mean val. rec. loss:  4.53951809e-04\n",
      "Epoch: 3430 mean train loss:  4.46711743e-04, mean val. rec. loss:  4.58677476e-04\n",
      "Epoch: 3431 mean train loss:  4.48285587e-04, mean val. rec. loss:  4.54147923e-04\n",
      "Epoch: 3432 mean train loss:  4.46390397e-04, mean val. rec. loss:  4.55392098e-04\n",
      "Epoch: 3433 mean train loss:  4.46262284e-04, mean val. rec. loss:  4.55689794e-04\n",
      "Epoch: 3434 mean train loss:  4.46592259e-04, mean val. rec. loss:  4.55179665e-04\n",
      "Epoch: 3435 mean train loss:  4.45170703e-04, mean val. rec. loss:  4.54448672e-04\n",
      "Epoch: 3436 mean train loss:  4.45616376e-04, mean val. rec. loss:  4.54005018e-04\n",
      "Epoch: 3437 mean train loss:  4.44080346e-04, mean val. rec. loss:  4.53960169e-04\n",
      "Epoch: 3438 mean train loss:  4.44431538e-04, mean val. rec. loss:  4.50908338e-04\n",
      "Epoch: 3439 mean train loss:  4.43703321e-04, mean val. rec. loss:  4.53521965e-04\n",
      "Epoch: 3440 mean train loss:  4.43420488e-04, mean val. rec. loss:  4.54770174e-04\n",
      "Epoch: 3441 mean train loss:  4.41869518e-04, mean val. rec. loss:  4.50769358e-04\n",
      "Epoch: 3442 mean train loss:  4.42480311e-04, mean val. rec. loss:  4.52193327e-04\n",
      "Epoch: 3443 mean train loss:  4.42522099e-04, mean val. rec. loss:  4.51183137e-04\n",
      "Epoch: 3444 mean train loss:  4.42330906e-04, mean val. rec. loss:  4.51201600e-04\n",
      "Epoch: 3445 mean train loss:  4.42716408e-04, mean val. rec. loss:  4.47828014e-04\n",
      "Epoch: 3446 mean train loss:  4.43038668e-04, mean val. rec. loss:  4.52517264e-04\n",
      "Epoch: 3447 mean train loss:  4.42242511e-04, mean val. rec. loss:  4.47168874e-04\n",
      "Epoch: 3448 mean train loss:  4.40897566e-04, mean val. rec. loss:  4.52687864e-04\n",
      "Epoch: 3449 mean train loss:  4.40846271e-04, mean val. rec. loss:  4.47759505e-04\n",
      "Epoch: 3450 mean train loss:  4.40562740e-04, mean val. rec. loss:  4.48419263e-04\n",
      "Epoch: 3451 mean train loss:  4.40041929e-04, mean val. rec. loss:  4.49711593e-04\n",
      "Epoch: 3452 mean train loss:  4.38762879e-04, mean val. rec. loss:  4.48618720e-04\n",
      "Epoch: 3453 mean train loss:  4.39217844e-04, mean val. rec. loss:  4.47236656e-04\n",
      "Epoch: 3454 mean train loss:  4.38813793e-04, mean val. rec. loss:  4.46589727e-04\n",
      "Epoch: 3455 mean train loss:  4.37597626e-04, mean val. rec. loss:  4.44918398e-04\n",
      "Epoch: 3456 mean train loss:  4.37319206e-04, mean val. rec. loss:  4.45656551e-04\n",
      "Epoch: 3457 mean train loss:  4.38386540e-04, mean val. rec. loss:  4.47967867e-04\n",
      "Epoch: 3458 mean train loss:  4.36174261e-04, mean val. rec. loss:  4.45931205e-04\n",
      "Epoch: 3459 mean train loss:  4.36893941e-04, mean val. rec. loss:  4.43579075e-04\n",
      "Epoch: 3460 mean train loss:  4.37786812e-04, mean val. rec. loss:  4.48748687e-04\n",
      "Epoch: 3461 mean train loss:  4.35606152e-04, mean val. rec. loss:  4.43008142e-04\n",
      "Epoch: 3462 mean train loss:  4.35410684e-04, mean val. rec. loss:  4.46626689e-04\n",
      "Epoch: 3463 mean train loss:  4.35064664e-04, mean val. rec. loss:  4.42583968e-04\n",
      "Epoch: 3464 mean train loss:  4.34804728e-04, mean val. rec. loss:  4.44366728e-04\n",
      "Epoch: 3465 mean train loss:  4.33530995e-04, mean val. rec. loss:  4.41113332e-04\n",
      "Epoch: 3466 mean train loss:  4.34165670e-04, mean val. rec. loss:  4.44665188e-04\n",
      "Epoch: 3467 mean train loss:  4.33496223e-04, mean val. rec. loss:  4.40075630e-04\n",
      "Epoch: 3468 mean train loss:  4.32649598e-04, mean val. rec. loss:  4.40779619e-04\n",
      "Epoch: 3469 mean train loss:  4.32961449e-04, mean val. rec. loss:  4.40887562e-04\n",
      "Epoch: 3470 mean train loss:  4.33140877e-04, mean val. rec. loss:  4.42638703e-04\n",
      "Epoch: 3471 mean train loss:  4.32222320e-04, mean val. rec. loss:  4.39435534e-04\n",
      "Epoch: 3472 mean train loss:  4.31368193e-04, mean val. rec. loss:  4.42503429e-04\n",
      "Epoch: 3473 mean train loss:  4.31811835e-04, mean val. rec. loss:  4.40665716e-04\n",
      "Epoch: 3474 mean train loss:  4.31043436e-04, mean val. rec. loss:  4.38872416e-04\n",
      "Epoch: 3475 mean train loss:  4.30681869e-04, mean val. rec. loss:  4.40479343e-04\n",
      "Epoch: 3476 mean train loss:  4.31616801e-04, mean val. rec. loss:  4.39533628e-04\n",
      "Epoch: 3477 mean train loss:  4.32253830e-04, mean val. rec. loss:  4.36282376e-04\n",
      "Epoch: 3478 mean train loss:  4.29987707e-04, mean val. rec. loss:  4.38922716e-04\n",
      "Epoch: 3479 mean train loss:  4.28233009e-04, mean val. rec. loss:  4.38975161e-04\n",
      "Epoch: 3480 mean train loss:  4.29251028e-04, mean val. rec. loss:  4.38695274e-04\n",
      "Epoch: 3481 mean train loss:  4.27378778e-04, mean val. rec. loss:  4.36337474e-04\n",
      "Epoch: 3482 mean train loss:  4.26936210e-04, mean val. rec. loss:  4.39269877e-04\n",
      "Epoch: 3483 mean train loss:  4.28142938e-04, mean val. rec. loss:  4.35387652e-04\n",
      "Epoch: 3484 mean train loss:  4.27543726e-04, mean val. rec. loss:  4.35437189e-04\n",
      "Epoch: 3485 mean train loss:  4.27114653e-04, mean val. rec. loss:  4.36962850e-04\n",
      "Epoch: 3486 mean train loss:  4.26802097e-04, mean val. rec. loss:  4.36499061e-04\n",
      "Epoch: 3487 mean train loss:  4.25304597e-04, mean val. rec. loss:  4.35935724e-04\n",
      "Epoch: 3488 mean train loss:  4.25682532e-04, mean val. rec. loss:  4.34599526e-04\n",
      "Epoch: 3489 mean train loss:  4.25773867e-04, mean val. rec. loss:  4.33178137e-04\n",
      "Epoch: 3490 mean train loss:  4.24640319e-04, mean val. rec. loss:  4.33438217e-04\n",
      "Epoch: 3491 mean train loss:  4.25380442e-04, mean val. rec. loss:  4.36676930e-04\n",
      "Epoch: 3492 mean train loss:  4.23979494e-04, mean val. rec. loss:  4.33123184e-04\n",
      "Epoch: 3493 mean train loss:  4.23981625e-04, mean val. rec. loss:  4.33373597e-04\n",
      "Epoch: 3494 mean train loss:  4.23757353e-04, mean val. rec. loss:  4.33122239e-04\n",
      "Epoch: 3495 mean train loss:  4.23147856e-04, mean val. rec. loss:  4.32064984e-04\n",
      "Epoch: 3496 mean train loss:  4.23232214e-04, mean val. rec. loss:  4.32543856e-04\n",
      "Epoch: 3497 mean train loss:  4.25624257e-04, mean val. rec. loss:  4.30601254e-04\n",
      "Epoch: 3498 mean train loss:  4.23963240e-04, mean val. rec. loss:  4.32216758e-04\n",
      "Epoch: 3499 mean train loss:  4.24133689e-04, mean val. rec. loss:  4.32107543e-04\n",
      "Epoch: 3500 mean train loss:  4.22271962e-04, mean val. rec. loss:  4.30418115e-04\n",
      "Epoch: 3501 mean train loss:  4.20931174e-04, mean val. rec. loss:  4.30695894e-04\n",
      "Epoch: 3502 mean train loss:  4.21019671e-04, mean val. rec. loss:  4.29626136e-04\n",
      "Epoch: 3503 mean train loss:  4.20051460e-04, mean val. rec. loss:  4.31089067e-04\n",
      "Epoch: 3504 mean train loss:  4.21548060e-04, mean val. rec. loss:  4.27831310e-04\n",
      "Epoch: 3505 mean train loss:  4.21450024e-04, mean val. rec. loss:  4.28311381e-04\n",
      "Epoch: 3506 mean train loss:  4.19503285e-04, mean val. rec. loss:  4.34296160e-04\n",
      "Epoch: 3507 mean train loss:  4.18639068e-04, mean val. rec. loss:  4.27553712e-04\n",
      "Epoch: 3508 mean train loss:  4.20679699e-04, mean val. rec. loss:  4.28825435e-04\n",
      "Epoch: 3509 mean train loss:  4.18397761e-04, mean val. rec. loss:  4.25368984e-04\n",
      "Epoch: 3510 mean train loss:  4.18508141e-04, mean val. rec. loss:  4.31159757e-04\n",
      "Epoch: 3511 mean train loss:  4.19028772e-04, mean val. rec. loss:  4.24686002e-04\n",
      "Epoch: 3512 mean train loss:  4.19710964e-04, mean val. rec. loss:  4.27922316e-04\n",
      "Epoch: 3513 mean train loss:  4.16939102e-04, mean val. rec. loss:  4.27639121e-04\n",
      "Epoch: 3514 mean train loss:  4.16909728e-04, mean val. rec. loss:  4.26177390e-04\n",
      "Epoch: 3515 mean train loss:  4.16463170e-04, mean val. rec. loss:  4.23206607e-04\n",
      "Epoch: 3516 mean train loss:  4.16943601e-04, mean val. rec. loss:  4.29284209e-04\n",
      "Epoch: 3517 mean train loss:  4.17108988e-04, mean val. rec. loss:  4.21700245e-04\n",
      "Epoch: 3518 mean train loss:  4.15304269e-04, mean val. rec. loss:  4.27387001e-04\n",
      "Epoch: 3519 mean train loss:  4.16061139e-04, mean val. rec. loss:  4.23143441e-04\n",
      "Epoch: 3520 mean train loss:  4.16833516e-04, mean val. rec. loss:  4.23185854e-04\n",
      "Epoch: 3521 mean train loss:  4.14456701e-04, mean val. rec. loss:  4.25306762e-04\n",
      "Epoch: 3522 mean train loss:  4.14714754e-04, mean val. rec. loss:  4.21692177e-04\n",
      "Epoch: 3523 mean train loss:  4.13738869e-04, mean val. rec. loss:  4.24192919e-04\n",
      "Epoch: 3524 mean train loss:  4.12150606e-04, mean val. rec. loss:  4.21187173e-04\n",
      "Epoch: 3525 mean train loss:  4.12959387e-04, mean val. rec. loss:  4.21636170e-04\n",
      "Epoch: 3526 mean train loss:  4.12392957e-04, mean val. rec. loss:  4.19019308e-04\n",
      "Epoch: 3527 mean train loss:  4.13255113e-04, mean val. rec. loss:  4.24001057e-04\n",
      "Epoch: 3528 mean train loss:  4.11994901e-04, mean val. rec. loss:  4.20321124e-04\n",
      "Epoch: 3529 mean train loss:  4.11962383e-04, mean val. rec. loss:  4.20032987e-04\n",
      "Epoch: 3530 mean train loss:  4.11494693e-04, mean val. rec. loss:  4.22705201e-04\n",
      "Epoch: 3531 mean train loss:  4.11453392e-04, mean val. rec. loss:  4.18231182e-04\n",
      "Epoch: 3532 mean train loss:  4.11147654e-04, mean val. rec. loss:  4.18349737e-04\n",
      "Epoch: 3533 mean train loss:  4.11819894e-04, mean val. rec. loss:  4.20968634e-04\n",
      "Epoch: 3534 mean train loss:  4.09092843e-04, mean val. rec. loss:  4.17372621e-04\n",
      "Epoch: 3535 mean train loss:  4.10083206e-04, mean val. rec. loss:  4.18830863e-04\n",
      "Epoch: 3536 mean train loss:  4.09447796e-04, mean val. rec. loss:  4.17842153e-04\n",
      "Epoch: 3537 mean train loss:  4.08536926e-04, mean val. rec. loss:  4.21090133e-04\n",
      "Epoch: 3538 mean train loss:  4.08732204e-04, mean val. rec. loss:  4.18335781e-04\n",
      "Epoch: 3539 mean train loss:  4.08242774e-04, mean val. rec. loss:  4.17056789e-04\n",
      "Epoch: 3540 mean train loss:  4.08968409e-04, mean val. rec. loss:  4.18168888e-04\n",
      "Epoch: 3541 mean train loss:  4.08809032e-04, mean val. rec. loss:  4.17762777e-04\n",
      "Epoch: 3542 mean train loss:  4.07770153e-04, mean val. rec. loss:  4.18074538e-04\n",
      "Epoch: 3543 mean train loss:  4.10372780e-04, mean val. rec. loss:  4.14065690e-04\n",
      "Epoch: 3544 mean train loss:  4.08712206e-04, mean val. rec. loss:  4.19612193e-04\n",
      "Epoch: 3545 mean train loss:  4.06750879e-04, mean val. rec. loss:  4.13988712e-04\n",
      "Epoch: 3546 mean train loss:  4.06427561e-04, mean val. rec. loss:  4.15715939e-04\n",
      "Epoch: 3547 mean train loss:  4.06317104e-04, mean val. rec. loss:  4.14638766e-04\n",
      "Epoch: 3548 mean train loss:  4.05677536e-04, mean val. rec. loss:  4.13814478e-04\n",
      "Epoch: 3549 mean train loss:  4.04588876e-04, mean val. rec. loss:  4.11496729e-04\n",
      "Epoch: 3550 mean train loss:  4.05364059e-04, mean val. rec. loss:  4.14809439e-04\n",
      "Epoch: 3551 mean train loss:  4.06256660e-04, mean val. rec. loss:  4.12781646e-04\n",
      "Epoch: 3552 mean train loss:  4.04774096e-04, mean val. rec. loss:  4.13675788e-04\n",
      "Epoch: 3553 mean train loss:  4.04557773e-04, mean val. rec. loss:  4.14403728e-04\n",
      "Epoch: 3554 mean train loss:  4.02552282e-04, mean val. rec. loss:  4.08804344e-04\n",
      "Epoch: 3555 mean train loss:  4.06412487e-04, mean val. rec. loss:  4.16627672e-04\n",
      "Epoch: 3556 mean train loss:  4.03746487e-04, mean val. rec. loss:  4.11472124e-04\n",
      "Epoch: 3557 mean train loss:  4.02701994e-04, mean val. rec. loss:  4.14314793e-04\n",
      "Epoch: 3558 mean train loss:  4.03396308e-04, mean val. rec. loss:  4.10171762e-04\n",
      "Epoch: 3559 mean train loss:  4.03468012e-04, mean val. rec. loss:  4.14137833e-04\n",
      "Epoch: 3560 mean train loss:  4.03490008e-04, mean val. rec. loss:  4.08985448e-04\n",
      "Epoch: 3561 mean train loss:  4.00289411e-04, mean val. rec. loss:  4.12475118e-04\n",
      "Epoch: 3562 mean train loss:  4.00642524e-04, mean val. rec. loss:  4.08754988e-04\n",
      "Epoch: 3563 mean train loss:  4.00960505e-04, mean val. rec. loss:  4.09003293e-04\n",
      "Epoch: 3564 mean train loss:  4.00499749e-04, mean val. rec. loss:  4.07534838e-04\n",
      "Epoch: 3565 mean train loss:  3.99004825e-04, mean val. rec. loss:  4.09393594e-04\n",
      "Epoch: 3566 mean train loss:  3.98977040e-04, mean val. rec. loss:  4.09872176e-04\n",
      "Epoch: 3567 mean train loss:  3.98815547e-04, mean val. rec. loss:  4.06168438e-04\n",
      "Epoch: 3568 mean train loss:  3.98874829e-04, mean val. rec. loss:  4.06655597e-04\n",
      "Epoch: 3569 mean train loss:  3.97469983e-04, mean val. rec. loss:  4.07055565e-04\n",
      "Epoch: 3570 mean train loss:  3.97497333e-04, mean val. rec. loss:  4.07259166e-04\n",
      "Epoch: 3571 mean train loss:  3.97980093e-04, mean val. rec. loss:  4.07809310e-04\n",
      "Epoch: 3572 mean train loss:  3.98645748e-04, mean val. rec. loss:  4.05633995e-04\n",
      "Epoch: 3573 mean train loss:  3.97687671e-04, mean val. rec. loss:  4.06120318e-04\n",
      "Epoch: 3574 mean train loss:  3.98330732e-04, mean val. rec. loss:  4.09937377e-04\n",
      "Epoch: 3575 mean train loss:  3.96995226e-04, mean val. rec. loss:  4.04488059e-04\n",
      "Epoch: 3576 mean train loss:  3.96003349e-04, mean val. rec. loss:  4.05416329e-04\n",
      "Epoch: 3577 mean train loss:  3.95951174e-04, mean val. rec. loss:  4.04635217e-04\n",
      "Epoch: 3578 mean train loss:  3.95142470e-04, mean val. rec. loss:  4.03812201e-04\n",
      "Epoch: 3579 mean train loss:  3.95326051e-04, mean val. rec. loss:  4.03833753e-04\n",
      "Epoch: 3580 mean train loss:  3.94524624e-04, mean val. rec. loss:  4.07663424e-04\n",
      "Epoch: 3581 mean train loss:  3.95020848e-04, mean val. rec. loss:  4.01890460e-04\n",
      "Epoch: 3582 mean train loss:  3.94270302e-04, mean val. rec. loss:  4.06183484e-04\n",
      "Epoch: 3583 mean train loss:  3.93638972e-04, mean val. rec. loss:  4.02559594e-04\n",
      "Epoch: 3584 mean train loss:  3.93173376e-04, mean val. rec. loss:  4.02233077e-04\n",
      "Epoch: 3585 mean train loss:  3.92733510e-04, mean val. rec. loss:  4.00165523e-04\n",
      "Epoch: 3586 mean train loss:  3.92852005e-04, mean val. rec. loss:  4.02538151e-04\n",
      "Epoch: 3587 mean train loss:  3.93450942e-04, mean val. rec. loss:  4.01532359e-04\n",
      "Epoch: 3588 mean train loss:  3.93986666e-04, mean val. rec. loss:  4.01101061e-04\n",
      "Epoch: 3589 mean train loss:  3.91565134e-04, mean val. rec. loss:  4.00407067e-04\n",
      "Epoch: 3590 mean train loss:  3.91180078e-04, mean val. rec. loss:  4.03669695e-04\n",
      "Epoch: 3591 mean train loss:  3.91479033e-04, mean val. rec. loss:  3.99423372e-04\n",
      "Epoch: 3592 mean train loss:  3.91299503e-04, mean val. rec. loss:  4.00800312e-04\n",
      "Epoch: 3593 mean train loss:  3.91265016e-04, mean val. rec. loss:  4.01461524e-04\n",
      "Epoch: 3594 mean train loss:  3.89737697e-04, mean val. rec. loss:  3.99001088e-04\n",
      "Epoch: 3595 mean train loss:  3.90099796e-04, mean val. rec. loss:  3.97187290e-04\n",
      "Epoch: 3596 mean train loss:  3.89844616e-04, mean val. rec. loss:  4.01449349e-04\n",
      "Epoch: 3597 mean train loss:  3.89889442e-04, mean val. rec. loss:  3.96370851e-04\n",
      "Epoch: 3598 mean train loss:  3.89484054e-04, mean val. rec. loss:  3.97319910e-04\n",
      "Epoch: 3599 mean train loss:  3.88478674e-04, mean val. rec. loss:  3.98312836e-04\n",
      "Epoch: 3600 mean train loss:  3.89879597e-04, mean val. rec. loss:  4.00195325e-04\n",
      "Epoch: 3601 mean train loss:  3.88979980e-04, mean val. rec. loss:  3.96134177e-04\n",
      "Epoch: 3602 mean train loss:  3.88178278e-04, mean val. rec. loss:  3.97083781e-04\n",
      "Epoch: 3603 mean train loss:  3.88216027e-04, mean val. rec. loss:  3.98331990e-04\n",
      "Epoch: 3604 mean train loss:  3.88233569e-04, mean val. rec. loss:  3.98684856e-04\n",
      "Epoch: 3605 mean train loss:  3.86769170e-04, mean val. rec. loss:  3.94803867e-04\n",
      "Epoch: 3606 mean train loss:  3.86606711e-04, mean val. rec. loss:  3.97019125e-04\n",
      "Epoch: 3607 mean train loss:  3.87370541e-04, mean val. rec. loss:  3.94892184e-04\n",
      "Epoch: 3608 mean train loss:  3.86911671e-04, mean val. rec. loss:  3.94161155e-04\n",
      "Epoch: 3609 mean train loss:  3.85565758e-04, mean val. rec. loss:  3.95079393e-04\n",
      "Epoch: 3610 mean train loss:  3.85344524e-04, mean val. rec. loss:  3.96476214e-04\n",
      "Epoch: 3611 mean train loss:  3.84752522e-04, mean val. rec. loss:  3.93603597e-04\n",
      "Epoch: 3612 mean train loss:  3.84543573e-04, mean val. rec. loss:  3.94913118e-04\n",
      "Epoch: 3613 mean train loss:  3.83931200e-04, mean val. rec. loss:  3.93509937e-04\n",
      "Epoch: 3614 mean train loss:  3.84297634e-04, mean val. rec. loss:  3.94186523e-04\n",
      "Epoch: 3615 mean train loss:  3.84990383e-04, mean val. rec. loss:  3.92519737e-04\n",
      "Epoch: 3616 mean train loss:  3.84234379e-04, mean val. rec. loss:  3.94777554e-04\n",
      "Epoch: 3617 mean train loss:  3.85221064e-04, mean val. rec. loss:  3.88912457e-04\n",
      "Epoch: 3618 mean train loss:  3.85489197e-04, mean val. rec. loss:  3.96801568e-04\n",
      "Epoch: 3619 mean train loss:  3.84457048e-04, mean val. rec. loss:  3.90480859e-04\n",
      "Epoch: 3620 mean train loss:  3.83188232e-04, mean val. rec. loss:  3.93061013e-04\n",
      "Epoch: 3621 mean train loss:  3.81468332e-04, mean val. rec. loss:  3.90246511e-04\n",
      "Epoch: 3622 mean train loss:  3.81640494e-04, mean val. rec. loss:  3.91840062e-04\n",
      "Epoch: 3623 mean train loss:  3.81510780e-04, mean val. rec. loss:  3.88471092e-04\n",
      "Epoch: 3624 mean train loss:  3.81617053e-04, mean val. rec. loss:  3.91597828e-04\n",
      "Epoch: 3625 mean train loss:  3.81573223e-04, mean val. rec. loss:  3.90046835e-04\n",
      "Epoch: 3626 mean train loss:  3.79561834e-04, mean val. rec. loss:  3.91657033e-04\n",
      "Epoch: 3627 mean train loss:  3.80360170e-04, mean val. rec. loss:  3.87209036e-04\n",
      "Epoch: 3628 mean train loss:  3.80617126e-04, mean val. rec. loss:  3.90267009e-04\n",
      "Epoch: 3629 mean train loss:  3.79604814e-04, mean val. rec. loss:  3.89716865e-04\n",
      "Epoch: 3630 mean train loss:  3.79587293e-04, mean val. rec. loss:  3.87331662e-04\n",
      "Epoch: 3631 mean train loss:  3.78319168e-04, mean val. rec. loss:  3.87698485e-04\n",
      "Epoch: 3632 mean train loss:  3.79118375e-04, mean val. rec. loss:  3.88757775e-04\n",
      "Epoch: 3633 mean train loss:  3.78098271e-04, mean val. rec. loss:  3.85182224e-04\n",
      "Epoch: 3634 mean train loss:  3.77853761e-04, mean val. rec. loss:  3.88400220e-04\n",
      "Epoch: 3635 mean train loss:  3.78190768e-04, mean val. rec. loss:  3.87565246e-04\n",
      "Epoch: 3636 mean train loss:  3.77072515e-04, mean val. rec. loss:  3.87230515e-04\n",
      "Epoch: 3637 mean train loss:  3.76985669e-04, mean val. rec. loss:  3.85884359e-04\n",
      "Epoch: 3638 mean train loss:  3.75975071e-04, mean val. rec. loss:  3.86189978e-04\n",
      "Epoch: 3639 mean train loss:  3.77063404e-04, mean val. rec. loss:  3.85802584e-04\n",
      "Epoch: 3640 mean train loss:  3.76195289e-04, mean val. rec. loss:  3.84689831e-04\n",
      "Epoch: 3641 mean train loss:  3.75303766e-04, mean val. rec. loss:  3.84782000e-04\n",
      "Epoch: 3642 mean train loss:  3.75393294e-04, mean val. rec. loss:  3.85507178e-04\n",
      "Epoch: 3643 mean train loss:  3.74492415e-04, mean val. rec. loss:  3.84883837e-04\n",
      "Epoch: 3644 mean train loss:  3.75300009e-04, mean val. rec. loss:  3.83038855e-04\n",
      "Epoch: 3645 mean train loss:  3.74825107e-04, mean val. rec. loss:  3.82669815e-04\n",
      "Epoch: 3646 mean train loss:  3.75616327e-04, mean val. rec. loss:  3.85633911e-04\n",
      "Epoch: 3647 mean train loss:  3.74649834e-04, mean val. rec. loss:  3.83462920e-04\n",
      "Epoch: 3648 mean train loss:  3.74626277e-04, mean val. rec. loss:  3.84633207e-04\n",
      "Epoch: 3649 mean train loss:  3.73511333e-04, mean val. rec. loss:  3.80703044e-04\n",
      "Epoch: 3650 mean train loss:  3.74384120e-04, mean val. rec. loss:  3.85800367e-04\n",
      "Epoch: 3651 mean train loss:  3.73052972e-04, mean val. rec. loss:  3.80088025e-04\n",
      "Epoch: 3652 mean train loss:  3.73320622e-04, mean val. rec. loss:  3.82557003e-04\n",
      "Epoch: 3653 mean train loss:  3.71885744e-04, mean val. rec. loss:  3.83442640e-04\n",
      "Epoch: 3654 mean train loss:  3.72749886e-04, mean val. rec. loss:  3.80327498e-04\n",
      "Epoch: 3655 mean train loss:  3.72144013e-04, mean val. rec. loss:  3.84575201e-04\n",
      "Epoch: 3656 mean train loss:  3.71176220e-04, mean val. rec. loss:  3.80206690e-04\n",
      "Epoch: 3657 mean train loss:  3.71658335e-04, mean val. rec. loss:  3.78836800e-04\n",
      "Epoch: 3658 mean train loss:  3.71907684e-04, mean val. rec. loss:  3.78467906e-04\n",
      "Epoch: 3659 mean train loss:  3.70666767e-04, mean val. rec. loss:  3.82765001e-04\n",
      "Epoch: 3660 mean train loss:  3.70289394e-04, mean val. rec. loss:  3.78979561e-04\n",
      "Epoch: 3661 mean train loss:  3.69154058e-04, mean val. rec. loss:  3.79578406e-04\n",
      "Epoch: 3662 mean train loss:  3.70229609e-04, mean val. rec. loss:  3.78261906e-04\n",
      "Epoch: 3663 mean train loss:  3.70015760e-04, mean val. rec. loss:  3.80386303e-04\n",
      "Epoch: 3664 mean train loss:  3.70147464e-04, mean val. rec. loss:  3.77588374e-04\n",
      "Epoch: 3665 mean train loss:  3.68790758e-04, mean val. rec. loss:  3.77077446e-04\n",
      "Epoch: 3666 mean train loss:  3.68902701e-04, mean val. rec. loss:  3.79601121e-04\n",
      "Epoch: 3667 mean train loss:  3.68841363e-04, mean val. rec. loss:  3.78804272e-04\n",
      "Epoch: 3668 mean train loss:  3.69220786e-04, mean val. rec. loss:  3.78284803e-04\n",
      "Epoch: 3669 mean train loss:  3.68172396e-04, mean val. rec. loss:  3.76357465e-04\n",
      "Epoch: 3670 mean train loss:  3.68569729e-04, mean val. rec. loss:  3.78113185e-04\n",
      "Epoch: 3671 mean train loss:  3.68157552e-04, mean val. rec. loss:  3.76824926e-04\n",
      "Epoch: 3672 mean train loss:  3.69905534e-04, mean val. rec. loss:  3.76804936e-04\n",
      "Epoch: 3673 mean train loss:  3.69478122e-04, mean val. rec. loss:  3.77126729e-04\n",
      "Epoch: 3674 mean train loss:  3.69598163e-04, mean val. rec. loss:  3.75314239e-04\n",
      "Epoch: 3675 mean train loss:  3.66357861e-04, mean val. rec. loss:  3.74559259e-04\n",
      "Epoch: 3676 mean train loss:  3.66043525e-04, mean val. rec. loss:  3.76770991e-04\n",
      "Epoch: 3677 mean train loss:  3.66125956e-04, mean val. rec. loss:  3.74107390e-04\n",
      "Epoch: 3678 mean train loss:  3.66206612e-04, mean val. rec. loss:  3.75005239e-04\n",
      "Epoch: 3679 mean train loss:  3.66756627e-04, mean val. rec. loss:  3.71142713e-04\n",
      "Epoch: 3680 mean train loss:  3.65662876e-04, mean val. rec. loss:  3.76712186e-04\n",
      "Epoch: 3681 mean train loss:  3.64187827e-04, mean val. rec. loss:  3.72800740e-04\n",
      "Epoch: 3682 mean train loss:  3.63761589e-04, mean val. rec. loss:  3.72392012e-04\n",
      "Epoch: 3683 mean train loss:  3.63945697e-04, mean val. rec. loss:  3.71333484e-04\n",
      "Epoch: 3684 mean train loss:  3.62656149e-04, mean val. rec. loss:  3.75460815e-04\n",
      "Epoch: 3685 mean train loss:  3.62983808e-04, mean val. rec. loss:  3.70823174e-04\n",
      "Epoch: 3686 mean train loss:  3.63866593e-04, mean val. rec. loss:  3.70490696e-04\n",
      "Epoch: 3687 mean train loss:  3.64709029e-04, mean val. rec. loss:  3.72797578e-04\n",
      "Epoch: 3688 mean train loss:  3.62586928e-04, mean val. rec. loss:  3.72463174e-04\n",
      "Epoch: 3689 mean train loss:  3.61816869e-04, mean val. rec. loss:  3.71869890e-04\n",
      "Epoch: 3690 mean train loss:  3.62125363e-04, mean val. rec. loss:  3.70459150e-04\n",
      "Epoch: 3691 mean train loss:  3.61535967e-04, mean val. rec. loss:  3.73628227e-04\n",
      "Epoch: 3692 mean train loss:  3.61582717e-04, mean val. rec. loss:  3.68494341e-04\n",
      "Epoch: 3693 mean train loss:  3.61862964e-04, mean val. rec. loss:  3.70533401e-04\n",
      "Epoch: 3694 mean train loss:  3.61212731e-04, mean val. rec. loss:  3.69665535e-04\n",
      "Epoch: 3695 mean train loss:  3.61114308e-04, mean val. rec. loss:  3.71286636e-04\n",
      "Epoch: 3696 mean train loss:  3.60922417e-04, mean val. rec. loss:  3.68311456e-04\n",
      "Epoch: 3697 mean train loss:  3.59466586e-04, mean val. rec. loss:  3.68478749e-04\n",
      "Epoch: 3698 mean train loss:  3.61248519e-04, mean val. rec. loss:  3.69075522e-04\n",
      "Epoch: 3699 mean train loss:  3.59083081e-04, mean val. rec. loss:  3.67520059e-04\n",
      "Epoch: 3700 mean train loss:  3.59174455e-04, mean val. rec. loss:  3.69231003e-04\n",
      "Epoch: 3701 mean train loss:  3.59744725e-04, mean val. rec. loss:  3.66762244e-04\n",
      "Epoch: 3702 mean train loss:  3.59567057e-04, mean val. rec. loss:  3.66627916e-04\n",
      "Epoch: 3703 mean train loss:  3.58490469e-04, mean val. rec. loss:  3.68404388e-04\n",
      "Epoch: 3704 mean train loss:  3.58582823e-04, mean val. rec. loss:  3.67668089e-04\n",
      "Epoch: 3705 mean train loss:  3.57416046e-04, mean val. rec. loss:  3.64019413e-04\n",
      "Epoch: 3706 mean train loss:  3.57501181e-04, mean val. rec. loss:  3.68229681e-04\n",
      "Epoch: 3707 mean train loss:  3.58560474e-04, mean val. rec. loss:  3.66587901e-04\n",
      "Epoch: 3708 mean train loss:  3.58173071e-04, mean val. rec. loss:  3.67240426e-04\n",
      "Epoch: 3709 mean train loss:  3.57805118e-04, mean val. rec. loss:  3.65052063e-04\n",
      "Epoch: 3710 mean train loss:  3.58527226e-04, mean val. rec. loss:  3.70563639e-04\n",
      "Epoch: 3711 mean train loss:  3.58495131e-04, mean val. rec. loss:  3.63466689e-04\n",
      "Epoch: 3712 mean train loss:  3.57674182e-04, mean val. rec. loss:  3.66437690e-04\n",
      "Epoch: 3713 mean train loss:  3.56565758e-04, mean val. rec. loss:  3.63959626e-04\n",
      "Epoch: 3714 mean train loss:  3.55588113e-04, mean val. rec. loss:  3.65171600e-04\n",
      "Epoch: 3715 mean train loss:  3.55034644e-04, mean val. rec. loss:  3.61291374e-04\n",
      "Epoch: 3716 mean train loss:  3.55038342e-04, mean val. rec. loss:  3.64556254e-04\n",
      "Epoch: 3717 mean train loss:  3.54250882e-04, mean val. rec. loss:  3.64947610e-04\n",
      "Epoch: 3718 mean train loss:  3.53432091e-04, mean val. rec. loss:  3.64234280e-04\n",
      "Epoch: 3719 mean train loss:  3.53241023e-04, mean val. rec. loss:  3.60818934e-04\n",
      "Epoch: 3720 mean train loss:  3.53692613e-04, mean val. rec. loss:  3.64422217e-04\n",
      "Epoch: 3721 mean train loss:  3.52608790e-04, mean val. rec. loss:  3.60797673e-04\n",
      "Epoch: 3722 mean train loss:  3.52749694e-04, mean val. rec. loss:  3.59735329e-04\n",
      "Epoch: 3723 mean train loss:  3.52664474e-04, mean val. rec. loss:  3.63115457e-04\n",
      "Epoch: 3724 mean train loss:  3.51925113e-04, mean val. rec. loss:  3.59620481e-04\n",
      "Epoch: 3725 mean train loss:  3.51164715e-04, mean val. rec. loss:  3.63444773e-04\n",
      "Epoch: 3726 mean train loss:  3.52200653e-04, mean val. rec. loss:  3.60243131e-04\n",
      "Epoch: 3727 mean train loss:  3.52206957e-04, mean val. rec. loss:  3.59362945e-04\n",
      "Epoch: 3728 mean train loss:  3.53731820e-04, mean val. rec. loss:  3.61468552e-04\n",
      "Epoch: 3729 mean train loss:  3.53639074e-04, mean val. rec. loss:  3.64510206e-04\n",
      "Epoch: 3730 mean train loss:  3.53482321e-04, mean val. rec. loss:  3.57347073e-04\n",
      "Epoch: 3731 mean train loss:  3.51858577e-04, mean val. rec. loss:  3.64194774e-04\n",
      "Epoch: 3732 mean train loss:  3.50889922e-04, mean val. rec. loss:  3.57702556e-04\n",
      "Epoch: 3733 mean train loss:  3.50464729e-04, mean val. rec. loss:  3.59464528e-04\n",
      "Epoch: 3734 mean train loss:  3.50230743e-04, mean val. rec. loss:  3.59241192e-04\n",
      "Epoch: 3735 mean train loss:  3.49560980e-04, mean val. rec. loss:  3.58478398e-04\n",
      "Epoch: 3736 mean train loss:  3.49088348e-04, mean val. rec. loss:  3.57495575e-04\n",
      "Epoch: 3737 mean train loss:  3.49474598e-04, mean val. rec. loss:  3.58980095e-04\n",
      "Epoch: 3738 mean train loss:  3.49731063e-04, mean val. rec. loss:  3.57203040e-04\n",
      "Epoch: 3739 mean train loss:  3.49458926e-04, mean val. rec. loss:  3.57723818e-04\n",
      "Epoch: 3740 mean train loss:  3.48215909e-04, mean val. rec. loss:  3.59243918e-04\n",
      "Epoch: 3741 mean train loss:  3.47619665e-04, mean val. rec. loss:  3.55032595e-04\n",
      "Epoch: 3742 mean train loss:  3.48071351e-04, mean val. rec. loss:  3.55477013e-04\n",
      "Epoch: 3743 mean train loss:  3.47645023e-04, mean val. rec. loss:  3.56831674e-04\n",
      "Epoch: 3744 mean train loss:  3.47464199e-04, mean val. rec. loss:  3.55307685e-04\n",
      "Epoch: 3745 mean train loss:  3.46853406e-04, mean val. rec. loss:  3.55655754e-04\n",
      "Epoch: 3746 mean train loss:  3.45998765e-04, mean val. rec. loss:  3.55427621e-04\n",
      "Epoch: 3747 mean train loss:  3.45951885e-04, mean val. rec. loss:  3.55166451e-04\n",
      "Epoch: 3748 mean train loss:  3.45048569e-04, mean val. rec. loss:  3.56200628e-04\n",
      "Epoch: 3749 mean train loss:  3.44869111e-04, mean val. rec. loss:  3.54249412e-04\n",
      "Epoch: 3750 mean train loss:  3.45495701e-04, mean val. rec. loss:  3.52951339e-04\n",
      "Epoch: 3751 mean train loss:  3.45329089e-04, mean val. rec. loss:  3.53248599e-04\n",
      "Epoch: 3752 mean train loss:  3.44859320e-04, mean val. rec. loss:  3.53497521e-04\n",
      "Epoch: 3753 mean train loss:  3.45410422e-04, mean val. rec. loss:  3.51706293e-04\n",
      "Epoch: 3754 mean train loss:  3.43947713e-04, mean val. rec. loss:  3.57414673e-04\n",
      "Epoch: 3755 mean train loss:  3.45482622e-04, mean val. rec. loss:  3.51874349e-04\n",
      "Epoch: 3756 mean train loss:  3.45096613e-04, mean val. rec. loss:  3.55653029e-04\n",
      "Epoch: 3757 mean train loss:  3.44033366e-04, mean val. rec. loss:  3.52102227e-04\n",
      "Epoch: 3758 mean train loss:  3.42936192e-04, mean val. rec. loss:  3.54143396e-04\n",
      "Epoch: 3759 mean train loss:  3.43758401e-04, mean val. rec. loss:  3.49745300e-04\n",
      "Epoch: 3760 mean train loss:  3.44691957e-04, mean val. rec. loss:  3.52729893e-04\n",
      "Epoch: 3761 mean train loss:  3.44640856e-04, mean val. rec. loss:  3.51977348e-04\n",
      "Epoch: 3762 mean train loss:  3.41930419e-04, mean val. rec. loss:  3.51694263e-04\n",
      "Epoch: 3763 mean train loss:  3.42241194e-04, mean val. rec. loss:  3.50262443e-04\n",
      "Epoch: 3764 mean train loss:  3.41608647e-04, mean val. rec. loss:  3.53447766e-04\n",
      "Epoch: 3765 mean train loss:  3.42211021e-04, mean val. rec. loss:  3.46871302e-04\n",
      "Epoch: 3766 mean train loss:  3.41854635e-04, mean val. rec. loss:  3.50367550e-04\n",
      "Epoch: 3767 mean train loss:  3.40435706e-04, mean val. rec. loss:  3.49889405e-04\n",
      "Epoch: 3768 mean train loss:  3.41246444e-04, mean val. rec. loss:  3.51434074e-04\n",
      "Epoch: 3769 mean train loss:  3.41362344e-04, mean val. rec. loss:  3.48493457e-04\n",
      "Epoch: 3770 mean train loss:  3.40983997e-04, mean val. rec. loss:  3.49717605e-04\n",
      "Epoch: 3771 mean train loss:  3.41321229e-04, mean val. rec. loss:  3.49643608e-04\n",
      "Epoch: 3772 mean train loss:  3.40090512e-04, mean val. rec. loss:  3.51374069e-04\n",
      "Epoch: 3773 mean train loss:  3.39060352e-04, mean val. rec. loss:  3.45506065e-04\n",
      "Epoch: 3774 mean train loss:  3.40177263e-04, mean val. rec. loss:  3.48313262e-04\n",
      "Epoch: 3775 mean train loss:  3.39162716e-04, mean val. rec. loss:  3.46585418e-04\n",
      "Epoch: 3776 mean train loss:  3.38507787e-04, mean val. rec. loss:  3.47090204e-04\n",
      "Epoch: 3777 mean train loss:  3.39769450e-04, mean val. rec. loss:  3.50579038e-04\n",
      "Epoch: 3778 mean train loss:  3.38518844e-04, mean val. rec. loss:  3.46306184e-04\n",
      "Epoch: 3779 mean train loss:  3.39496148e-04, mean val. rec. loss:  3.47547124e-04\n",
      "Epoch: 3780 mean train loss:  3.37207396e-04, mean val. rec. loss:  3.47765408e-04\n",
      "Epoch: 3781 mean train loss:  3.39442189e-04, mean val. rec. loss:  3.47936008e-04\n",
      "Epoch: 3782 mean train loss:  3.38664231e-04, mean val. rec. loss:  3.43910805e-04\n",
      "Epoch: 3783 mean train loss:  3.36489025e-04, mean val. rec. loss:  3.48020908e-04\n",
      "Epoch: 3784 mean train loss:  3.38496847e-04, mean val. rec. loss:  3.45862057e-04\n",
      "Epoch: 3785 mean train loss:  3.35877784e-04, mean val. rec. loss:  3.45367557e-04\n",
      "Epoch: 3786 mean train loss:  3.35708765e-04, mean val. rec. loss:  3.45315657e-04\n",
      "Epoch: 3787 mean train loss:  3.36190918e-04, mean val. rec. loss:  3.45799036e-04\n",
      "Epoch: 3788 mean train loss:  3.35645977e-04, mean val. rec. loss:  3.44325784e-04\n",
      "Epoch: 3789 mean train loss:  3.36012433e-04, mean val. rec. loss:  3.43982839e-04\n",
      "Epoch: 3790 mean train loss:  3.35433221e-04, mean val. rec. loss:  3.42126590e-04\n",
      "Epoch: 3791 mean train loss:  3.34648613e-04, mean val. rec. loss:  3.44325675e-04\n",
      "Epoch: 3792 mean train loss:  3.34120591e-04, mean val. rec. loss:  3.42970069e-04\n",
      "Epoch: 3793 mean train loss:  3.34760931e-04, mean val. rec. loss:  3.44254840e-04\n",
      "Epoch: 3794 mean train loss:  3.34826934e-04, mean val. rec. loss:  3.41687406e-04\n",
      "Epoch: 3795 mean train loss:  3.34201260e-04, mean val. rec. loss:  3.42272695e-04\n",
      "Epoch: 3796 mean train loss:  3.33499064e-04, mean val. rec. loss:  3.46121119e-04\n",
      "Epoch: 3797 mean train loss:  3.33256544e-04, mean val. rec. loss:  3.39133783e-04\n",
      "Epoch: 3798 mean train loss:  3.33810832e-04, mean val. rec. loss:  3.43404420e-04\n",
      "Epoch: 3799 mean train loss:  3.33983840e-04, mean val. rec. loss:  3.41295542e-04\n",
      "Epoch: 3800 mean train loss:  3.33180285e-04, mean val. rec. loss:  3.42006727e-04\n",
      "Epoch: 3801 mean train loss:  3.32931218e-04, mean val. rec. loss:  3.41469631e-04\n",
      "Epoch: 3802 mean train loss:  3.32162894e-04, mean val. rec. loss:  3.39802591e-04\n",
      "Epoch: 3803 mean train loss:  3.32185125e-04, mean val. rec. loss:  3.41124215e-04\n",
      "Epoch: 3804 mean train loss:  3.31311331e-04, mean val. rec. loss:  3.39676512e-04\n",
      "Epoch: 3805 mean train loss:  3.31607789e-04, mean val. rec. loss:  3.40546777e-04\n",
      "Epoch: 3806 mean train loss:  3.31438366e-04, mean val. rec. loss:  3.40251588e-04\n",
      "Epoch: 3807 mean train loss:  3.30535340e-04, mean val. rec. loss:  3.37756334e-04\n",
      "Epoch: 3808 mean train loss:  3.29888174e-04, mean val. rec. loss:  3.40066160e-04\n",
      "Epoch: 3809 mean train loss:  3.29557180e-04, mean val. rec. loss:  3.39730302e-04\n",
      "Epoch: 3810 mean train loss:  3.28942767e-04, mean val. rec. loss:  3.38014742e-04\n",
      "Epoch: 3811 mean train loss:  3.30225595e-04, mean val. rec. loss:  3.38936797e-04\n",
      "Epoch: 3812 mean train loss:  3.30929773e-04, mean val. rec. loss:  3.36456590e-04\n",
      "Epoch: 3813 mean train loss:  3.30599596e-04, mean val. rec. loss:  3.38703249e-04\n",
      "Epoch: 3814 mean train loss:  3.29830824e-04, mean val. rec. loss:  3.36429695e-04\n",
      "Epoch: 3815 mean train loss:  3.29981725e-04, mean val. rec. loss:  3.37596274e-04\n",
      "Epoch: 3816 mean train loss:  3.28891461e-04, mean val. rec. loss:  3.37579919e-04\n",
      "Epoch: 3817 mean train loss:  3.28482544e-04, mean val. rec. loss:  3.36982419e-04\n",
      "Epoch: 3818 mean train loss:  3.27772150e-04, mean val. rec. loss:  3.37758515e-04\n",
      "Epoch: 3819 mean train loss:  3.28237623e-04, mean val. rec. loss:  3.34544734e-04\n",
      "Epoch: 3820 mean train loss:  3.27963186e-04, mean val. rec. loss:  3.36452519e-04\n",
      "Epoch: 3821 mean train loss:  3.27822319e-04, mean val. rec. loss:  3.38616240e-04\n",
      "Epoch: 3822 mean train loss:  3.28629837e-04, mean val. rec. loss:  3.34323470e-04\n",
      "Epoch: 3823 mean train loss:  3.28260473e-04, mean val. rec. loss:  3.35575349e-04\n",
      "Epoch: 3824 mean train loss:  3.27309689e-04, mean val. rec. loss:  3.33761769e-04\n",
      "Epoch: 3825 mean train loss:  3.26257522e-04, mean val. rec. loss:  3.35181777e-04\n",
      "Epoch: 3826 mean train loss:  3.25731237e-04, mean val. rec. loss:  3.35756453e-04\n",
      "Epoch: 3827 mean train loss:  3.26425351e-04, mean val. rec. loss:  3.33410174e-04\n",
      "Epoch: 3828 mean train loss:  3.25827190e-04, mean val. rec. loss:  3.32796392e-04\n",
      "Epoch: 3829 mean train loss:  3.25092302e-04, mean val. rec. loss:  3.34692110e-04\n",
      "Epoch: 3830 mean train loss:  3.24804108e-04, mean val. rec. loss:  3.35100402e-04\n",
      "Epoch: 3831 mean train loss:  3.24699890e-04, mean val. rec. loss:  3.32208523e-04\n",
      "Epoch: 3832 mean train loss:  3.23983037e-04, mean val. rec. loss:  3.33355258e-04\n",
      "Epoch: 3833 mean train loss:  3.24404806e-04, mean val. rec. loss:  3.32760447e-04\n",
      "Epoch: 3834 mean train loss:  3.24006437e-04, mean val. rec. loss:  3.34100970e-04\n",
      "Epoch: 3835 mean train loss:  3.24864627e-04, mean val. rec. loss:  3.32032435e-04\n",
      "Epoch: 3836 mean train loss:  3.23280590e-04, mean val. rec. loss:  3.32420337e-04\n",
      "Epoch: 3837 mean train loss:  3.23857505e-04, mean val. rec. loss:  3.30161830e-04\n",
      "Epoch: 3838 mean train loss:  3.23167325e-04, mean val. rec. loss:  3.30839906e-04\n",
      "Epoch: 3839 mean train loss:  3.22870575e-04, mean val. rec. loss:  3.33762605e-04\n",
      "Epoch: 3840 mean train loss:  3.22458976e-04, mean val. rec. loss:  3.30815010e-04\n",
      "Epoch: 3841 mean train loss:  3.22078883e-04, mean val. rec. loss:  3.31242928e-04\n",
      "Epoch: 3842 mean train loss:  3.23104612e-04, mean val. rec. loss:  3.31301914e-04\n",
      "Epoch: 3843 mean train loss:  3.23626075e-04, mean val. rec. loss:  3.33938802e-04\n",
      "Epoch: 3844 mean train loss:  3.24102730e-04, mean val. rec. loss:  3.31649838e-04\n",
      "Epoch: 3845 mean train loss:  3.22498722e-04, mean val. rec. loss:  3.30517823e-04\n",
      "Epoch: 3846 mean train loss:  3.22994002e-04, mean val. rec. loss:  3.29515411e-04\n",
      "Epoch: 3847 mean train loss:  3.20704392e-04, mean val. rec. loss:  3.29325657e-04\n",
      "Epoch: 3848 mean train loss:  3.21396773e-04, mean val. rec. loss:  3.29764333e-04\n",
      "Epoch: 3849 mean train loss:  3.21175273e-04, mean val. rec. loss:  3.28524629e-04\n",
      "Epoch: 3850 mean train loss:  3.20164871e-04, mean val. rec. loss:  3.30535341e-04\n",
      "Epoch: 3851 mean train loss:  3.19786095e-04, mean val. rec. loss:  3.28928995e-04\n",
      "Epoch: 3852 mean train loss:  3.19586123e-04, mean val. rec. loss:  3.27474497e-04\n",
      "Epoch: 3853 mean train loss:  3.20775171e-04, mean val. rec. loss:  3.27897072e-04\n",
      "Epoch: 3854 mean train loss:  3.20037213e-04, mean val. rec. loss:  3.28740550e-04\n",
      "Epoch: 3855 mean train loss:  3.18240704e-04, mean val. rec. loss:  3.26836654e-04\n",
      "Epoch: 3856 mean train loss:  3.18731103e-04, mean val. rec. loss:  3.27961583e-04\n",
      "Epoch: 3857 mean train loss:  3.18901384e-04, mean val. rec. loss:  3.25949345e-04\n",
      "Epoch: 3858 mean train loss:  3.18720598e-04, mean val. rec. loss:  3.33268032e-04\n",
      "Epoch: 3859 mean train loss:  3.18294402e-04, mean val. rec. loss:  3.23690438e-04\n",
      "Epoch: 3860 mean train loss:  3.19278957e-04, mean val. rec. loss:  3.24491611e-04\n",
      "Epoch: 3861 mean train loss:  3.18004430e-04, mean val. rec. loss:  3.31160935e-04\n",
      "Epoch: 3862 mean train loss:  3.18604511e-04, mean val. rec. loss:  3.23356942e-04\n",
      "Epoch: 3863 mean train loss:  3.17439261e-04, mean val. rec. loss:  3.26436685e-04\n",
      "Epoch: 3864 mean train loss:  3.16572791e-04, mean val. rec. loss:  3.23233008e-04\n",
      "Epoch: 3865 mean train loss:  3.16947419e-04, mean val. rec. loss:  3.27608425e-04\n",
      "Epoch: 3866 mean train loss:  3.16530386e-04, mean val. rec. loss:  3.25320479e-04\n",
      "Epoch: 3867 mean train loss:  3.16705418e-04, mean val. rec. loss:  3.26858897e-04\n",
      "Epoch: 3868 mean train loss:  3.15738102e-04, mean val. rec. loss:  3.23326195e-04\n",
      "Epoch: 3869 mean train loss:  3.15864224e-04, mean val. rec. loss:  3.25990814e-04\n",
      "Epoch: 3870 mean train loss:  3.15733653e-04, mean val. rec. loss:  3.23625890e-04\n",
      "Epoch: 3871 mean train loss:  3.15800480e-04, mean val. rec. loss:  3.24548745e-04\n",
      "Epoch: 3872 mean train loss:  3.15357943e-04, mean val. rec. loss:  3.23128046e-04\n",
      "Epoch: 3873 mean train loss:  3.14772376e-04, mean val. rec. loss:  3.24816747e-04\n",
      "Epoch: 3874 mean train loss:  3.14538903e-04, mean val. rec. loss:  3.22156999e-04\n",
      "Epoch: 3875 mean train loss:  3.14615438e-04, mean val. rec. loss:  3.23291886e-04\n",
      "Epoch: 3876 mean train loss:  3.13915376e-04, mean val. rec. loss:  3.21331329e-04\n",
      "Epoch: 3877 mean train loss:  3.13549865e-04, mean val. rec. loss:  3.24131803e-04\n",
      "Epoch: 3878 mean train loss:  3.13875924e-04, mean val. rec. loss:  3.21027128e-04\n",
      "Epoch: 3879 mean train loss:  3.13506385e-04, mean val. rec. loss:  3.24337221e-04\n",
      "Epoch: 3880 mean train loss:  3.13014602e-04, mean val. rec. loss:  3.21190459e-04\n",
      "Epoch: 3881 mean train loss:  3.13325759e-04, mean val. rec. loss:  3.21983273e-04\n",
      "Epoch: 3882 mean train loss:  3.13831100e-04, mean val. rec. loss:  3.20821710e-04\n",
      "Epoch: 3883 mean train loss:  3.12870821e-04, mean val. rec. loss:  3.22410137e-04\n",
      "Epoch: 3884 mean train loss:  3.11728535e-04, mean val. rec. loss:  3.19664217e-04\n",
      "Epoch: 3885 mean train loss:  3.12277702e-04, mean val. rec. loss:  3.22056980e-04\n",
      "Epoch: 3886 mean train loss:  3.12130800e-04, mean val. rec. loss:  3.18922793e-04\n",
      "Epoch: 3887 mean train loss:  3.11124197e-04, mean val. rec. loss:  3.19289652e-04\n",
      "Epoch: 3888 mean train loss:  3.11016288e-04, mean val. rec. loss:  3.19402029e-04\n",
      "Epoch: 3889 mean train loss:  3.11679269e-04, mean val. rec. loss:  3.21067979e-04\n",
      "Epoch: 3890 mean train loss:  3.10476715e-04, mean val. rec. loss:  3.19413296e-04\n",
      "Epoch: 3891 mean train loss:  3.10466435e-04, mean val. rec. loss:  3.19977214e-04\n",
      "Epoch: 3892 mean train loss:  3.10139701e-04, mean val. rec. loss:  3.18727551e-04\n",
      "Epoch: 3893 mean train loss:  3.10127909e-04, mean val. rec. loss:  3.17140105e-04\n",
      "Epoch: 3894 mean train loss:  3.10065837e-04, mean val. rec. loss:  3.19097391e-04\n",
      "Epoch: 3895 mean train loss:  3.09411238e-04, mean val. rec. loss:  3.17781727e-04\n",
      "Epoch: 3896 mean train loss:  3.08556274e-04, mean val. rec. loss:  3.19272716e-04\n",
      "Epoch: 3897 mean train loss:  3.09473819e-04, mean val. rec. loss:  3.17023985e-04\n",
      "Epoch: 3898 mean train loss:  3.09358958e-04, mean val. rec. loss:  3.20777879e-04\n",
      "Epoch: 3899 mean train loss:  3.09324015e-04, mean val. rec. loss:  3.15486222e-04\n",
      "Epoch: 3900 mean train loss:  3.09468111e-04, mean val. rec. loss:  3.18795951e-04\n",
      "Epoch: 3901 mean train loss:  3.09485451e-04, mean val. rec. loss:  3.16247271e-04\n",
      "Epoch: 3902 mean train loss:  3.07843776e-04, mean val. rec. loss:  3.16841501e-04\n",
      "Epoch: 3903 mean train loss:  3.07614113e-04, mean val. rec. loss:  3.17195131e-04\n",
      "Epoch: 3904 mean train loss:  3.07538976e-04, mean val. rec. loss:  3.15951865e-04\n",
      "Epoch: 3905 mean train loss:  3.08668769e-04, mean val. rec. loss:  3.19591819e-04\n",
      "Epoch: 3906 mean train loss:  3.08318282e-04, mean val. rec. loss:  3.13154990e-04\n",
      "Epoch: 3907 mean train loss:  3.07972881e-04, mean val. rec. loss:  3.17055060e-04\n",
      "Epoch: 3908 mean train loss:  3.06635354e-04, mean val. rec. loss:  3.13406274e-04\n",
      "Epoch: 3909 mean train loss:  3.07252517e-04, mean val. rec. loss:  3.15492618e-04\n",
      "Epoch: 3910 mean train loss:  3.06822837e-04, mean val. rec. loss:  3.13820490e-04\n",
      "Epoch: 3911 mean train loss:  3.06646675e-04, mean val. rec. loss:  3.17332766e-04\n",
      "Epoch: 3912 mean train loss:  3.07035452e-04, mean val. rec. loss:  3.14910528e-04\n",
      "Epoch: 3913 mean train loss:  3.06005379e-04, mean val. rec. loss:  3.12967635e-04\n",
      "Epoch: 3914 mean train loss:  3.06058221e-04, mean val. rec. loss:  3.15636760e-04\n",
      "Epoch: 3915 mean train loss:  3.06131052e-04, mean val. rec. loss:  3.13245778e-04\n",
      "Epoch: 3916 mean train loss:  3.05982725e-04, mean val. rec. loss:  3.13792323e-04\n",
      "Epoch: 3917 mean train loss:  3.06893664e-04, mean val. rec. loss:  3.14991685e-04\n",
      "Epoch: 3918 mean train loss:  3.05807288e-04, mean val. rec. loss:  3.10293712e-04\n",
      "Epoch: 3919 mean train loss:  3.05576989e-04, mean val. rec. loss:  3.16638845e-04\n",
      "Epoch: 3920 mean train loss:  3.05336606e-04, mean val. rec. loss:  3.12611134e-04\n",
      "Epoch: 3921 mean train loss:  3.05406388e-04, mean val. rec. loss:  3.14584011e-04\n",
      "Epoch: 3922 mean train loss:  3.04214047e-04, mean val. rec. loss:  3.09650200e-04\n",
      "Epoch: 3923 mean train loss:  3.04977793e-04, mean val. rec. loss:  3.15028211e-04\n",
      "Epoch: 3924 mean train loss:  3.03570979e-04, mean val. rec. loss:  3.11172372e-04\n",
      "Epoch: 3925 mean train loss:  3.04148249e-04, mean val. rec. loss:  3.14078316e-04\n",
      "Epoch: 3926 mean train loss:  3.02934962e-04, mean val. rec. loss:  3.09726523e-04\n",
      "Epoch: 3927 mean train loss:  3.03218886e-04, mean val. rec. loss:  3.11851792e-04\n",
      "Epoch: 3928 mean train loss:  3.01952395e-04, mean val. rec. loss:  3.08899036e-04\n",
      "Epoch: 3929 mean train loss:  3.01312892e-04, mean val. rec. loss:  3.12644607e-04\n",
      "Epoch: 3930 mean train loss:  3.02021470e-04, mean val. rec. loss:  3.09256882e-04\n",
      "Epoch: 3931 mean train loss:  3.01879949e-04, mean val. rec. loss:  3.11375755e-04\n",
      "Epoch: 3932 mean train loss:  3.01318989e-04, mean val. rec. loss:  3.09211452e-04\n",
      "Epoch: 3933 mean train loss:  3.00887883e-04, mean val. rec. loss:  3.10937915e-04\n",
      "Epoch: 3934 mean train loss:  3.02140785e-04, mean val. rec. loss:  3.07111406e-04\n",
      "Epoch: 3935 mean train loss:  3.00933085e-04, mean val. rec. loss:  3.11127087e-04\n",
      "Epoch: 3936 mean train loss:  3.00071643e-04, mean val. rec. loss:  3.08499176e-04\n",
      "Epoch: 3937 mean train loss:  3.01314431e-04, mean val. rec. loss:  3.06670295e-04\n",
      "Epoch: 3938 mean train loss:  3.00412903e-04, mean val. rec. loss:  3.10044936e-04\n",
      "Epoch: 3939 mean train loss:  3.00091113e-04, mean val. rec. loss:  3.07767784e-04\n",
      "Epoch: 3940 mean train loss:  3.01679962e-04, mean val. rec. loss:  3.08885952e-04\n",
      "Epoch: 3941 mean train loss:  3.00377091e-04, mean val. rec. loss:  3.09710241e-04\n",
      "Epoch: 3942 mean train loss:  3.00504888e-04, mean val. rec. loss:  3.04882665e-04\n",
      "Epoch: 3943 mean train loss:  2.99665374e-04, mean val. rec. loss:  3.08260612e-04\n",
      "Epoch: 3944 mean train loss:  3.00173155e-04, mean val. rec. loss:  3.06280066e-04\n",
      "Epoch: 3945 mean train loss:  2.99838696e-04, mean val. rec. loss:  3.09155700e-04\n",
      "Epoch: 3946 mean train loss:  2.99607787e-04, mean val. rec. loss:  3.05456577e-04\n",
      "Epoch: 3947 mean train loss:  2.99294792e-04, mean val. rec. loss:  3.05685982e-04\n",
      "Epoch: 3948 mean train loss:  2.97676442e-04, mean val. rec. loss:  3.08081072e-04\n",
      "Epoch: 3949 mean train loss:  2.98203032e-04, mean val. rec. loss:  3.05624052e-04\n",
      "Epoch: 3950 mean train loss:  2.96761012e-04, mean val. rec. loss:  3.05414273e-04\n",
      "Epoch: 3951 mean train loss:  2.97016959e-04, mean val. rec. loss:  3.05612858e-04\n",
      "Epoch: 3952 mean train loss:  2.97189248e-04, mean val. rec. loss:  3.05725307e-04\n",
      "Epoch: 3953 mean train loss:  2.97135625e-04, mean val. rec. loss:  3.05588507e-04\n",
      "Epoch: 3954 mean train loss:  2.97101091e-04, mean val. rec. loss:  3.04075785e-04\n",
      "Epoch: 3955 mean train loss:  2.96005529e-04, mean val. rec. loss:  3.05533336e-04\n",
      "Epoch: 3956 mean train loss:  2.96425129e-04, mean val. rec. loss:  3.03849960e-04\n",
      "Epoch: 3957 mean train loss:  2.96077251e-04, mean val. rec. loss:  3.02756596e-04\n",
      "Epoch: 3958 mean train loss:  2.95765257e-04, mean val. rec. loss:  3.04510299e-04\n",
      "Epoch: 3959 mean train loss:  2.95470718e-04, mean val. rec. loss:  3.03428220e-04\n",
      "Epoch: 3960 mean train loss:  2.95349974e-04, mean val. rec. loss:  3.06046591e-04\n",
      "Epoch: 3961 mean train loss:  2.95921834e-04, mean val. rec. loss:  3.02482505e-04\n",
      "Epoch: 3962 mean train loss:  2.96824938e-04, mean val. rec. loss:  3.05466536e-04\n",
      "Epoch: 3963 mean train loss:  2.95050731e-04, mean val. rec. loss:  3.02550015e-04\n",
      "Epoch: 3964 mean train loss:  2.94759447e-04, mean val. rec. loss:  3.01564921e-04\n",
      "Epoch: 3965 mean train loss:  2.95358206e-04, mean val. rec. loss:  3.04535268e-04\n",
      "Epoch: 3966 mean train loss:  2.94646743e-04, mean val. rec. loss:  3.02851564e-04\n",
      "Epoch: 3967 mean train loss:  2.95162411e-04, mean val. rec. loss:  3.06134653e-04\n",
      "Epoch: 3968 mean train loss:  2.95065725e-04, mean val. rec. loss:  3.02170617e-04\n",
      "Epoch: 3969 mean train loss:  2.93125800e-04, mean val. rec. loss:  3.04272898e-04\n",
      "Epoch: 3970 mean train loss:  2.92807617e-04, mean val. rec. loss:  3.00044185e-04\n",
      "Epoch: 3971 mean train loss:  2.92869242e-04, mean val. rec. loss:  3.02336583e-04\n",
      "Epoch: 3972 mean train loss:  2.92294278e-04, mean val. rec. loss:  3.00273154e-04\n",
      "Epoch: 3973 mean train loss:  2.92205476e-04, mean val. rec. loss:  3.01954477e-04\n",
      "Epoch: 3974 mean train loss:  2.92813203e-04, mean val. rec. loss:  3.00669725e-04\n",
      "Epoch: 3975 mean train loss:  2.91709564e-04, mean val. rec. loss:  3.01934670e-04\n",
      "Epoch: 3976 mean train loss:  2.92092309e-04, mean val. rec. loss:  2.97700977e-04\n",
      "Epoch: 3977 mean train loss:  2.91771550e-04, mean val. rec. loss:  3.02301783e-04\n",
      "Epoch: 3978 mean train loss:  2.91794322e-04, mean val. rec. loss:  3.00361907e-04\n",
      "Epoch: 3979 mean train loss:  2.91522250e-04, mean val. rec. loss:  2.97456217e-04\n",
      "Epoch: 3980 mean train loss:  2.91883054e-04, mean val. rec. loss:  3.03757118e-04\n",
      "Epoch: 3981 mean train loss:  2.91785073e-04, mean val. rec. loss:  2.97753022e-04\n",
      "Epoch: 3982 mean train loss:  2.91987928e-04, mean val. rec. loss:  3.00372446e-04\n",
      "Epoch: 3983 mean train loss:  2.90623440e-04, mean val. rec. loss:  2.96959517e-04\n",
      "Epoch: 3984 mean train loss:  2.90214160e-04, mean val. rec. loss:  2.99377376e-04\n",
      "Epoch: 3985 mean train loss:  2.89944159e-04, mean val. rec. loss:  2.95749343e-04\n",
      "Epoch: 3986 mean train loss:  2.89448114e-04, mean val. rec. loss:  2.97392032e-04\n",
      "Epoch: 3987 mean train loss:  2.89413191e-04, mean val. rec. loss:  2.99498621e-04\n",
      "Epoch: 3988 mean train loss:  2.90106043e-04, mean val. rec. loss:  2.96118765e-04\n",
      "Epoch: 3989 mean train loss:  2.90505654e-04, mean val. rec. loss:  2.98715819e-04\n",
      "Epoch: 3990 mean train loss:  2.89975824e-04, mean val. rec. loss:  2.97589255e-04\n",
      "Epoch: 3991 mean train loss:  2.89550679e-04, mean val. rec. loss:  2.96210152e-04\n",
      "Epoch: 3992 mean train loss:  2.89295424e-04, mean val. rec. loss:  2.96235303e-04\n",
      "Epoch: 3993 mean train loss:  2.88083431e-04, mean val. rec. loss:  2.97308368e-04\n",
      "Epoch: 3994 mean train loss:  2.87764939e-04, mean val. rec. loss:  2.96549554e-04\n",
      "Epoch: 3995 mean train loss:  2.87751564e-04, mean val. rec. loss:  2.97703867e-04\n",
      "Epoch: 3996 mean train loss:  2.88701866e-04, mean val. rec. loss:  2.94560249e-04\n",
      "Epoch: 3997 mean train loss:  2.88860819e-04, mean val. rec. loss:  2.95373107e-04\n",
      "Epoch: 3998 mean train loss:  2.87510302e-04, mean val. rec. loss:  2.98281232e-04\n",
      "Epoch: 3999 mean train loss:  2.88018143e-04, mean val. rec. loss:  2.95256042e-04\n",
      "Epoch: 4000 mean train loss:  2.87879614e-04, mean val. rec. loss:  2.94176107e-04\n",
      "Epoch: 4001 mean train loss:  2.87773110e-04, mean val. rec. loss:  3.00411189e-04\n",
      "Epoch: 4002 mean train loss:  2.87097411e-04, mean val. rec. loss:  2.92885976e-04\n",
      "Epoch: 4003 mean train loss:  2.86892581e-04, mean val. rec. loss:  2.94156700e-04\n",
      "Epoch: 4004 mean train loss:  2.86013973e-04, mean val. rec. loss:  2.95184680e-04\n",
      "Epoch: 4005 mean train loss:  2.85446837e-04, mean val. rec. loss:  2.93918535e-04\n",
      "Epoch: 4006 mean train loss:  2.85626891e-04, mean val. rec. loss:  2.93567486e-04\n",
      "Epoch: 4007 mean train loss:  2.84848656e-04, mean val. rec. loss:  2.93793656e-04\n",
      "Epoch: 4008 mean train loss:  2.85152464e-04, mean val. rec. loss:  2.92693933e-04\n",
      "Epoch: 4009 mean train loss:  2.86156077e-04, mean val. rec. loss:  2.92573996e-04\n",
      "Epoch: 4010 mean train loss:  2.84457185e-04, mean val. rec. loss:  2.91038686e-04\n",
      "Epoch: 4011 mean train loss:  2.84876222e-04, mean val. rec. loss:  2.91702842e-04\n",
      "Epoch: 4012 mean train loss:  2.85660077e-04, mean val. rec. loss:  2.95179574e-04\n",
      "Epoch: 4013 mean train loss:  2.84992975e-04, mean val. rec. loss:  2.93496542e-04\n",
      "Epoch: 4014 mean train loss:  2.85753521e-04, mean val. rec. loss:  2.95190804e-04\n",
      "Epoch: 4015 mean train loss:  2.84498105e-04, mean val. rec. loss:  2.92177135e-04\n",
      "Epoch: 4016 mean train loss:  2.84345623e-04, mean val. rec. loss:  2.95627208e-04\n",
      "Epoch: 4017 mean train loss:  2.85641337e-04, mean val. rec. loss:  2.90671227e-04\n",
      "Epoch: 4018 mean train loss:  2.84028418e-04, mean val. rec. loss:  2.92056672e-04\n",
      "Epoch: 4019 mean train loss:  2.84765344e-04, mean val. rec. loss:  2.90066186e-04\n",
      "Epoch: 4020 mean train loss:  2.83516018e-04, mean val. rec. loss:  2.95491553e-04\n",
      "Epoch: 4021 mean train loss:  2.83858514e-04, mean val. rec. loss:  2.88364836e-04\n",
      "Epoch: 4022 mean train loss:  2.82616433e-04, mean val. rec. loss:  2.91109158e-04\n",
      "Epoch: 4023 mean train loss:  2.83346130e-04, mean val. rec. loss:  2.92075880e-04\n",
      "Epoch: 4024 mean train loss:  2.83039758e-04, mean val. rec. loss:  2.90241092e-04\n",
      "Epoch: 4025 mean train loss:  2.81774678e-04, mean val. rec. loss:  2.88735294e-04\n",
      "Epoch: 4026 mean train loss:  2.82412100e-04, mean val. rec. loss:  2.88571799e-04\n",
      "Epoch: 4027 mean train loss:  2.81090035e-04, mean val. rec. loss:  2.89505193e-04\n",
      "Epoch: 4028 mean train loss:  2.81255906e-04, mean val. rec. loss:  2.88471925e-04\n",
      "Epoch: 4029 mean train loss:  2.80680169e-04, mean val. rec. loss:  2.90077416e-04\n",
      "Epoch: 4030 mean train loss:  2.80613114e-04, mean val. rec. loss:  2.86926147e-04\n",
      "Epoch: 4031 mean train loss:  2.80950961e-04, mean val. rec. loss:  2.92779214e-04\n",
      "Epoch: 4032 mean train loss:  2.80618292e-04, mean val. rec. loss:  2.85023542e-04\n",
      "Epoch: 4033 mean train loss:  2.79824536e-04, mean val. rec. loss:  2.88637455e-04\n",
      "Epoch: 4034 mean train loss:  2.80942731e-04, mean val. rec. loss:  2.90497992e-04\n",
      "Epoch: 4035 mean train loss:  2.81393728e-04, mean val. rec. loss:  2.84947964e-04\n",
      "Epoch: 4036 mean train loss:  2.79930608e-04, mean val. rec. loss:  2.88511994e-04\n",
      "Epoch: 4037 mean train loss:  2.79694617e-04, mean val. rec. loss:  2.89119362e-04\n",
      "Epoch: 4038 mean train loss:  2.80908582e-04, mean val. rec. loss:  2.85296324e-04\n",
      "Epoch: 4039 mean train loss:  2.78723488e-04, mean val. rec. loss:  2.89928513e-04\n",
      "Epoch: 4040 mean train loss:  2.79288366e-04, mean val. rec. loss:  2.85266340e-04\n",
      "Epoch: 4041 mean train loss:  2.78757953e-04, mean val. rec. loss:  2.88668311e-04\n",
      "Epoch: 4042 mean train loss:  2.79203871e-04, mean val. rec. loss:  2.85274863e-04\n",
      "Epoch: 4043 mean train loss:  2.78467608e-04, mean val. rec. loss:  2.86383309e-04\n",
      "Epoch: 4044 mean train loss:  2.77607222e-04, mean val. rec. loss:  2.84703567e-04\n",
      "Epoch: 4045 mean train loss:  2.78003141e-04, mean val. rec. loss:  2.88556643e-04\n",
      "Epoch: 4046 mean train loss:  2.78319411e-04, mean val. rec. loss:  2.84269034e-04\n",
      "Epoch: 4047 mean train loss:  2.77099305e-04, mean val. rec. loss:  2.91797119e-04\n",
      "Epoch: 4048 mean train loss:  2.79233380e-04, mean val. rec. loss:  2.84721103e-04\n",
      "Epoch: 4049 mean train loss:  2.78257992e-04, mean val. rec. loss:  2.83355538e-04\n",
      "Epoch: 4050 mean train loss:  2.77174849e-04, mean val. rec. loss:  2.87517360e-04\n",
      "Epoch: 4051 mean train loss:  2.77313152e-04, mean val. rec. loss:  2.83628593e-04\n",
      "Epoch: 4052 mean train loss:  2.76473127e-04, mean val. rec. loss:  2.83802319e-04\n",
      "Epoch: 4053 mean train loss:  2.76754377e-04, mean val. rec. loss:  2.83847204e-04\n",
      "Epoch: 4054 mean train loss:  2.76171614e-04, mean val. rec. loss:  2.84889050e-04\n",
      "Epoch: 4055 mean train loss:  2.75800115e-04, mean val. rec. loss:  2.83266640e-04\n",
      "Epoch: 4056 mean train loss:  2.76274718e-04, mean val. rec. loss:  2.83482889e-04\n",
      "Epoch: 4057 mean train loss:  2.74906464e-04, mean val. rec. loss:  2.84342686e-04\n",
      "Epoch: 4058 mean train loss:  2.75575587e-04, mean val. rec. loss:  2.82615587e-04\n",
      "Epoch: 4059 mean train loss:  2.75569143e-04, mean val. rec. loss:  2.83157644e-04\n",
      "Epoch: 4060 mean train loss:  2.75289889e-04, mean val. rec. loss:  2.83697647e-04\n",
      "Epoch: 4061 mean train loss:  2.75848861e-04, mean val. rec. loss:  2.84175720e-04\n",
      "Epoch: 4062 mean train loss:  2.76827488e-04, mean val. rec. loss:  2.80980003e-04\n",
      "Epoch: 4063 mean train loss:  2.76633012e-04, mean val. rec. loss:  2.83855563e-04\n",
      "Epoch: 4064 mean train loss:  2.73911903e-04, mean val. rec. loss:  2.83730648e-04\n",
      "Epoch: 4065 mean train loss:  2.74904582e-04, mean val. rec. loss:  2.84048243e-04\n",
      "Epoch: 4066 mean train loss:  2.75126683e-04, mean val. rec. loss:  2.81898532e-04\n",
      "Epoch: 4067 mean train loss:  2.74315008e-04, mean val. rec. loss:  2.83445600e-04\n",
      "Epoch: 4068 mean train loss:  2.74319549e-04, mean val. rec. loss:  2.82335191e-04\n",
      "Epoch: 4069 mean train loss:  2.74808518e-04, mean val. rec. loss:  2.86425832e-04\n",
      "Epoch: 4070 mean train loss:  2.73418332e-04, mean val. rec. loss:  2.80112791e-04\n",
      "Epoch: 4071 mean train loss:  2.72631686e-04, mean val. rec. loss:  2.84088312e-04\n",
      "Epoch: 4072 mean train loss:  2.72896782e-04, mean val. rec. loss:  2.79257319e-04\n",
      "Epoch: 4073 mean train loss:  2.72526252e-04, mean val. rec. loss:  2.81876398e-04\n",
      "Epoch: 4074 mean train loss:  2.73322092e-04, mean val. rec. loss:  2.80169979e-04\n",
      "Epoch: 4075 mean train loss:  2.72391230e-04, mean val. rec. loss:  2.79381072e-04\n",
      "Epoch: 4076 mean train loss:  2.71893660e-04, mean val. rec. loss:  2.79078760e-04\n",
      "Epoch: 4077 mean train loss:  2.71875866e-04, mean val. rec. loss:  2.81297634e-04\n",
      "Epoch: 4078 mean train loss:  2.71232060e-04, mean val. rec. loss:  2.78110657e-04\n",
      "Epoch: 4079 mean train loss:  2.70939949e-04, mean val. rec. loss:  2.76623812e-04\n",
      "Epoch: 4080 mean train loss:  2.70863256e-04, mean val. rec. loss:  2.80891650e-04\n",
      "Epoch: 4081 mean train loss:  2.71608815e-04, mean val. rec. loss:  2.78745592e-04\n",
      "Epoch: 4082 mean train loss:  2.71727054e-04, mean val. rec. loss:  2.79250741e-04\n",
      "Epoch: 4083 mean train loss:  2.71433744e-04, mean val. rec. loss:  2.75666757e-04\n",
      "Epoch: 4084 mean train loss:  2.72080630e-04, mean val. rec. loss:  2.80397113e-04\n",
      "Epoch: 4085 mean train loss:  2.71119025e-04, mean val. rec. loss:  2.76797047e-04\n",
      "Epoch: 4086 mean train loss:  2.71091727e-04, mean val. rec. loss:  2.77911526e-04\n",
      "Epoch: 4087 mean train loss:  2.69865362e-04, mean val. rec. loss:  2.77181860e-04\n",
      "Epoch: 4088 mean train loss:  2.69768136e-04, mean val. rec. loss:  2.77306103e-04\n",
      "Epoch: 4089 mean train loss:  2.69318040e-04, mean val. rec. loss:  2.76270036e-04\n",
      "Epoch: 4090 mean train loss:  2.69828423e-04, mean val. rec. loss:  2.78448423e-04\n",
      "Epoch: 4091 mean train loss:  2.69874853e-04, mean val. rec. loss:  2.75266352e-04\n",
      "Epoch: 4092 mean train loss:  2.68612801e-04, mean val. rec. loss:  2.78620022e-04\n",
      "Epoch: 4093 mean train loss:  2.68453436e-04, mean val. rec. loss:  2.76129675e-04\n",
      "Epoch: 4094 mean train loss:  2.67899493e-04, mean val. rec. loss:  2.75339586e-04\n",
      "Epoch: 4095 mean train loss:  2.68225523e-04, mean val. rec. loss:  2.75099968e-04\n",
      "Epoch: 4096 mean train loss:  2.68278841e-04, mean val. rec. loss:  2.75172911e-04\n",
      "Epoch: 4097 mean train loss:  2.67343145e-04, mean val. rec. loss:  2.77201213e-04\n",
      "Epoch: 4098 mean train loss:  2.67391151e-04, mean val. rec. loss:  2.75408786e-04\n",
      "Epoch: 4099 mean train loss:  2.67270473e-04, mean val. rec. loss:  2.76174196e-04\n",
      "Epoch: 4100 mean train loss:  2.68574133e-04, mean val. rec. loss:  2.75865052e-04\n",
      "Epoch: 4101 mean train loss:  2.67562740e-04, mean val. rec. loss:  2.76228676e-04\n",
      "Epoch: 4102 mean train loss:  2.67373782e-04, mean val. rec. loss:  2.71566176e-04\n",
      "Epoch: 4103 mean train loss:  2.66913187e-04, mean val. rec. loss:  2.75675771e-04\n",
      "Epoch: 4104 mean train loss:  2.66909414e-04, mean val. rec. loss:  2.74954427e-04\n",
      "Epoch: 4105 mean train loss:  2.65397409e-04, mean val. rec. loss:  2.74323654e-04\n",
      "Epoch: 4106 mean train loss:  2.65516341e-04, mean val. rec. loss:  2.73494604e-04\n",
      "Epoch: 4107 mean train loss:  2.66003036e-04, mean val. rec. loss:  2.75944173e-04\n",
      "Epoch: 4108 mean train loss:  2.66607046e-04, mean val. rec. loss:  2.70075515e-04\n",
      "Epoch: 4109 mean train loss:  2.67087061e-04, mean val. rec. loss:  2.75858037e-04\n",
      "Epoch: 4110 mean train loss:  2.66206506e-04, mean val. rec. loss:  2.70394418e-04\n",
      "Epoch: 4111 mean train loss:  2.65351980e-04, mean val. rec. loss:  2.77253477e-04\n",
      "Epoch: 4112 mean train loss:  2.67191050e-04, mean val. rec. loss:  2.74666126e-04\n",
      "Epoch: 4113 mean train loss:  2.66206079e-04, mean val. rec. loss:  2.69883035e-04\n",
      "Epoch: 4114 mean train loss:  2.64363923e-04, mean val. rec. loss:  2.72579237e-04\n",
      "Epoch: 4115 mean train loss:  2.65566956e-04, mean val. rec. loss:  2.69732352e-04\n",
      "Epoch: 4116 mean train loss:  2.64490316e-04, mean val. rec. loss:  2.74764110e-04\n",
      "Epoch: 4117 mean train loss:  2.64651000e-04, mean val. rec. loss:  2.70101283e-04\n",
      "Epoch: 4118 mean train loss:  2.63649188e-04, mean val. rec. loss:  2.72299495e-04\n",
      "Epoch: 4119 mean train loss:  2.64517704e-04, mean val. rec. loss:  2.71305442e-04\n",
      "Epoch: 4120 mean train loss:  2.65158563e-04, mean val. rec. loss:  2.72605114e-04\n",
      "Epoch: 4121 mean train loss:  2.63945442e-04, mean val. rec. loss:  2.69893466e-04\n",
      "Epoch: 4122 mean train loss:  2.62527675e-04, mean val. rec. loss:  2.75129298e-04\n",
      "Epoch: 4123 mean train loss:  2.63187020e-04, mean val. rec. loss:  2.69074411e-04\n",
      "Epoch: 4124 mean train loss:  2.62720704e-04, mean val. rec. loss:  2.70270665e-04\n",
      "Epoch: 4125 mean train loss:  2.62342564e-04, mean val. rec. loss:  2.70717464e-04\n",
      "Epoch: 4126 mean train loss:  2.62100593e-04, mean val. rec. loss:  2.70626349e-04\n",
      "Epoch: 4127 mean train loss:  2.61901415e-04, mean val. rec. loss:  2.71045398e-04\n",
      "Epoch: 4128 mean train loss:  2.62502285e-04, mean val. rec. loss:  2.69950563e-04\n",
      "Epoch: 4129 mean train loss:  2.61640985e-04, mean val. rec. loss:  2.68140363e-04\n",
      "Epoch: 4130 mean train loss:  2.61437592e-04, mean val. rec. loss:  2.71167988e-04\n",
      "Epoch: 4131 mean train loss:  2.61026107e-04, mean val. rec. loss:  2.66799730e-04\n",
      "Epoch: 4132 mean train loss:  2.61235425e-04, mean val. rec. loss:  2.68899540e-04\n",
      "Epoch: 4133 mean train loss:  2.61893406e-04, mean val. rec. loss:  2.68807935e-04\n",
      "Epoch: 4134 mean train loss:  2.61147467e-04, mean val. rec. loss:  2.68688743e-04\n",
      "Epoch: 4135 mean train loss:  2.61418969e-04, mean val. rec. loss:  2.66616374e-04\n",
      "Epoch: 4136 mean train loss:  2.60664161e-04, mean val. rec. loss:  2.70347697e-04\n",
      "Epoch: 4137 mean train loss:  2.59998432e-04, mean val. rec. loss:  2.65610581e-04\n",
      "Epoch: 4138 mean train loss:  2.60816192e-04, mean val. rec. loss:  2.72733118e-04\n",
      "Epoch: 4139 mean train loss:  2.61271352e-04, mean val. rec. loss:  2.67861420e-04\n",
      "Epoch: 4140 mean train loss:  2.60894668e-04, mean val. rec. loss:  2.69866553e-04\n",
      "Epoch: 4141 mean train loss:  2.60739423e-04, mean val. rec. loss:  2.65805242e-04\n",
      "Epoch: 4142 mean train loss:  2.59252056e-04, mean val. rec. loss:  2.67033206e-04\n",
      "Epoch: 4143 mean train loss:  2.58745664e-04, mean val. rec. loss:  2.66600509e-04\n",
      "Epoch: 4144 mean train loss:  2.58773792e-04, mean val. rec. loss:  2.67181128e-04\n",
      "Epoch: 4145 mean train loss:  2.60386682e-04, mean val. rec. loss:  2.66298961e-04\n",
      "Epoch: 4146 mean train loss:  2.60086034e-04, mean val. rec. loss:  2.66044932e-04\n",
      "Epoch: 4147 mean train loss:  2.58847788e-04, mean val. rec. loss:  2.67255797e-04\n",
      "Epoch: 4148 mean train loss:  2.58720226e-04, mean val. rec. loss:  2.65381358e-04\n",
      "Epoch: 4149 mean train loss:  2.59029894e-04, mean val. rec. loss:  2.68119755e-04\n",
      "Epoch: 4150 mean train loss:  2.58089513e-04, mean val. rec. loss:  2.65455591e-04\n",
      "Epoch: 4151 mean train loss:  2.58077308e-04, mean val. rec. loss:  2.68517035e-04\n",
      "Epoch: 4152 mean train loss:  2.58741662e-04, mean val. rec. loss:  2.64908900e-04\n",
      "Epoch: 4153 mean train loss:  2.57857888e-04, mean val. rec. loss:  2.64635918e-04\n",
      "Epoch: 4154 mean train loss:  2.57330427e-04, mean val. rec. loss:  2.65117317e-04\n",
      "Epoch: 4155 mean train loss:  2.57941293e-04, mean val. rec. loss:  2.63976124e-04\n",
      "Epoch: 4156 mean train loss:  2.56902807e-04, mean val. rec. loss:  2.64665048e-04\n",
      "Epoch: 4157 mean train loss:  2.57506942e-04, mean val. rec. loss:  2.63183346e-04\n",
      "Epoch: 4158 mean train loss:  2.56631921e-04, mean val. rec. loss:  2.64606352e-04\n",
      "Epoch: 4159 mean train loss:  2.56617331e-04, mean val. rec. loss:  2.64602191e-04\n",
      "Epoch: 4160 mean train loss:  2.56284013e-04, mean val. rec. loss:  2.64434680e-04\n",
      "Epoch: 4161 mean train loss:  2.55768094e-04, mean val. rec. loss:  2.64708153e-04\n",
      "Epoch: 4162 mean train loss:  2.56135600e-04, mean val. rec. loss:  2.62472942e-04\n",
      "Epoch: 4163 mean train loss:  2.55962767e-04, mean val. rec. loss:  2.63662673e-04\n",
      "Epoch: 4164 mean train loss:  2.55298640e-04, mean val. rec. loss:  2.62230380e-04\n",
      "Epoch: 4165 mean train loss:  2.55479991e-04, mean val. rec. loss:  2.66361691e-04\n",
      "Epoch: 4166 mean train loss:  2.54831346e-04, mean val. rec. loss:  2.62308302e-04\n",
      "Epoch: 4167 mean train loss:  2.56289272e-04, mean val. rec. loss:  2.61550597e-04\n",
      "Epoch: 4168 mean train loss:  2.55566533e-04, mean val. rec. loss:  2.62128180e-04\n",
      "Epoch: 4169 mean train loss:  2.55161360e-04, mean val. rec. loss:  2.64827580e-04\n",
      "Epoch: 4170 mean train loss:  2.54896857e-04, mean val. rec. loss:  2.61160822e-04\n",
      "Epoch: 4171 mean train loss:  2.54660706e-04, mean val. rec. loss:  2.62317061e-04\n",
      "Epoch: 4172 mean train loss:  2.54556230e-04, mean val. rec. loss:  2.60880353e-04\n",
      "Epoch: 4173 mean train loss:  2.54527742e-04, mean val. rec. loss:  2.61793031e-04\n",
      "Epoch: 4174 mean train loss:  2.54206081e-04, mean val. rec. loss:  2.61562263e-04\n",
      "Epoch: 4175 mean train loss:  2.54700092e-04, mean val. rec. loss:  2.60619401e-04\n",
      "Epoch: 4176 mean train loss:  2.53224331e-04, mean val. rec. loss:  2.61101635e-04\n",
      "Epoch: 4177 mean train loss:  2.53852034e-04, mean val. rec. loss:  2.59495999e-04\n",
      "Epoch: 4178 mean train loss:  2.53043307e-04, mean val. rec. loss:  2.62612213e-04\n",
      "Epoch: 4179 mean train loss:  2.54354170e-04, mean val. rec. loss:  2.59413825e-04\n",
      "Epoch: 4180 mean train loss:  2.53534937e-04, mean val. rec. loss:  2.63021123e-04\n",
      "Epoch: 4181 mean train loss:  2.53225273e-04, mean val. rec. loss:  2.57350141e-04\n",
      "Epoch: 4182 mean train loss:  2.52291039e-04, mean val. rec. loss:  2.61383449e-04\n",
      "Epoch: 4183 mean train loss:  2.52439863e-04, mean val. rec. loss:  2.58208193e-04\n",
      "Epoch: 4184 mean train loss:  2.52961941e-04, mean val. rec. loss:  2.62300834e-04\n",
      "Epoch: 4185 mean train loss:  2.51504908e-04, mean val. rec. loss:  2.56820823e-04\n",
      "Epoch: 4186 mean train loss:  2.51636357e-04, mean val. rec. loss:  2.61987964e-04\n",
      "Epoch: 4187 mean train loss:  2.51536498e-04, mean val. rec. loss:  2.59011330e-04\n",
      "Epoch: 4188 mean train loss:  2.52192490e-04, mean val. rec. loss:  2.61785599e-04\n",
      "Epoch: 4189 mean train loss:  2.52119164e-04, mean val. rec. loss:  2.57109505e-04\n",
      "Epoch: 4190 mean train loss:  2.50935077e-04, mean val. rec. loss:  2.61671659e-04\n",
      "Epoch: 4191 mean train loss:  2.50522948e-04, mean val. rec. loss:  2.55774071e-04\n",
      "Epoch: 4192 mean train loss:  2.51031239e-04, mean val. rec. loss:  2.59928914e-04\n",
      "Epoch: 4193 mean train loss:  2.50943893e-04, mean val. rec. loss:  2.57893270e-04\n",
      "Epoch: 4194 mean train loss:  2.50887590e-04, mean val. rec. loss:  2.58917252e-04\n",
      "Epoch: 4195 mean train loss:  2.50255014e-04, mean val. rec. loss:  2.56072457e-04\n",
      "Epoch: 4196 mean train loss:  2.50569530e-04, mean val. rec. loss:  2.59768218e-04\n",
      "Epoch: 4197 mean train loss:  2.51404880e-04, mean val. rec. loss:  2.58167797e-04\n",
      "Epoch: 4198 mean train loss:  2.49244062e-04, mean val. rec. loss:  2.58166579e-04\n",
      "Epoch: 4199 mean train loss:  2.49168787e-04, mean val. rec. loss:  2.55176025e-04\n",
      "Epoch: 4200 mean train loss:  2.48716903e-04, mean val. rec. loss:  2.59231940e-04\n",
      "Epoch: 4201 mean train loss:  2.49219005e-04, mean val. rec. loss:  2.56403626e-04\n",
      "Epoch: 4202 mean train loss:  2.49380469e-04, mean val. rec. loss:  2.56777500e-04\n",
      "Epoch: 4203 mean train loss:  2.47587659e-04, mean val. rec. loss:  2.54748035e-04\n",
      "Epoch: 4204 mean train loss:  2.49208569e-04, mean val. rec. loss:  2.61869227e-04\n",
      "Epoch: 4205 mean train loss:  2.48931820e-04, mean val. rec. loss:  2.55924681e-04\n",
      "Epoch: 4206 mean train loss:  2.49698097e-04, mean val. rec. loss:  2.61955072e-04\n",
      "Epoch: 4207 mean train loss:  2.49871131e-04, mean val. rec. loss:  2.56326649e-04\n",
      "Epoch: 4208 mean train loss:  2.48995182e-04, mean val. rec. loss:  2.56789894e-04\n",
      "Epoch: 4209 mean train loss:  2.48482364e-04, mean val. rec. loss:  2.57783347e-04\n",
      "Epoch: 4210 mean train loss:  2.47578398e-04, mean val. rec. loss:  2.54343814e-04\n",
      "Epoch: 4211 mean train loss:  2.47018100e-04, mean val. rec. loss:  2.54051569e-04\n",
      "Epoch: 4212 mean train loss:  2.46864391e-04, mean val. rec. loss:  2.54825194e-04\n",
      "Epoch: 4213 mean train loss:  2.46332856e-04, mean val. rec. loss:  2.55004008e-04\n",
      "Epoch: 4214 mean train loss:  2.46655999e-04, mean val. rec. loss:  2.52876558e-04\n",
      "Epoch: 4215 mean train loss:  2.45491705e-04, mean val. rec. loss:  2.55335304e-04\n",
      "Epoch: 4216 mean train loss:  2.47112479e-04, mean val. rec. loss:  2.53281488e-04\n",
      "Epoch: 4217 mean train loss:  2.46854432e-04, mean val. rec. loss:  2.55745867e-04\n",
      "Epoch: 4218 mean train loss:  2.46408614e-04, mean val. rec. loss:  2.52709538e-04\n",
      "Epoch: 4219 mean train loss:  2.46347939e-04, mean val. rec. loss:  2.56745154e-04\n",
      "Epoch: 4220 mean train loss:  2.47084359e-04, mean val. rec. loss:  2.51021654e-04\n",
      "Epoch: 4221 mean train loss:  2.46468472e-04, mean val. rec. loss:  2.54756812e-04\n",
      "Epoch: 4222 mean train loss:  2.46599121e-04, mean val. rec. loss:  2.52999238e-04\n",
      "Epoch: 4223 mean train loss:  2.45021906e-04, mean val. rec. loss:  2.51377519e-04\n",
      "Epoch: 4224 mean train loss:  2.45064936e-04, mean val. rec. loss:  2.53290247e-04\n",
      "Epoch: 4225 mean train loss:  2.44726683e-04, mean val. rec. loss:  2.51157200e-04\n",
      "Epoch: 4226 mean train loss:  2.44660170e-04, mean val. rec. loss:  2.54536202e-04\n",
      "Epoch: 4227 mean train loss:  2.44339146e-04, mean val. rec. loss:  2.51967151e-04\n",
      "Epoch: 4228 mean train loss:  2.44387088e-04, mean val. rec. loss:  2.53413254e-04\n",
      "Epoch: 4229 mean train loss:  2.45383976e-04, mean val. rec. loss:  2.51491168e-04\n",
      "Epoch: 4230 mean train loss:  2.45459739e-04, mean val. rec. loss:  2.53589669e-04\n",
      "Epoch: 4231 mean train loss:  2.44051986e-04, mean val. rec. loss:  2.50438801e-04\n",
      "Epoch: 4232 mean train loss:  2.44155274e-04, mean val. rec. loss:  2.50718833e-04\n",
      "Epoch: 4233 mean train loss:  2.44479537e-04, mean val. rec. loss:  2.51551863e-04\n",
      "Epoch: 4234 mean train loss:  2.43661533e-04, mean val. rec. loss:  2.48828785e-04\n",
      "Epoch: 4235 mean train loss:  2.42991797e-04, mean val. rec. loss:  2.51661059e-04\n",
      "Epoch: 4236 mean train loss:  2.42998087e-04, mean val. rec. loss:  2.50590211e-04\n",
      "Epoch: 4237 mean train loss:  2.42549240e-04, mean val. rec. loss:  2.48728311e-04\n",
      "Epoch: 4238 mean train loss:  2.43594407e-04, mean val. rec. loss:  2.50091785e-04\n",
      "Epoch: 4239 mean train loss:  2.43212650e-04, mean val. rec. loss:  2.49383944e-04\n",
      "Epoch: 4240 mean train loss:  2.41895738e-04, mean val. rec. loss:  2.51285695e-04\n",
      "Epoch: 4241 mean train loss:  2.42064996e-04, mean val. rec. loss:  2.49510186e-04\n",
      "Epoch: 4242 mean train loss:  2.41675742e-04, mean val. rec. loss:  2.47529803e-04\n",
      "Epoch: 4243 mean train loss:  2.42612646e-04, mean val. rec. loss:  2.52380168e-04\n",
      "Epoch: 4244 mean train loss:  2.42850811e-04, mean val. rec. loss:  2.47858047e-04\n",
      "Epoch: 4245 mean train loss:  2.42727530e-04, mean val. rec. loss:  2.49180361e-04\n",
      "Epoch: 4246 mean train loss:  2.42057747e-04, mean val. rec. loss:  2.49988168e-04\n",
      "Epoch: 4247 mean train loss:  2.42298808e-04, mean val. rec. loss:  2.46290390e-04\n",
      "Epoch: 4248 mean train loss:  2.41297552e-04, mean val. rec. loss:  2.49429592e-04\n",
      "Epoch: 4249 mean train loss:  2.40210068e-04, mean val. rec. loss:  2.48056796e-04\n",
      "Epoch: 4250 mean train loss:  2.40810613e-04, mean val. rec. loss:  2.49489524e-04\n",
      "Epoch: 4251 mean train loss:  2.41739648e-04, mean val. rec. loss:  2.46122952e-04\n",
      "Epoch: 4252 mean train loss:  2.41348217e-04, mean val. rec. loss:  2.53282560e-04\n",
      "Epoch: 4253 mean train loss:  2.41863160e-04, mean val. rec. loss:  2.47921795e-04\n",
      "Epoch: 4254 mean train loss:  2.41409215e-04, mean val. rec. loss:  2.49712860e-04\n",
      "Epoch: 4255 mean train loss:  2.40045945e-04, mean val. rec. loss:  2.47252605e-04\n",
      "Epoch: 4256 mean train loss:  2.39783719e-04, mean val. rec. loss:  2.51263344e-04\n",
      "Epoch: 4257 mean train loss:  2.40576250e-04, mean val. rec. loss:  2.46089824e-04\n",
      "Epoch: 4258 mean train loss:  2.40519556e-04, mean val. rec. loss:  2.47895936e-04\n",
      "Epoch: 4259 mean train loss:  2.38527296e-04, mean val. rec. loss:  2.44173208e-04\n",
      "Epoch: 4260 mean train loss:  2.39250936e-04, mean val. rec. loss:  2.50261241e-04\n",
      "Epoch: 4261 mean train loss:  2.38638505e-04, mean val. rec. loss:  2.45816915e-04\n",
      "Epoch: 4262 mean train loss:  2.38587964e-04, mean val. rec. loss:  2.46766918e-04\n",
      "Epoch: 4263 mean train loss:  2.38247930e-04, mean val. rec. loss:  2.44523658e-04\n",
      "Epoch: 4264 mean train loss:  2.38557443e-04, mean val. rec. loss:  2.45195300e-04\n",
      "Epoch: 4265 mean train loss:  2.39137315e-04, mean val. rec. loss:  2.46923762e-04\n",
      "Epoch: 4266 mean train loss:  2.39077623e-04, mean val. rec. loss:  2.42756525e-04\n",
      "Epoch: 4267 mean train loss:  2.38518734e-04, mean val. rec. loss:  2.44712648e-04\n",
      "Epoch: 4268 mean train loss:  2.37015825e-04, mean val. rec. loss:  2.46977842e-04\n",
      "Epoch: 4269 mean train loss:  2.37896602e-04, mean val. rec. loss:  2.44962315e-04\n",
      "Epoch: 4270 mean train loss:  2.37914587e-04, mean val. rec. loss:  2.44732819e-04\n",
      "Epoch: 4271 mean train loss:  2.37324534e-04, mean val. rec. loss:  2.45652584e-04\n",
      "Epoch: 4272 mean train loss:  2.37667674e-04, mean val. rec. loss:  2.43365946e-04\n",
      "Epoch: 4273 mean train loss:  2.36169692e-04, mean val. rec. loss:  2.45103749e-04\n",
      "Epoch: 4274 mean train loss:  2.36797716e-04, mean val. rec. loss:  2.43674600e-04\n",
      "Epoch: 4275 mean train loss:  2.36365569e-04, mean val. rec. loss:  2.44209879e-04\n",
      "Epoch: 4276 mean train loss:  2.36734486e-04, mean val. rec. loss:  2.43404471e-04\n",
      "Epoch: 4277 mean train loss:  2.35699624e-04, mean val. rec. loss:  2.44301012e-04\n",
      "Epoch: 4278 mean train loss:  2.36365351e-04, mean val. rec. loss:  2.43613996e-04\n",
      "Epoch: 4279 mean train loss:  2.36226037e-04, mean val. rec. loss:  2.42383851e-04\n",
      "Epoch: 4280 mean train loss:  2.35670045e-04, mean val. rec. loss:  2.44663238e-04\n",
      "Epoch: 4281 mean train loss:  2.35640993e-04, mean val. rec. loss:  2.42825598e-04\n",
      "Epoch: 4282 mean train loss:  2.37162433e-04, mean val. rec. loss:  2.48817827e-04\n",
      "Epoch: 4283 mean train loss:  2.37748537e-04, mean val. rec. loss:  2.44228960e-04\n",
      "Epoch: 4284 mean train loss:  2.36800273e-04, mean val. rec. loss:  2.42602534e-04\n",
      "Epoch: 4285 mean train loss:  2.35514540e-04, mean val. rec. loss:  2.43364038e-04\n",
      "Epoch: 4286 mean train loss:  2.34808639e-04, mean val. rec. loss:  2.43462295e-04\n",
      "Epoch: 4287 mean train loss:  2.35402961e-04, mean val. rec. loss:  2.41356706e-04\n",
      "Epoch: 4288 mean train loss:  2.34167343e-04, mean val. rec. loss:  2.41456126e-04\n",
      "Epoch: 4289 mean train loss:  2.34806426e-04, mean val. rec. loss:  2.44758006e-04\n",
      "Epoch: 4290 mean train loss:  2.35486838e-04, mean val. rec. loss:  2.38711260e-04\n",
      "Epoch: 4291 mean train loss:  2.33393658e-04, mean val. rec. loss:  2.43545215e-04\n",
      "Epoch: 4292 mean train loss:  2.34334589e-04, mean val. rec. loss:  2.42818329e-04\n",
      "Epoch: 4293 mean train loss:  2.34894166e-04, mean val. rec. loss:  2.47459895e-04\n",
      "Epoch: 4294 mean train loss:  2.35673978e-04, mean val. rec. loss:  2.40218385e-04\n",
      "Epoch: 4295 mean train loss:  2.35341882e-04, mean val. rec. loss:  2.44055216e-04\n",
      "Epoch: 4296 mean train loss:  2.34829012e-04, mean val. rec. loss:  2.38551436e-04\n",
      "Epoch: 4297 mean train loss:  2.34223397e-04, mean val. rec. loss:  2.41036931e-04\n",
      "Epoch: 4298 mean train loss:  2.33845146e-04, mean val. rec. loss:  2.41986081e-04\n",
      "Epoch: 4299 mean train loss:  2.33784236e-04, mean val. rec. loss:  2.40126761e-04\n",
      "Epoch: 4300 mean train loss:  2.33574492e-04, mean val. rec. loss:  2.40166467e-04\n",
      "Epoch: 4301 mean train loss:  2.33962861e-04, mean val. rec. loss:  2.43512050e-04\n",
      "Epoch: 4302 mean train loss:  2.33779747e-04, mean val. rec. loss:  2.37406681e-04\n",
      "Epoch: 4303 mean train loss:  2.32513024e-04, mean val. rec. loss:  2.42203874e-04\n",
      "Epoch: 4304 mean train loss:  2.32777639e-04, mean val. rec. loss:  2.40129251e-04\n",
      "Epoch: 4305 mean train loss:  2.32673160e-04, mean val. rec. loss:  2.39147264e-04\n",
      "Epoch: 4306 mean train loss:  2.32190151e-04, mean val. rec. loss:  2.38492213e-04\n",
      "Epoch: 4307 mean train loss:  2.31656561e-04, mean val. rec. loss:  2.39596243e-04\n",
      "Epoch: 4308 mean train loss:  2.30939685e-04, mean val. rec. loss:  2.37753042e-04\n",
      "Epoch: 4309 mean train loss:  2.30438452e-04, mean val. rec. loss:  2.37425780e-04\n",
      "Epoch: 4310 mean train loss:  2.30508041e-04, mean val. rec. loss:  2.38433607e-04\n",
      "Epoch: 4311 mean train loss:  2.30622918e-04, mean val. rec. loss:  2.37895203e-04\n",
      "Epoch: 4312 mean train loss:  2.30971530e-04, mean val. rec. loss:  2.36906765e-04\n",
      "Epoch: 4313 mean train loss:  2.30461784e-04, mean val. rec. loss:  2.39966246e-04\n",
      "Epoch: 4314 mean train loss:  2.31513477e-04, mean val. rec. loss:  2.37194721e-04\n",
      "Epoch: 4315 mean train loss:  2.31559931e-04, mean val. rec. loss:  2.38203657e-04\n",
      "Epoch: 4316 mean train loss:  2.30872426e-04, mean val. rec. loss:  2.34864162e-04\n",
      "Epoch: 4317 mean train loss:  2.32564835e-04, mean val. rec. loss:  2.44555386e-04\n",
      "Epoch: 4318 mean train loss:  2.32616870e-04, mean val. rec. loss:  2.37240224e-04\n",
      "Epoch: 4319 mean train loss:  2.32692328e-04, mean val. rec. loss:  2.41888732e-04\n",
      "Epoch: 4320 mean train loss:  2.31455925e-04, mean val. rec. loss:  2.36159163e-04\n",
      "Epoch: 4321 mean train loss:  2.29579532e-04, mean val. rec. loss:  2.38606988e-04\n",
      "Epoch: 4322 mean train loss:  2.29630666e-04, mean val. rec. loss:  2.35880457e-04\n",
      "Epoch: 4323 mean train loss:  2.29042080e-04, mean val. rec. loss:  2.36897770e-04\n",
      "Epoch: 4324 mean train loss:  2.29084539e-04, mean val. rec. loss:  2.36418807e-04\n",
      "Epoch: 4325 mean train loss:  2.28687565e-04, mean val. rec. loss:  2.40913070e-04\n",
      "Epoch: 4326 mean train loss:  2.30919665e-04, mean val. rec. loss:  2.35358607e-04\n",
      "Epoch: 4327 mean train loss:  2.30257916e-04, mean val. rec. loss:  2.38964125e-04\n",
      "Epoch: 4328 mean train loss:  2.28546353e-04, mean val. rec. loss:  2.34100750e-04\n",
      "Epoch: 4329 mean train loss:  2.27934405e-04, mean val. rec. loss:  2.34370406e-04\n",
      "Epoch: 4330 mean train loss:  2.27882792e-04, mean val. rec. loss:  2.36123691e-04\n",
      "Epoch: 4331 mean train loss:  2.28180346e-04, mean val. rec. loss:  2.34659852e-04\n",
      "Epoch: 4332 mean train loss:  2.28555668e-04, mean val. rec. loss:  2.33147202e-04\n",
      "Epoch: 4333 mean train loss:  2.27905041e-04, mean val. rec. loss:  2.35028474e-04\n",
      "Epoch: 4334 mean train loss:  2.27451939e-04, mean val. rec. loss:  2.33692730e-04\n",
      "Epoch: 4335 mean train loss:  2.27479307e-04, mean val. rec. loss:  2.34822784e-04\n",
      "Epoch: 4336 mean train loss:  2.26847896e-04, mean val. rec. loss:  2.33885264e-04\n",
      "Epoch: 4337 mean train loss:  2.26898022e-04, mean val. rec. loss:  2.33303665e-04\n",
      "Epoch: 4338 mean train loss:  2.26756173e-04, mean val. rec. loss:  2.34788365e-04\n",
      "Epoch: 4339 mean train loss:  2.26548639e-04, mean val. rec. loss:  2.30743936e-04\n",
      "Epoch: 4340 mean train loss:  2.26159718e-04, mean val. rec. loss:  2.33694875e-04\n",
      "Epoch: 4341 mean train loss:  2.26137107e-04, mean val. rec. loss:  2.35983566e-04\n",
      "Epoch: 4342 mean train loss:  2.26340677e-04, mean val. rec. loss:  2.31832429e-04\n",
      "Epoch: 4343 mean train loss:  2.26920923e-04, mean val. rec. loss:  2.35590411e-04\n",
      "Epoch: 4344 mean train loss:  2.26447310e-04, mean val. rec. loss:  2.31779785e-04\n",
      "Epoch: 4345 mean train loss:  2.26880056e-04, mean val. rec. loss:  2.31363188e-04\n",
      "Epoch: 4346 mean train loss:  2.26776599e-04, mean val. rec. loss:  2.36532274e-04\n",
      "Epoch: 4347 mean train loss:  2.25898512e-04, mean val. rec. loss:  2.31117119e-04\n",
      "Epoch: 4348 mean train loss:  2.25498081e-04, mean val. rec. loss:  2.32405979e-04\n",
      "Epoch: 4349 mean train loss:  2.24837469e-04, mean val. rec. loss:  2.31884856e-04\n",
      "Epoch: 4350 mean train loss:  2.25598104e-04, mean val. rec. loss:  2.33883465e-04\n",
      "Epoch: 4351 mean train loss:  2.25716162e-04, mean val. rec. loss:  2.30403662e-04\n",
      "Epoch: 4352 mean train loss:  2.24929135e-04, mean val. rec. loss:  2.34156156e-04\n",
      "Epoch: 4353 mean train loss:  2.24943999e-04, mean val. rec. loss:  2.30499720e-04\n",
      "Epoch: 4354 mean train loss:  2.24533913e-04, mean val. rec. loss:  2.28776619e-04\n",
      "Epoch: 4355 mean train loss:  2.23866467e-04, mean val. rec. loss:  2.32988669e-04\n",
      "Epoch: 4356 mean train loss:  2.24331088e-04, mean val. rec. loss:  2.30529068e-04\n",
      "Epoch: 4357 mean train loss:  2.23528943e-04, mean val. rec. loss:  2.29845014e-04\n",
      "Epoch: 4358 mean train loss:  2.24760603e-04, mean val. rec. loss:  2.33160068e-04\n",
      "Epoch: 4359 mean train loss:  2.23578094e-04, mean val. rec. loss:  2.29312080e-04\n",
      "Epoch: 4360 mean train loss:  2.23276932e-04, mean val. rec. loss:  2.31137454e-04\n",
      "Epoch: 4361 mean train loss:  2.22943471e-04, mean val. rec. loss:  2.28819887e-04\n",
      "Epoch: 4362 mean train loss:  2.24085961e-04, mean val. rec. loss:  2.30068114e-04\n",
      "Epoch: 4363 mean train loss:  2.24685903e-04, mean val. rec. loss:  2.36549356e-04\n",
      "Epoch: 4364 mean train loss:  2.26891123e-04, mean val. rec. loss:  2.27598773e-04\n",
      "Epoch: 4365 mean train loss:  2.24107176e-04, mean val. rec. loss:  2.34626633e-04\n",
      "Epoch: 4366 mean train loss:  2.23938911e-04, mean val. rec. loss:  2.29512228e-04\n",
      "Epoch: 4367 mean train loss:  2.23705495e-04, mean val. rec. loss:  2.34452708e-04\n",
      "Epoch: 4368 mean train loss:  2.24025074e-04, mean val. rec. loss:  2.28939115e-04\n",
      "Epoch: 4369 mean train loss:  2.24745127e-04, mean val. rec. loss:  2.35396569e-04\n",
      "Epoch: 4370 mean train loss:  2.23443053e-04, mean val. rec. loss:  2.27236639e-04\n",
      "Epoch: 4371 mean train loss:  2.23378838e-04, mean val. rec. loss:  2.29551879e-04\n",
      "Epoch: 4372 mean train loss:  2.21485917e-04, mean val. rec. loss:  2.29716737e-04\n",
      "Epoch: 4373 mean train loss:  2.21270618e-04, mean val. rec. loss:  2.27391774e-04\n",
      "Epoch: 4374 mean train loss:  2.20807505e-04, mean val. rec. loss:  2.26010927e-04\n",
      "Epoch: 4375 mean train loss:  2.21157842e-04, mean val. rec. loss:  2.28891976e-04\n",
      "Epoch: 4376 mean train loss:  2.21135253e-04, mean val. rec. loss:  2.27960108e-04\n",
      "Epoch: 4377 mean train loss:  2.21937036e-04, mean val. rec. loss:  2.31651744e-04\n",
      "Epoch: 4378 mean train loss:  2.21580551e-04, mean val. rec. loss:  2.26366011e-04\n",
      "Epoch: 4379 mean train loss:  2.20294941e-04, mean val. rec. loss:  2.29648192e-04\n",
      "Epoch: 4380 mean train loss:  2.20583332e-04, mean val. rec. loss:  2.27381580e-04\n",
      "Epoch: 4381 mean train loss:  2.20211318e-04, mean val. rec. loss:  2.27002563e-04\n",
      "Epoch: 4382 mean train loss:  2.20747443e-04, mean val. rec. loss:  2.28605074e-04\n",
      "Epoch: 4383 mean train loss:  2.22695306e-04, mean val. rec. loss:  2.25956193e-04\n",
      "Epoch: 4384 mean train loss:  2.21651569e-04, mean val. rec. loss:  2.33304682e-04\n",
      "Epoch: 4385 mean train loss:  2.22072310e-04, mean val. rec. loss:  2.26883154e-04\n",
      "Epoch: 4386 mean train loss:  2.20013748e-04, mean val. rec. loss:  2.27781640e-04\n",
      "Epoch: 4387 mean train loss:  2.20082643e-04, mean val. rec. loss:  2.24891087e-04\n",
      "Epoch: 4388 mean train loss:  2.20309293e-04, mean val. rec. loss:  2.31381197e-04\n",
      "Epoch: 4389 mean train loss:  2.19553996e-04, mean val. rec. loss:  2.22995714e-04\n",
      "Epoch: 4390 mean train loss:  2.18920173e-04, mean val. rec. loss:  2.27961380e-04\n",
      "Epoch: 4391 mean train loss:  2.19707913e-04, mean val. rec. loss:  2.26878193e-04\n",
      "Epoch: 4392 mean train loss:  2.18342618e-04, mean val. rec. loss:  2.26441280e-04\n",
      "Epoch: 4393 mean train loss:  2.20041438e-04, mean val. rec. loss:  2.26718496e-04\n",
      "Epoch: 4394 mean train loss:  2.18845040e-04, mean val. rec. loss:  2.27382089e-04\n",
      "Epoch: 4395 mean train loss:  2.18724792e-04, mean val. rec. loss:  2.26628417e-04\n",
      "Epoch: 4396 mean train loss:  2.19425346e-04, mean val. rec. loss:  2.27671353e-04\n",
      "Epoch: 4397 mean train loss:  2.17587316e-04, mean val. rec. loss:  2.23620218e-04\n",
      "Epoch: 4398 mean train loss:  2.17622765e-04, mean val. rec. loss:  2.24989689e-04\n",
      "Epoch: 4399 mean train loss:  2.17426193e-04, mean val. rec. loss:  2.25119147e-04\n",
      "Epoch: 4400 mean train loss:  2.17782570e-04, mean val. rec. loss:  2.23260355e-04\n",
      "Epoch: 4401 mean train loss:  2.17348087e-04, mean val. rec. loss:  2.25508086e-04\n",
      "Epoch: 4402 mean train loss:  2.17318701e-04, mean val. rec. loss:  2.25300560e-04\n",
      "Epoch: 4403 mean train loss:  2.17467424e-04, mean val. rec. loss:  2.27777878e-04\n",
      "Epoch: 4404 mean train loss:  2.18621540e-04, mean val. rec. loss:  2.23603281e-04\n",
      "Epoch: 4405 mean train loss:  2.19694107e-04, mean val. rec. loss:  2.29275009e-04\n",
      "Epoch: 4406 mean train loss:  2.17579700e-04, mean val. rec. loss:  2.21646032e-04\n",
      "Epoch: 4407 mean train loss:  2.16473290e-04, mean val. rec. loss:  2.23989094e-04\n",
      "Epoch: 4408 mean train loss:  2.16202769e-04, mean val. rec. loss:  2.22595363e-04\n",
      "Epoch: 4409 mean train loss:  2.16230979e-04, mean val. rec. loss:  2.24988490e-04\n",
      "Epoch: 4410 mean train loss:  2.15496457e-04, mean val. rec. loss:  2.22961096e-04\n",
      "Epoch: 4411 mean train loss:  2.16220761e-04, mean val. rec. loss:  2.24195057e-04\n",
      "Epoch: 4412 mean train loss:  2.16629806e-04, mean val. rec. loss:  2.24062183e-04\n",
      "Epoch: 4413 mean train loss:  2.18031528e-04, mean val. rec. loss:  2.23871847e-04\n",
      "Epoch: 4414 mean train loss:  2.17247858e-04, mean val. rec. loss:  2.21752648e-04\n",
      "Epoch: 4415 mean train loss:  2.15302137e-04, mean val. rec. loss:  2.23610459e-04\n",
      "Epoch: 4416 mean train loss:  2.15563214e-04, mean val. rec. loss:  2.20758177e-04\n",
      "Epoch: 4417 mean train loss:  2.16042092e-04, mean val. rec. loss:  2.25607851e-04\n",
      "Epoch: 4418 mean train loss:  2.14795798e-04, mean val. rec. loss:  2.20333767e-04\n",
      "Epoch: 4419 mean train loss:  2.14895902e-04, mean val. rec. loss:  2.20388428e-04\n",
      "Epoch: 4420 mean train loss:  2.15322380e-04, mean val. rec. loss:  2.21149296e-04\n",
      "Epoch: 4421 mean train loss:  2.15035546e-04, mean val. rec. loss:  2.24480469e-04\n",
      "Epoch: 4422 mean train loss:  2.14536716e-04, mean val. rec. loss:  2.19468554e-04\n",
      "Epoch: 4423 mean train loss:  2.14246986e-04, mean val. rec. loss:  2.21938513e-04\n",
      "Epoch: 4424 mean train loss:  2.14477221e-04, mean val. rec. loss:  2.20363587e-04\n",
      "Epoch: 4425 mean train loss:  2.14262923e-04, mean val. rec. loss:  2.21383644e-04\n",
      "Epoch: 4426 mean train loss:  2.14220641e-04, mean val. rec. loss:  2.19739174e-04\n",
      "Epoch: 4427 mean train loss:  2.13891877e-04, mean val. rec. loss:  2.20397260e-04\n",
      "Epoch: 4428 mean train loss:  2.13371426e-04, mean val. rec. loss:  2.19846753e-04\n",
      "Epoch: 4429 mean train loss:  2.13434424e-04, mean val. rec. loss:  2.21561477e-04\n",
      "Epoch: 4430 mean train loss:  2.12973415e-04, mean val. rec. loss:  2.18984339e-04\n",
      "Epoch: 4431 mean train loss:  2.12339755e-04, mean val. rec. loss:  2.18486132e-04\n",
      "Epoch: 4432 mean train loss:  2.12837022e-04, mean val. rec. loss:  2.20517233e-04\n",
      "Epoch: 4433 mean train loss:  2.12178619e-04, mean val. rec. loss:  2.20234329e-04\n",
      "Epoch: 4434 mean train loss:  2.12571198e-04, mean val. rec. loss:  2.19360975e-04\n",
      "Epoch: 4435 mean train loss:  2.13790801e-04, mean val. rec. loss:  2.21455497e-04\n",
      "Epoch: 4436 mean train loss:  2.13167651e-04, mean val. rec. loss:  2.22067226e-04\n",
      "Epoch: 4437 mean train loss:  2.13870747e-04, mean val. rec. loss:  2.19587909e-04\n",
      "Epoch: 4438 mean train loss:  2.13029345e-04, mean val. rec. loss:  2.20325262e-04\n",
      "Epoch: 4439 mean train loss:  2.13052620e-04, mean val. rec. loss:  2.17467674e-04\n",
      "Epoch: 4440 mean train loss:  2.11379660e-04, mean val. rec. loss:  2.18728039e-04\n",
      "Epoch: 4441 mean train loss:  2.12228485e-04, mean val. rec. loss:  2.20863648e-04\n",
      "Epoch: 4442 mean train loss:  2.12273490e-04, mean val. rec. loss:  2.16307328e-04\n",
      "Epoch: 4443 mean train loss:  2.11775255e-04, mean val. rec. loss:  2.20524720e-04\n",
      "Epoch: 4444 mean train loss:  2.10770639e-04, mean val. rec. loss:  2.16974336e-04\n",
      "Epoch: 4445 mean train loss:  2.11750683e-04, mean val. rec. loss:  2.16058424e-04\n",
      "Epoch: 4446 mean train loss:  2.10617778e-04, mean val. rec. loss:  2.19066623e-04\n",
      "Epoch: 4447 mean train loss:  2.11843084e-04, mean val. rec. loss:  2.17625971e-04\n",
      "Epoch: 4448 mean train loss:  2.10515077e-04, mean val. rec. loss:  2.16113322e-04\n",
      "Epoch: 4449 mean train loss:  2.10862684e-04, mean val. rec. loss:  2.18293125e-04\n",
      "Epoch: 4450 mean train loss:  2.10088099e-04, mean val. rec. loss:  2.17264982e-04\n",
      "Epoch: 4451 mean train loss:  2.09834105e-04, mean val. rec. loss:  2.17606491e-04\n",
      "Epoch: 4452 mean train loss:  2.10254140e-04, mean val. rec. loss:  2.15911938e-04\n",
      "Epoch: 4453 mean train loss:  2.10946471e-04, mean val. rec. loss:  2.18294961e-04\n",
      "Epoch: 4454 mean train loss:  2.09787621e-04, mean val. rec. loss:  2.14937693e-04\n",
      "Epoch: 4455 mean train loss:  2.09910408e-04, mean val. rec. loss:  2.16541894e-04\n",
      "Epoch: 4456 mean train loss:  2.09487112e-04, mean val. rec. loss:  2.15691255e-04\n",
      "Epoch: 4457 mean train loss:  2.09967975e-04, mean val. rec. loss:  2.18579246e-04\n",
      "Epoch: 4458 mean train loss:  2.09269706e-04, mean val. rec. loss:  2.14875181e-04\n",
      "Epoch: 4459 mean train loss:  2.09031417e-04, mean val. rec. loss:  2.16181395e-04\n",
      "Epoch: 4460 mean train loss:  2.08538563e-04, mean val. rec. loss:  2.14823099e-04\n",
      "Epoch: 4461 mean train loss:  2.09730809e-04, mean val. rec. loss:  2.16330170e-04\n",
      "Epoch: 4462 mean train loss:  2.08481398e-04, mean val. rec. loss:  2.14195833e-04\n",
      "Epoch: 4463 mean train loss:  2.09012250e-04, mean val. rec. loss:  2.15751696e-04\n",
      "Epoch: 4464 mean train loss:  2.08656418e-04, mean val. rec. loss:  2.18506775e-04\n",
      "Epoch: 4465 mean train loss:  2.08920786e-04, mean val. rec. loss:  2.11952845e-04\n",
      "Epoch: 4466 mean train loss:  2.08832628e-04, mean val. rec. loss:  2.16979334e-04\n",
      "Epoch: 4467 mean train loss:  2.08917853e-04, mean val. rec. loss:  2.13298002e-04\n",
      "Epoch: 4468 mean train loss:  2.07703153e-04, mean val. rec. loss:  2.15682296e-04\n",
      "Epoch: 4469 mean train loss:  2.08048907e-04, mean val. rec. loss:  2.17373815e-04\n",
      "Epoch: 4470 mean train loss:  2.07956110e-04, mean val. rec. loss:  2.14539995e-04\n",
      "Epoch: 4471 mean train loss:  2.08544371e-04, mean val. rec. loss:  2.17232035e-04\n",
      "Epoch: 4472 mean train loss:  2.08645113e-04, mean val. rec. loss:  2.14061650e-04\n",
      "Epoch: 4473 mean train loss:  2.07090198e-04, mean val. rec. loss:  2.17497531e-04\n",
      "Epoch: 4474 mean train loss:  2.07671800e-04, mean val. rec. loss:  2.12556796e-04\n",
      "Epoch: 4475 mean train loss:  2.06827435e-04, mean val. rec. loss:  2.15797453e-04\n",
      "Epoch: 4476 mean train loss:  2.07016787e-04, mean val. rec. loss:  2.12509621e-04\n",
      "Epoch: 4477 mean train loss:  2.08093734e-04, mean val. rec. loss:  2.16205963e-04\n",
      "Epoch: 4478 mean train loss:  2.07389085e-04, mean val. rec. loss:  2.12583000e-04\n",
      "Epoch: 4479 mean train loss:  2.06486552e-04, mean val. rec. loss:  2.12140490e-04\n",
      "Epoch: 4480 mean train loss:  2.06964012e-04, mean val. rec. loss:  2.14274700e-04\n",
      "Epoch: 4481 mean train loss:  2.06180389e-04, mean val. rec. loss:  2.12569299e-04\n",
      "Epoch: 4482 mean train loss:  2.07621761e-04, mean val. rec. loss:  2.15846227e-04\n",
      "Epoch: 4483 mean train loss:  2.06339077e-04, mean val. rec. loss:  2.10513011e-04\n",
      "Epoch: 4484 mean train loss:  2.06172761e-04, mean val. rec. loss:  2.16451923e-04\n",
      "Epoch: 4485 mean train loss:  2.06582303e-04, mean val. rec. loss:  2.11000570e-04\n",
      "Epoch: 4486 mean train loss:  2.05678080e-04, mean val. rec. loss:  2.14015075e-04\n",
      "Epoch: 4487 mean train loss:  2.05181449e-04, mean val. rec. loss:  2.10753229e-04\n",
      "Epoch: 4488 mean train loss:  2.05763508e-04, mean val. rec. loss:  2.12672753e-04\n",
      "Epoch: 4489 mean train loss:  2.05593558e-04, mean val. rec. loss:  2.10956030e-04\n",
      "Epoch: 4490 mean train loss:  2.06532992e-04, mean val. rec. loss:  2.14850466e-04\n",
      "Epoch: 4491 mean train loss:  2.05939662e-04, mean val. rec. loss:  2.09394152e-04\n",
      "Epoch: 4492 mean train loss:  2.05190055e-04, mean val. rec. loss:  2.12732975e-04\n",
      "Epoch: 4493 mean train loss:  2.05127309e-04, mean val. rec. loss:  2.11091449e-04\n",
      "Epoch: 4494 mean train loss:  2.04107211e-04, mean val. rec. loss:  2.11718569e-04\n",
      "Epoch: 4495 mean train loss:  2.05127676e-04, mean val. rec. loss:  2.09607947e-04\n",
      "Epoch: 4496 mean train loss:  2.04509672e-04, mean val. rec. loss:  2.11727983e-04\n",
      "Epoch: 4497 mean train loss:  2.04106136e-04, mean val. rec. loss:  2.12013213e-04\n",
      "Epoch: 4498 mean train loss:  2.03738771e-04, mean val. rec. loss:  2.09000961e-04\n",
      "Epoch: 4499 mean train loss:  2.03411376e-04, mean val. rec. loss:  2.12610531e-04\n",
      "Epoch: 4500 mean train loss:  2.03621959e-04, mean val. rec. loss:  2.10703146e-04\n",
      "Epoch: 4501 mean train loss:  2.04420504e-04, mean val. rec. loss:  2.10228054e-04\n",
      "Epoch: 4502 mean train loss:  2.03624238e-04, mean val. rec. loss:  2.09630335e-04\n",
      "Epoch: 4503 mean train loss:  2.03003352e-04, mean val. rec. loss:  2.14327835e-04\n",
      "Epoch: 4504 mean train loss:  2.03279573e-04, mean val. rec. loss:  2.07836090e-04\n",
      "Epoch: 4505 mean train loss:  2.03776304e-04, mean val. rec. loss:  2.11985191e-04\n",
      "Epoch: 4506 mean train loss:  2.03573833e-04, mean val. rec. loss:  2.10778924e-04\n",
      "Epoch: 4507 mean train loss:  2.03819721e-04, mean val. rec. loss:  2.11841740e-04\n",
      "Epoch: 4508 mean train loss:  2.03242826e-04, mean val. rec. loss:  2.09458263e-04\n",
      "Epoch: 4509 mean train loss:  2.03371407e-04, mean val. rec. loss:  2.12174472e-04\n",
      "Epoch: 4510 mean train loss:  2.02589233e-04, mean val. rec. loss:  2.07060357e-04\n",
      "Epoch: 4511 mean train loss:  2.02600085e-04, mean val. rec. loss:  2.09214248e-04\n",
      "Epoch: 4512 mean train loss:  2.01681638e-04, mean val. rec. loss:  2.07756133e-04\n",
      "Epoch: 4513 mean train loss:  2.01734735e-04, mean val. rec. loss:  2.08023227e-04\n",
      "Epoch: 4514 mean train loss:  2.02219146e-04, mean val. rec. loss:  2.08620818e-04\n",
      "Epoch: 4515 mean train loss:  2.01717219e-04, mean val. rec. loss:  2.07226069e-04\n",
      "Epoch: 4516 mean train loss:  2.02236034e-04, mean val. rec. loss:  2.09185590e-04\n",
      "Epoch: 4517 mean train loss:  2.01410485e-04, mean val. rec. loss:  2.06353461e-04\n",
      "Epoch: 4518 mean train loss:  2.00734388e-04, mean val. rec. loss:  2.07422456e-04\n",
      "Epoch: 4519 mean train loss:  2.01346932e-04, mean val. rec. loss:  2.07905817e-04\n",
      "Epoch: 4520 mean train loss:  2.01723781e-04, mean val. rec. loss:  2.08454597e-04\n",
      "Epoch: 4521 mean train loss:  2.01631942e-04, mean val. rec. loss:  2.07472175e-04\n",
      "Epoch: 4522 mean train loss:  2.00457746e-04, mean val. rec. loss:  2.08016340e-04\n",
      "Epoch: 4523 mean train loss:  2.00734469e-04, mean val. rec. loss:  2.06392222e-04\n",
      "Epoch: 4524 mean train loss:  2.00373737e-04, mean val. rec. loss:  2.07075586e-04\n",
      "Epoch: 4525 mean train loss:  2.01100286e-04, mean val. rec. loss:  2.06450536e-04\n",
      "Epoch: 4526 mean train loss:  1.99705212e-04, mean val. rec. loss:  2.07250075e-04\n",
      "Epoch: 4527 mean train loss:  2.00069059e-04, mean val. rec. loss:  2.06719212e-04\n",
      "Epoch: 4528 mean train loss:  1.99691999e-04, mean val. rec. loss:  2.05978024e-04\n",
      "Epoch: 4529 mean train loss:  1.99450610e-04, mean val. rec. loss:  2.06962228e-04\n",
      "Epoch: 4530 mean train loss:  2.00423802e-04, mean val. rec. loss:  2.05655142e-04\n",
      "Epoch: 4531 mean train loss:  1.99710368e-04, mean val. rec. loss:  2.05275489e-04\n",
      "Epoch: 4532 mean train loss:  1.98888694e-04, mean val. rec. loss:  2.05501841e-04\n",
      "Epoch: 4533 mean train loss:  1.99469625e-04, mean val. rec. loss:  2.06897135e-04\n",
      "Epoch: 4534 mean train loss:  1.99554979e-04, mean val. rec. loss:  2.07725876e-04\n",
      "Epoch: 4535 mean train loss:  1.99719502e-04, mean val. rec. loss:  2.03578029e-04\n",
      "Epoch: 4536 mean train loss:  1.98197224e-04, mean val. rec. loss:  2.09444943e-04\n",
      "Epoch: 4537 mean train loss:  1.98582410e-04, mean val. rec. loss:  2.04240040e-04\n",
      "Epoch: 4538 mean train loss:  1.99507002e-04, mean val. rec. loss:  2.05522776e-04\n",
      "Epoch: 4539 mean train loss:  1.98017398e-04, mean val. rec. loss:  2.05258662e-04\n",
      "Epoch: 4540 mean train loss:  1.98682497e-04, mean val. rec. loss:  2.05034963e-04\n",
      "Epoch: 4541 mean train loss:  1.98131809e-04, mean val. rec. loss:  2.04330083e-04\n",
      "Epoch: 4542 mean train loss:  1.97619584e-04, mean val. rec. loss:  2.04972814e-04\n",
      "Epoch: 4543 mean train loss:  1.97805941e-04, mean val. rec. loss:  2.06298109e-04\n",
      "Epoch: 4544 mean train loss:  1.98247589e-04, mean val. rec. loss:  2.04082088e-04\n",
      "Epoch: 4545 mean train loss:  1.97699712e-04, mean val. rec. loss:  2.04881408e-04\n",
      "Epoch: 4546 mean train loss:  1.98121601e-04, mean val. rec. loss:  2.05567352e-04\n",
      "Epoch: 4547 mean train loss:  1.97409475e-04, mean val. rec. loss:  2.05026258e-04\n",
      "Epoch: 4548 mean train loss:  1.97343010e-04, mean val. rec. loss:  2.03151165e-04\n",
      "Epoch: 4549 mean train loss:  1.97000741e-04, mean val. rec. loss:  2.06685720e-04\n",
      "Epoch: 4550 mean train loss:  1.97975288e-04, mean val. rec. loss:  2.02452464e-04\n",
      "Epoch: 4551 mean train loss:  1.97029700e-04, mean val. rec. loss:  2.07969292e-04\n",
      "Epoch: 4552 mean train loss:  1.98200004e-04, mean val. rec. loss:  2.03166684e-04\n",
      "Epoch: 4553 mean train loss:  1.96863978e-04, mean val. rec. loss:  2.08529485e-04\n",
      "Epoch: 4554 mean train loss:  1.98170701e-04, mean val. rec. loss:  2.01749911e-04\n",
      "Epoch: 4555 mean train loss:  1.96731002e-04, mean val. rec. loss:  2.04911174e-04\n",
      "Epoch: 4556 mean train loss:  1.96329984e-04, mean val. rec. loss:  2.02064107e-04\n",
      "Epoch: 4557 mean train loss:  1.95706934e-04, mean val. rec. loss:  2.03916758e-04\n",
      "Epoch: 4558 mean train loss:  1.96466284e-04, mean val. rec. loss:  2.01567808e-04\n",
      "Epoch: 4559 mean train loss:  1.97526704e-04, mean val. rec. loss:  2.03363143e-04\n",
      "Epoch: 4560 mean train loss:  1.95806141e-04, mean val. rec. loss:  2.02410759e-04\n",
      "Epoch: 4561 mean train loss:  1.95880551e-04, mean val. rec. loss:  2.02482957e-04\n",
      "Epoch: 4562 mean train loss:  1.95492119e-04, mean val. rec. loss:  2.03116293e-04\n",
      "Epoch: 4563 mean train loss:  1.95618667e-04, mean val. rec. loss:  2.01399806e-04\n",
      "Epoch: 4564 mean train loss:  1.95397475e-04, mean val. rec. loss:  2.02070976e-04\n",
      "Epoch: 4565 mean train loss:  1.94553362e-04, mean val. rec. loss:  2.02840276e-04\n",
      "Epoch: 4566 mean train loss:  1.95343162e-04, mean val. rec. loss:  2.00074402e-04\n",
      "Epoch: 4567 mean train loss:  1.94943407e-04, mean val. rec. loss:  2.03295107e-04\n",
      "Epoch: 4568 mean train loss:  1.95099939e-04, mean val. rec. loss:  2.00847318e-04\n",
      "Epoch: 4569 mean train loss:  1.96151192e-04, mean val. rec. loss:  2.01864486e-04\n",
      "Epoch: 4570 mean train loss:  1.95992461e-04, mean val. rec. loss:  2.02036594e-04\n",
      "Epoch: 4571 mean train loss:  1.94638916e-04, mean val. rec. loss:  2.02888759e-04\n",
      "Epoch: 4572 mean train loss:  1.94739980e-04, mean val. rec. loss:  2.01538296e-04\n",
      "Epoch: 4573 mean train loss:  1.94089144e-04, mean val. rec. loss:  2.00237952e-04\n",
      "Epoch: 4574 mean train loss:  1.94073231e-04, mean val. rec. loss:  2.01103818e-04\n",
      "Epoch: 4575 mean train loss:  1.94007403e-04, mean val. rec. loss:  1.99756462e-04\n",
      "Epoch: 4576 mean train loss:  1.93360067e-04, mean val. rec. loss:  1.99182350e-04\n",
      "Epoch: 4577 mean train loss:  1.93605677e-04, mean val. rec. loss:  2.01055662e-04\n",
      "Epoch: 4578 mean train loss:  1.93284524e-04, mean val. rec. loss:  1.98951763e-04\n",
      "Epoch: 4579 mean train loss:  1.94600332e-04, mean val. rec. loss:  1.99121001e-04\n",
      "Epoch: 4580 mean train loss:  1.93600985e-04, mean val. rec. loss:  2.01951149e-04\n",
      "Epoch: 4581 mean train loss:  1.93232651e-04, mean val. rec. loss:  1.97153448e-04\n",
      "Epoch: 4582 mean train loss:  1.93776990e-04, mean val. rec. loss:  2.01710077e-04\n",
      "Epoch: 4583 mean train loss:  1.92947683e-04, mean val. rec. loss:  1.99102519e-04\n",
      "Epoch: 4584 mean train loss:  1.93133376e-04, mean val. rec. loss:  1.99070282e-04\n",
      "Epoch: 4585 mean train loss:  1.92325671e-04, mean val. rec. loss:  1.98218045e-04\n",
      "Epoch: 4586 mean train loss:  1.92248916e-04, mean val. rec. loss:  2.01099021e-04\n",
      "Epoch: 4587 mean train loss:  1.92096630e-04, mean val. rec. loss:  1.97315907e-04\n",
      "Epoch: 4588 mean train loss:  1.92661291e-04, mean val. rec. loss:  2.02687321e-04\n",
      "Epoch: 4589 mean train loss:  1.92963351e-04, mean val. rec. loss:  1.98206378e-04\n",
      "Epoch: 4590 mean train loss:  1.93777158e-04, mean val. rec. loss:  2.01637643e-04\n",
      "Epoch: 4591 mean train loss:  1.92351289e-04, mean val. rec. loss:  1.98510235e-04\n",
      "Epoch: 4592 mean train loss:  1.91626681e-04, mean val. rec. loss:  1.99941091e-04\n",
      "Epoch: 4593 mean train loss:  1.92149009e-04, mean val. rec. loss:  1.97638898e-04\n",
      "Epoch: 4594 mean train loss:  1.92663119e-04, mean val. rec. loss:  2.02815271e-04\n",
      "Epoch: 4595 mean train loss:  1.92739376e-04, mean val. rec. loss:  1.97553053e-04\n",
      "Epoch: 4596 mean train loss:  1.92384739e-04, mean val. rec. loss:  2.00607737e-04\n",
      "Epoch: 4597 mean train loss:  1.91358365e-04, mean val. rec. loss:  1.98096419e-04\n",
      "Epoch: 4598 mean train loss:  1.91945345e-04, mean val. rec. loss:  1.99876181e-04\n",
      "Epoch: 4599 mean train loss:  1.91275370e-04, mean val. rec. loss:  1.96864202e-04\n",
      "Epoch: 4600 mean train loss:  1.91347819e-04, mean val. rec. loss:  1.98247665e-04\n",
      "Epoch: 4601 mean train loss:  1.91137336e-04, mean val. rec. loss:  1.96490382e-04\n",
      "Epoch: 4602 mean train loss:  1.91024565e-04, mean val. rec. loss:  1.99085310e-04\n",
      "Epoch: 4603 mean train loss:  1.90267617e-04, mean val. rec. loss:  1.96039386e-04\n",
      "Epoch: 4604 mean train loss:  1.89935005e-04, mean val. rec. loss:  1.98718942e-04\n",
      "Epoch: 4605 mean train loss:  1.90113008e-04, mean val. rec. loss:  1.96921135e-04\n",
      "Epoch: 4606 mean train loss:  1.91538878e-04, mean val. rec. loss:  1.96577609e-04\n",
      "Epoch: 4607 mean train loss:  1.91424359e-04, mean val. rec. loss:  1.96729182e-04\n",
      "Epoch: 4608 mean train loss:  1.90280500e-04, mean val. rec. loss:  1.96005695e-04\n",
      "Epoch: 4609 mean train loss:  1.89907317e-04, mean val. rec. loss:  1.97862561e-04\n",
      "Epoch: 4610 mean train loss:  1.90023685e-04, mean val. rec. loss:  1.95531420e-04\n",
      "Epoch: 4611 mean train loss:  1.88759116e-04, mean val. rec. loss:  1.94692012e-04\n",
      "Epoch: 4612 mean train loss:  1.88594463e-04, mean val. rec. loss:  1.95748504e-04\n",
      "Epoch: 4613 mean train loss:  1.89230905e-04, mean val. rec. loss:  1.96023758e-04\n",
      "Epoch: 4614 mean train loss:  1.89263848e-04, mean val. rec. loss:  1.94912931e-04\n",
      "Epoch: 4615 mean train loss:  1.88680912e-04, mean val. rec. loss:  1.95870767e-04\n",
      "Epoch: 4616 mean train loss:  1.89079250e-04, mean val. rec. loss:  1.95758481e-04\n",
      "Epoch: 4617 mean train loss:  1.89036945e-04, mean val. rec. loss:  1.94715818e-04\n",
      "Epoch: 4618 mean train loss:  1.88951288e-04, mean val. rec. loss:  1.97411856e-04\n",
      "Epoch: 4619 mean train loss:  1.87931084e-04, mean val. rec. loss:  1.95877599e-04\n",
      "Epoch: 4620 mean train loss:  1.88624706e-04, mean val. rec. loss:  1.98652613e-04\n",
      "Epoch: 4621 mean train loss:  1.89700659e-04, mean val. rec. loss:  1.96648480e-04\n",
      "Epoch: 4622 mean train loss:  1.89089653e-04, mean val. rec. loss:  1.97367552e-04\n",
      "Epoch: 4623 mean train loss:  1.88342244e-04, mean val. rec. loss:  1.92087434e-04\n",
      "Epoch: 4624 mean train loss:  1.88664952e-04, mean val. rec. loss:  1.94038069e-04\n",
      "Epoch: 4625 mean train loss:  1.87589271e-04, mean val. rec. loss:  1.95153893e-04\n",
      "Epoch: 4626 mean train loss:  1.87620266e-04, mean val. rec. loss:  1.92862059e-04\n",
      "Epoch: 4627 mean train loss:  1.87860229e-04, mean val. rec. loss:  1.93960837e-04\n",
      "Epoch: 4628 mean train loss:  1.87465012e-04, mean val. rec. loss:  1.94393498e-04\n",
      "Epoch: 4629 mean train loss:  1.87494172e-04, mean val. rec. loss:  1.93759726e-04\n",
      "Epoch: 4630 mean train loss:  1.88021951e-04, mean val. rec. loss:  1.95607252e-04\n",
      "Epoch: 4631 mean train loss:  1.87006822e-04, mean val. rec. loss:  1.93324940e-04\n",
      "Epoch: 4632 mean train loss:  1.86765182e-04, mean val. rec. loss:  1.92363851e-04\n",
      "Epoch: 4633 mean train loss:  1.86918738e-04, mean val. rec. loss:  1.92756515e-04\n",
      "Epoch: 4634 mean train loss:  1.86532067e-04, mean val. rec. loss:  1.95325729e-04\n",
      "Epoch: 4635 mean train loss:  1.87227994e-04, mean val. rec. loss:  1.92279859e-04\n",
      "Epoch: 4636 mean train loss:  1.86553682e-04, mean val. rec. loss:  1.95430474e-04\n",
      "Epoch: 4637 mean train loss:  1.85754143e-04, mean val. rec. loss:  1.90524920e-04\n",
      "Epoch: 4638 mean train loss:  1.86652225e-04, mean val. rec. loss:  1.94743839e-04\n",
      "Epoch: 4639 mean train loss:  1.85697607e-04, mean val. rec. loss:  1.91062507e-04\n",
      "Epoch: 4640 mean train loss:  1.85893870e-04, mean val. rec. loss:  1.92484532e-04\n",
      "Epoch: 4641 mean train loss:  1.85887365e-04, mean val. rec. loss:  1.93328665e-04\n",
      "Epoch: 4642 mean train loss:  1.85791835e-04, mean val. rec. loss:  1.90082447e-04\n",
      "Epoch: 4643 mean train loss:  1.86026094e-04, mean val. rec. loss:  1.93829362e-04\n",
      "Epoch: 4644 mean train loss:  1.85173543e-04, mean val. rec. loss:  1.90945351e-04\n",
      "Epoch: 4645 mean train loss:  1.86097779e-04, mean val. rec. loss:  1.90276071e-04\n",
      "Epoch: 4646 mean train loss:  1.84930679e-04, mean val. rec. loss:  1.92169427e-04\n",
      "Epoch: 4647 mean train loss:  1.84611782e-04, mean val. rec. loss:  1.90509928e-04\n",
      "Epoch: 4648 mean train loss:  1.84341677e-04, mean val. rec. loss:  1.91965354e-04\n",
      "Epoch: 4649 mean train loss:  1.85899678e-04, mean val. rec. loss:  1.90895796e-04\n",
      "Epoch: 4650 mean train loss:  1.85112857e-04, mean val. rec. loss:  1.90291536e-04\n",
      "Epoch: 4651 mean train loss:  1.84921980e-04, mean val. rec. loss:  1.92351040e-04\n",
      "Epoch: 4652 mean train loss:  1.85810771e-04, mean val. rec. loss:  1.92204299e-04\n",
      "Epoch: 4653 mean train loss:  1.85754904e-04, mean val. rec. loss:  1.93056737e-04\n",
      "Epoch: 4654 mean train loss:  1.85221694e-04, mean val. rec. loss:  1.92209097e-04\n",
      "Epoch: 4655 mean train loss:  1.84392509e-04, mean val. rec. loss:  1.91167615e-04\n",
      "Epoch: 4656 mean train loss:  1.84834506e-04, mean val. rec. loss:  1.91646451e-04\n",
      "Epoch: 4657 mean train loss:  1.85748234e-04, mean val. rec. loss:  1.90352994e-04\n",
      "Epoch: 4658 mean train loss:  1.85325427e-04, mean val. rec. loss:  1.93348127e-04\n",
      "Epoch: 4659 mean train loss:  1.84178066e-04, mean val. rec. loss:  1.90258626e-04\n",
      "Epoch: 4660 mean train loss:  1.83725597e-04, mean val. rec. loss:  1.89808084e-04\n",
      "Epoch: 4661 mean train loss:  1.84140119e-04, mean val. rec. loss:  1.90908662e-04\n",
      "Epoch: 4662 mean train loss:  1.84845138e-04, mean val. rec. loss:  1.92380024e-04\n",
      "Epoch: 4663 mean train loss:  1.84581237e-04, mean val. rec. loss:  1.89337080e-04\n",
      "Epoch: 4664 mean train loss:  1.82610986e-04, mean val. rec. loss:  1.92256490e-04\n",
      "Epoch: 4665 mean train loss:  1.83186378e-04, mean val. rec. loss:  1.88393128e-04\n",
      "Epoch: 4666 mean train loss:  1.83586612e-04, mean val. rec. loss:  1.91294874e-04\n",
      "Epoch: 4667 mean train loss:  1.83597281e-04, mean val. rec. loss:  1.88212042e-04\n",
      "Epoch: 4668 mean train loss:  1.84123872e-04, mean val. rec. loss:  1.91015986e-04\n",
      "Epoch: 4669 mean train loss:  1.84197529e-04, mean val. rec. loss:  1.88302212e-04\n",
      "Epoch: 4670 mean train loss:  1.83342696e-04, mean val. rec. loss:  1.92240935e-04\n",
      "Epoch: 4671 mean train loss:  1.84009499e-04, mean val. rec. loss:  1.89974268e-04\n",
      "Epoch: 4672 mean train loss:  1.82585079e-04, mean val. rec. loss:  1.88467415e-04\n",
      "Epoch: 4673 mean train loss:  1.81649229e-04, mean val. rec. loss:  1.89459415e-04\n",
      "Epoch: 4674 mean train loss:  1.82461866e-04, mean val. rec. loss:  1.88116057e-04\n",
      "Epoch: 4675 mean train loss:  1.83030534e-04, mean val. rec. loss:  1.93540352e-04\n",
      "Epoch: 4676 mean train loss:  1.82816250e-04, mean val. rec. loss:  1.88364597e-04\n",
      "Epoch: 4677 mean train loss:  1.82880468e-04, mean val. rec. loss:  1.92801109e-04\n",
      "Epoch: 4678 mean train loss:  1.83381479e-04, mean val. rec. loss:  1.90781438e-04\n",
      "Epoch: 4679 mean train loss:  1.84815987e-04, mean val. rec. loss:  1.92407173e-04\n",
      "Epoch: 4680 mean train loss:  1.82593244e-04, mean val. rec. loss:  1.87669585e-04\n",
      "Epoch: 4681 mean train loss:  1.80668940e-04, mean val. rec. loss:  1.87012935e-04\n",
      "Epoch: 4682 mean train loss:  1.81020236e-04, mean val. rec. loss:  1.86415507e-04\n",
      "Epoch: 4683 mean train loss:  1.80670243e-04, mean val. rec. loss:  1.89941213e-04\n",
      "Epoch: 4684 mean train loss:  1.80937485e-04, mean val. rec. loss:  1.86911898e-04\n",
      "Epoch: 4685 mean train loss:  1.81700744e-04, mean val. rec. loss:  1.90942589e-04\n",
      "Epoch: 4686 mean train loss:  1.81239252e-04, mean val. rec. loss:  1.85754786e-04\n",
      "Epoch: 4687 mean train loss:  1.82160947e-04, mean val. rec. loss:  1.87229729e-04\n",
      "Epoch: 4688 mean train loss:  1.80392462e-04, mean val. rec. loss:  1.88721335e-04\n",
      "Epoch: 4689 mean train loss:  1.80962472e-04, mean val. rec. loss:  1.86723761e-04\n",
      "Epoch: 4690 mean train loss:  1.81079898e-04, mean val. rec. loss:  1.87017551e-04\n",
      "Epoch: 4691 mean train loss:  1.80266633e-04, mean val. rec. loss:  1.87016715e-04\n",
      "Epoch: 4692 mean train loss:  1.79368184e-04, mean val. rec. loss:  1.85861329e-04\n",
      "Epoch: 4693 mean train loss:  1.80436875e-04, mean val. rec. loss:  1.88709577e-04\n",
      "Epoch: 4694 mean train loss:  1.80240410e-04, mean val. rec. loss:  1.85041384e-04\n",
      "Epoch: 4695 mean train loss:  1.79603513e-04, mean val. rec. loss:  1.85854406e-04\n",
      "Epoch: 4696 mean train loss:  1.78806857e-04, mean val. rec. loss:  1.85132572e-04\n",
      "Epoch: 4697 mean train loss:  1.79000180e-04, mean val. rec. loss:  1.87363239e-04\n",
      "Epoch: 4698 mean train loss:  1.78981252e-04, mean val. rec. loss:  1.84775271e-04\n",
      "Epoch: 4699 mean train loss:  1.78301326e-04, mean val. rec. loss:  1.86101874e-04\n",
      "Epoch: 4700 mean train loss:  1.78488742e-04, mean val. rec. loss:  1.84511302e-04\n",
      "Epoch: 4701 mean train loss:  1.80026100e-04, mean val. rec. loss:  1.87267000e-04\n",
      "Epoch: 4702 mean train loss:  1.79054362e-04, mean val. rec. loss:  1.84553571e-04\n",
      "Epoch: 4703 mean train loss:  1.78311402e-04, mean val. rec. loss:  1.83893649e-04\n",
      "Epoch: 4704 mean train loss:  1.78259805e-04, mean val. rec. loss:  1.83695300e-04\n",
      "Epoch: 4705 mean train loss:  1.77955919e-04, mean val. rec. loss:  1.85060665e-04\n",
      "Epoch: 4706 mean train loss:  1.78409863e-04, mean val. rec. loss:  1.84193617e-04\n",
      "Epoch: 4707 mean train loss:  1.77675949e-04, mean val. rec. loss:  1.84731113e-04\n",
      "Epoch: 4708 mean train loss:  1.78612951e-04, mean val. rec. loss:  1.84426293e-04\n",
      "Epoch: 4709 mean train loss:  1.77464363e-04, mean val. rec. loss:  1.83537966e-04\n",
      "Epoch: 4710 mean train loss:  1.77907406e-04, mean val. rec. loss:  1.84812906e-04\n",
      "Epoch: 4711 mean train loss:  1.78728068e-04, mean val. rec. loss:  1.84857264e-04\n",
      "Epoch: 4712 mean train loss:  1.78927800e-04, mean val. rec. loss:  1.85522910e-04\n",
      "Epoch: 4713 mean train loss:  1.77216771e-04, mean val. rec. loss:  1.84200195e-04\n",
      "Epoch: 4714 mean train loss:  1.76922299e-04, mean val. rec. loss:  1.85451493e-04\n",
      "Epoch: 4715 mean train loss:  1.77605214e-04, mean val. rec. loss:  1.82643642e-04\n",
      "Epoch: 4716 mean train loss:  1.77123633e-04, mean val. rec. loss:  1.83305526e-04\n",
      "Epoch: 4717 mean train loss:  1.77082385e-04, mean val. rec. loss:  1.83102270e-04\n",
      "Epoch: 4718 mean train loss:  1.76760700e-04, mean val. rec. loss:  1.85228539e-04\n",
      "Epoch: 4719 mean train loss:  1.77543204e-04, mean val. rec. loss:  1.81894059e-04\n",
      "Epoch: 4720 mean train loss:  1.76004475e-04, mean val. rec. loss:  1.82896107e-04\n",
      "Epoch: 4721 mean train loss:  1.76226924e-04, mean val. rec. loss:  1.84341938e-04\n",
      "Epoch: 4722 mean train loss:  1.76334282e-04, mean val. rec. loss:  1.82074399e-04\n",
      "Epoch: 4723 mean train loss:  1.77096149e-04, mean val. rec. loss:  1.82944427e-04\n",
      "Epoch: 4724 mean train loss:  1.75927612e-04, mean val. rec. loss:  1.83865755e-04\n",
      "Epoch: 4725 mean train loss:  1.76300929e-04, mean val. rec. loss:  1.80408795e-04\n",
      "Epoch: 4726 mean train loss:  1.76523479e-04, mean val. rec. loss:  1.83994886e-04\n",
      "Epoch: 4727 mean train loss:  1.75213196e-04, mean val. rec. loss:  1.81049526e-04\n",
      "Epoch: 4728 mean train loss:  1.76378548e-04, mean val. rec. loss:  1.82580912e-04\n",
      "Epoch: 4729 mean train loss:  1.76048317e-04, mean val. rec. loss:  1.83200327e-04\n",
      "Epoch: 4730 mean train loss:  1.76447320e-04, mean val. rec. loss:  1.82505079e-04\n",
      "Epoch: 4731 mean train loss:  1.75662235e-04, mean val. rec. loss:  1.81395342e-04\n",
      "Epoch: 4732 mean train loss:  1.75347283e-04, mean val. rec. loss:  1.83792303e-04\n",
      "Epoch: 4733 mean train loss:  1.74990742e-04, mean val. rec. loss:  1.81231375e-04\n",
      "Epoch: 4734 mean train loss:  1.75223832e-04, mean val. rec. loss:  1.82697758e-04\n",
      "Epoch: 4735 mean train loss:  1.75716521e-04, mean val. rec. loss:  1.81964512e-04\n",
      "Epoch: 4736 mean train loss:  1.75915258e-04, mean val. rec. loss:  1.83952400e-04\n",
      "Epoch: 4737 mean train loss:  1.75530218e-04, mean val. rec. loss:  1.80660061e-04\n",
      "Epoch: 4738 mean train loss:  1.74947856e-04, mean val. rec. loss:  1.83717180e-04\n",
      "Epoch: 4739 mean train loss:  1.75049491e-04, mean val. rec. loss:  1.79305945e-04\n",
      "Epoch: 4740 mean train loss:  1.74914705e-04, mean val. rec. loss:  1.82039799e-04\n",
      "Epoch: 4741 mean train loss:  1.74379970e-04, mean val. rec. loss:  1.84347317e-04\n",
      "Epoch: 4742 mean train loss:  1.75965769e-04, mean val. rec. loss:  1.81093194e-04\n",
      "Epoch: 4743 mean train loss:  1.75696555e-04, mean val. rec. loss:  1.82689218e-04\n",
      "Epoch: 4744 mean train loss:  1.74637934e-04, mean val. rec. loss:  1.78744135e-04\n",
      "Epoch: 4745 mean train loss:  1.73843922e-04, mean val. rec. loss:  1.81459981e-04\n",
      "Epoch: 4746 mean train loss:  1.73800869e-04, mean val. rec. loss:  1.79646310e-04\n",
      "Epoch: 4747 mean train loss:  1.73563477e-04, mean val. rec. loss:  1.80135231e-04\n",
      "Epoch: 4748 mean train loss:  1.73563757e-04, mean val. rec. loss:  1.80530730e-04\n",
      "Epoch: 4749 mean train loss:  1.73902951e-04, mean val. rec. loss:  1.81579045e-04\n",
      "Epoch: 4750 mean train loss:  1.73783573e-04, mean val. rec. loss:  1.78988496e-04\n",
      "Epoch: 4751 mean train loss:  1.73558051e-04, mean val. rec. loss:  1.81298176e-04\n",
      "Epoch: 4752 mean train loss:  1.73794274e-04, mean val. rec. loss:  1.79335366e-04\n",
      "Epoch: 4753 mean train loss:  1.72522679e-04, mean val. rec. loss:  1.78660888e-04\n",
      "Epoch: 4754 mean train loss:  1.73072248e-04, mean val. rec. loss:  1.78904722e-04\n",
      "Epoch: 4755 mean train loss:  1.72912881e-04, mean val. rec. loss:  1.80213480e-04\n",
      "Epoch: 4756 mean train loss:  1.72943117e-04, mean val. rec. loss:  1.76798025e-04\n",
      "Epoch: 4757 mean train loss:  1.73054330e-04, mean val. rec. loss:  1.80015568e-04\n",
      "Epoch: 4758 mean train loss:  1.72963390e-04, mean val. rec. loss:  1.78357395e-04\n",
      "Epoch: 4759 mean train loss:  1.72535524e-04, mean val. rec. loss:  1.79928214e-04\n",
      "Epoch: 4760 mean train loss:  1.72173011e-04, mean val. rec. loss:  1.79578437e-04\n",
      "Epoch: 4761 mean train loss:  1.72563280e-04, mean val. rec. loss:  1.77968657e-04\n",
      "Epoch: 4762 mean train loss:  1.72353364e-04, mean val. rec. loss:  1.79041995e-04\n",
      "Epoch: 4763 mean train loss:  1.72298551e-04, mean val. rec. loss:  1.80386661e-04\n",
      "Epoch: 4764 mean train loss:  1.71854858e-04, mean val. rec. loss:  1.76397438e-04\n",
      "Epoch: 4765 mean train loss:  1.72862266e-04, mean val. rec. loss:  1.76733351e-04\n",
      "Epoch: 4766 mean train loss:  1.72426025e-04, mean val. rec. loss:  1.81719461e-04\n",
      "Epoch: 4767 mean train loss:  1.72353433e-04, mean val. rec. loss:  1.75210070e-04\n",
      "Epoch: 4768 mean train loss:  1.71651265e-04, mean val. rec. loss:  1.78354651e-04\n",
      "Epoch: 4769 mean train loss:  1.71806801e-04, mean val. rec. loss:  1.78958112e-04\n",
      "Epoch: 4770 mean train loss:  1.72358262e-04, mean val. rec. loss:  1.78200116e-04\n",
      "Epoch: 4771 mean train loss:  1.72423435e-04, mean val. rec. loss:  1.81692948e-04\n",
      "Epoch: 4772 mean train loss:  1.72073122e-04, mean val. rec. loss:  1.81682517e-04\n",
      "Epoch: 4773 mean train loss:  1.73067972e-04, mean val. rec. loss:  1.78700249e-04\n",
      "Epoch: 4774 mean train loss:  1.72953820e-04, mean val. rec. loss:  1.78187013e-04\n",
      "Epoch: 4775 mean train loss:  1.71908714e-04, mean val. rec. loss:  1.75849621e-04\n",
      "Epoch: 4776 mean train loss:  1.71223260e-04, mean val. rec. loss:  1.79077558e-04\n",
      "Epoch: 4777 mean train loss:  1.70704201e-04, mean val. rec. loss:  1.76167361e-04\n",
      "Epoch: 4778 mean train loss:  1.71481882e-04, mean val. rec. loss:  1.77293144e-04\n",
      "Epoch: 4779 mean train loss:  1.71511087e-04, mean val. rec. loss:  1.77680283e-04\n",
      "Epoch: 4780 mean train loss:  1.70427776e-04, mean val. rec. loss:  1.77180440e-04\n",
      "Epoch: 4781 mean train loss:  1.70489963e-04, mean val. rec. loss:  1.75075415e-04\n",
      "Epoch: 4782 mean train loss:  1.69807442e-04, mean val. rec. loss:  1.78468500e-04\n",
      "Epoch: 4783 mean train loss:  1.69673305e-04, mean val. rec. loss:  1.74481622e-04\n",
      "Epoch: 4784 mean train loss:  1.69757565e-04, mean val. rec. loss:  1.76080934e-04\n",
      "Epoch: 4785 mean train loss:  1.69479839e-04, mean val. rec. loss:  1.75605060e-04\n",
      "Epoch: 4786 mean train loss:  1.69623955e-04, mean val. rec. loss:  1.75765393e-04\n",
      "Epoch: 4787 mean train loss:  1.69469933e-04, mean val. rec. loss:  1.74932945e-04\n",
      "Epoch: 4788 mean train loss:  1.68402503e-04, mean val. rec. loss:  1.75614110e-04\n",
      "Epoch: 4789 mean train loss:  1.68911523e-04, mean val. rec. loss:  1.76915672e-04\n",
      "Epoch: 4790 mean train loss:  1.69342663e-04, mean val. rec. loss:  1.73907018e-04\n",
      "Epoch: 4791 mean train loss:  1.68852360e-04, mean val. rec. loss:  1.75492684e-04\n",
      "Epoch: 4792 mean train loss:  1.68776854e-04, mean val. rec. loss:  1.73836910e-04\n",
      "Epoch: 4793 mean train loss:  1.68652434e-04, mean val. rec. loss:  1.75510983e-04\n",
      "Epoch: 4794 mean train loss:  1.68506426e-04, mean val. rec. loss:  1.74190922e-04\n",
      "Epoch: 4795 mean train loss:  1.68755607e-04, mean val. rec. loss:  1.77571414e-04\n",
      "Epoch: 4796 mean train loss:  1.69075190e-04, mean val. rec. loss:  1.73701328e-04\n",
      "Epoch: 4797 mean train loss:  1.69582652e-04, mean val. rec. loss:  1.74937997e-04\n",
      "Epoch: 4798 mean train loss:  1.69606932e-04, mean val. rec. loss:  1.74853151e-04\n",
      "Epoch: 4799 mean train loss:  1.69963052e-04, mean val. rec. loss:  1.75883021e-04\n",
      "Epoch: 4800 mean train loss:  1.70238630e-04, mean val. rec. loss:  1.76592789e-04\n",
      "Epoch: 4801 mean train loss:  1.68490868e-04, mean val. rec. loss:  1.72787868e-04\n",
      "Epoch: 4802 mean train loss:  1.68599996e-04, mean val. rec. loss:  1.76130453e-04\n",
      "Epoch: 4803 mean train loss:  1.67336838e-04, mean val. rec. loss:  1.72366602e-04\n",
      "Epoch: 4804 mean train loss:  1.67689890e-04, mean val. rec. loss:  1.73965278e-04\n",
      "Epoch: 4805 mean train loss:  1.67734697e-04, mean val. rec. loss:  1.74719967e-04\n",
      "Epoch: 4806 mean train loss:  1.67648457e-04, mean val. rec. loss:  1.73239046e-04\n",
      "Epoch: 4807 mean train loss:  1.67321468e-04, mean val. rec. loss:  1.73552407e-04\n",
      "Epoch: 4808 mean train loss:  1.68298206e-04, mean val. rec. loss:  1.76708037e-04\n",
      "Epoch: 4809 mean train loss:  1.67130398e-04, mean val. rec. loss:  1.73397090e-04\n",
      "Epoch: 4810 mean train loss:  1.66637418e-04, mean val. rec. loss:  1.76795081e-04\n",
      "Epoch: 4811 mean train loss:  1.67888459e-04, mean val. rec. loss:  1.71782512e-04\n",
      "Epoch: 4812 mean train loss:  1.67606527e-04, mean val. rec. loss:  1.74229574e-04\n",
      "Epoch: 4813 mean train loss:  1.66625497e-04, mean val. rec. loss:  1.73133993e-04\n",
      "Epoch: 4814 mean train loss:  1.66020262e-04, mean val. rec. loss:  1.72047172e-04\n",
      "Epoch: 4815 mean train loss:  1.66016133e-04, mean val. rec. loss:  1.72248483e-04\n",
      "Epoch: 4816 mean train loss:  1.66492410e-04, mean val. rec. loss:  1.70946884e-04\n",
      "Epoch: 4817 mean train loss:  1.65999862e-04, mean val. rec. loss:  1.71980189e-04\n",
      "Epoch: 4818 mean train loss:  1.66200303e-04, mean val. rec. loss:  1.71438477e-04\n",
      "Epoch: 4819 mean train loss:  1.66328692e-04, mean val. rec. loss:  1.71499845e-04\n",
      "Epoch: 4820 mean train loss:  1.65647157e-04, mean val. rec. loss:  1.72194257e-04\n",
      "Epoch: 4821 mean train loss:  1.65619522e-04, mean val. rec. loss:  1.70832182e-04\n",
      "Epoch: 4822 mean train loss:  1.65688563e-04, mean val. rec. loss:  1.74227012e-04\n",
      "Epoch: 4823 mean train loss:  1.65413412e-04, mean val. rec. loss:  1.70662527e-04\n",
      "Epoch: 4824 mean train loss:  1.65502814e-04, mean val. rec. loss:  1.71413927e-04\n",
      "Epoch: 4825 mean train loss:  1.64901986e-04, mean val. rec. loss:  1.70882155e-04\n",
      "Epoch: 4826 mean train loss:  1.65926448e-04, mean val. rec. loss:  1.73508231e-04\n",
      "Epoch: 4827 mean train loss:  1.65055048e-04, mean val. rec. loss:  1.70449913e-04\n",
      "Epoch: 4828 mean train loss:  1.66067782e-04, mean val. rec. loss:  1.74726291e-04\n",
      "Epoch: 4829 mean train loss:  1.65823352e-04, mean val. rec. loss:  1.72220716e-04\n",
      "Epoch: 4830 mean train loss:  1.64951448e-04, mean val. rec. loss:  1.71985495e-04\n",
      "Epoch: 4831 mean train loss:  1.64628466e-04, mean val. rec. loss:  1.71224991e-04\n",
      "Epoch: 4832 mean train loss:  1.64985316e-04, mean val. rec. loss:  1.70853552e-04\n",
      "Epoch: 4833 mean train loss:  1.64116137e-04, mean val. rec. loss:  1.69782304e-04\n",
      "Epoch: 4834 mean train loss:  1.64330009e-04, mean val. rec. loss:  1.71453342e-04\n",
      "Epoch: 4835 mean train loss:  1.64613245e-04, mean val. rec. loss:  1.72327241e-04\n",
      "Epoch: 4836 mean train loss:  1.64258002e-04, mean val. rec. loss:  1.68271217e-04\n",
      "Epoch: 4837 mean train loss:  1.65063963e-04, mean val. rec. loss:  1.74649314e-04\n",
      "Epoch: 4838 mean train loss:  1.64571392e-04, mean val. rec. loss:  1.68099054e-04\n",
      "Epoch: 4839 mean train loss:  1.63764981e-04, mean val. rec. loss:  1.72193130e-04\n",
      "Epoch: 4840 mean train loss:  1.63971482e-04, mean val. rec. loss:  1.70355163e-04\n",
      "Epoch: 4841 mean train loss:  1.63843461e-04, mean val. rec. loss:  1.70525981e-04\n",
      "Epoch: 4842 mean train loss:  1.64183657e-04, mean val. rec. loss:  1.68730755e-04\n",
      "Epoch: 4843 mean train loss:  1.63920847e-04, mean val. rec. loss:  1.70183636e-04\n",
      "Epoch: 4844 mean train loss:  1.63597588e-04, mean val. rec. loss:  1.69085221e-04\n",
      "Epoch: 4845 mean train loss:  1.63581388e-04, mean val. rec. loss:  1.68863811e-04\n",
      "Epoch: 4846 mean train loss:  1.63527473e-04, mean val. rec. loss:  1.69806927e-04\n",
      "Epoch: 4847 mean train loss:  1.63685351e-04, mean val. rec. loss:  1.68680163e-04\n",
      "Epoch: 4848 mean train loss:  1.64049837e-04, mean val. rec. loss:  1.72313103e-04\n",
      "Epoch: 4849 mean train loss:  1.63909902e-04, mean val. rec. loss:  1.69514301e-04\n",
      "Epoch: 4850 mean train loss:  1.64632627e-04, mean val. rec. loss:  1.70615733e-04\n",
      "Epoch: 4851 mean train loss:  1.62656105e-04, mean val. rec. loss:  1.69227109e-04\n",
      "Epoch: 4852 mean train loss:  1.62088013e-04, mean val. rec. loss:  1.70466086e-04\n",
      "Epoch: 4853 mean train loss:  1.63308680e-04, mean val. rec. loss:  1.67499083e-04\n",
      "Epoch: 4854 mean train loss:  1.63924875e-04, mean val. rec. loss:  1.71287576e-04\n",
      "Epoch: 4855 mean train loss:  1.62699210e-04, mean val. rec. loss:  1.67233006e-04\n",
      "Epoch: 4856 mean train loss:  1.62669829e-04, mean val. rec. loss:  1.68831992e-04\n",
      "Epoch: 4857 mean train loss:  1.62152294e-04, mean val. rec. loss:  1.67890384e-04\n",
      "Epoch: 4858 mean train loss:  1.62480500e-04, mean val. rec. loss:  1.69073463e-04\n",
      "Epoch: 4859 mean train loss:  1.61419299e-04, mean val. rec. loss:  1.66653587e-04\n",
      "Epoch: 4860 mean train loss:  1.61433915e-04, mean val. rec. loss:  1.68434876e-04\n",
      "Epoch: 4861 mean train loss:  1.61097967e-04, mean val. rec. loss:  1.67737011e-04\n",
      "Epoch: 4862 mean train loss:  1.61155837e-04, mean val. rec. loss:  1.67970995e-04\n",
      "Epoch: 4863 mean train loss:  1.61668670e-04, mean val. rec. loss:  1.68532060e-04\n",
      "Epoch: 4864 mean train loss:  1.61453432e-04, mean val. rec. loss:  1.66118836e-04\n",
      "Epoch: 4865 mean train loss:  1.62863651e-04, mean val. rec. loss:  1.70835598e-04\n",
      "Epoch: 4866 mean train loss:  1.61850855e-04, mean val. rec. loss:  1.67986242e-04\n",
      "Epoch: 4867 mean train loss:  1.61529981e-04, mean val. rec. loss:  1.68044756e-04\n",
      "Epoch: 4868 mean train loss:  1.61482071e-04, mean val. rec. loss:  1.69626696e-04\n",
      "Epoch: 4869 mean train loss:  1.61238246e-04, mean val. rec. loss:  1.66968529e-04\n",
      "Epoch: 4870 mean train loss:  1.60787227e-04, mean val. rec. loss:  1.66731092e-04\n",
      "Epoch: 4871 mean train loss:  1.62096541e-04, mean val. rec. loss:  1.67342330e-04\n",
      "Epoch: 4872 mean train loss:  1.61789355e-04, mean val. rec. loss:  1.66356236e-04\n",
      "Epoch: 4873 mean train loss:  1.61101204e-04, mean val. rec. loss:  1.69581829e-04\n",
      "Epoch: 4874 mean train loss:  1.60937328e-04, mean val. rec. loss:  1.66481552e-04\n",
      "Epoch: 4875 mean train loss:  1.60717729e-04, mean val. rec. loss:  1.67656163e-04\n",
      "Epoch: 4876 mean train loss:  1.60700484e-04, mean val. rec. loss:  1.68283066e-04\n",
      "Epoch: 4877 mean train loss:  1.60585044e-04, mean val. rec. loss:  1.68090059e-04\n",
      "Epoch: 4878 mean train loss:  1.60222832e-04, mean val. rec. loss:  1.66037788e-04\n",
      "Epoch: 4879 mean train loss:  1.60409804e-04, mean val. rec. loss:  1.67929181e-04\n",
      "Epoch: 4880 mean train loss:  1.59926981e-04, mean val. rec. loss:  1.64626212e-04\n",
      "Epoch: 4881 mean train loss:  1.59636412e-04, mean val. rec. loss:  1.68902336e-04\n",
      "Epoch: 4882 mean train loss:  1.60002149e-04, mean val. rec. loss:  1.65944274e-04\n",
      "Epoch: 4883 mean train loss:  1.60716380e-04, mean val. rec. loss:  1.67053756e-04\n",
      "Epoch: 4884 mean train loss:  1.59735153e-04, mean val. rec. loss:  1.64285011e-04\n",
      "Epoch: 4885 mean train loss:  1.59723608e-04, mean val. rec. loss:  1.71249414e-04\n",
      "Epoch: 4886 mean train loss:  1.60361971e-04, mean val. rec. loss:  1.63665832e-04\n",
      "Epoch: 4887 mean train loss:  1.59724114e-04, mean val. rec. loss:  1.68139469e-04\n",
      "Epoch: 4888 mean train loss:  1.59952579e-04, mean val. rec. loss:  1.65443995e-04\n",
      "Epoch: 4889 mean train loss:  1.59323607e-04, mean val. rec. loss:  1.64961706e-04\n",
      "Epoch: 4890 mean train loss:  1.59304569e-04, mean val. rec. loss:  1.66656967e-04\n",
      "Epoch: 4891 mean train loss:  1.58620161e-04, mean val. rec. loss:  1.64574221e-04\n",
      "Epoch: 4892 mean train loss:  1.59720565e-04, mean val. rec. loss:  1.65662569e-04\n",
      "Epoch: 4893 mean train loss:  1.59022730e-04, mean val. rec. loss:  1.65115297e-04\n",
      "Epoch: 4894 mean train loss:  1.59052541e-04, mean val. rec. loss:  1.67397428e-04\n",
      "Epoch: 4895 mean train loss:  1.59306900e-04, mean val. rec. loss:  1.65283644e-04\n",
      "Epoch: 4896 mean train loss:  1.58518518e-04, mean val. rec. loss:  1.66697509e-04\n",
      "Epoch: 4897 mean train loss:  1.59069302e-04, mean val. rec. loss:  1.65691172e-04\n",
      "Epoch: 4898 mean train loss:  1.59608978e-04, mean val. rec. loss:  1.64708531e-04\n",
      "Epoch: 4899 mean train loss:  1.58578892e-04, mean val. rec. loss:  1.66256290e-04\n",
      "Epoch: 4900 mean train loss:  1.58846610e-04, mean val. rec. loss:  1.64493010e-04\n",
      "Epoch: 4901 mean train loss:  1.58105908e-04, mean val. rec. loss:  1.64249576e-04\n",
      "Epoch: 4902 mean train loss:  1.57693363e-04, mean val. rec. loss:  1.62599472e-04\n",
      "Epoch: 4903 mean train loss:  1.57855351e-04, mean val. rec. loss:  1.65188313e-04\n",
      "Epoch: 4904 mean train loss:  1.57261854e-04, mean val. rec. loss:  1.62793950e-04\n",
      "Epoch: 4905 mean train loss:  1.58129884e-04, mean val. rec. loss:  1.64892906e-04\n",
      "Epoch: 4906 mean train loss:  1.56797545e-04, mean val. rec. loss:  1.62705452e-04\n",
      "Epoch: 4907 mean train loss:  1.57646319e-04, mean val. rec. loss:  1.67049976e-04\n",
      "Epoch: 4908 mean train loss:  1.57082166e-04, mean val. rec. loss:  1.64013192e-04\n",
      "Epoch: 4909 mean train loss:  1.57807313e-04, mean val. rec. loss:  1.65274376e-04\n",
      "Epoch: 4910 mean train loss:  1.57687940e-04, mean val. rec. loss:  1.61940841e-04\n",
      "Epoch: 4911 mean train loss:  1.56609288e-04, mean val. rec. loss:  1.63021248e-04\n",
      "Epoch: 4912 mean train loss:  1.56833190e-04, mean val. rec. loss:  1.62921810e-04\n",
      "Epoch: 4913 mean train loss:  1.57666258e-04, mean val. rec. loss:  1.63312238e-04\n",
      "Epoch: 4914 mean train loss:  1.57287469e-04, mean val. rec. loss:  1.63211655e-04\n",
      "Epoch: 4915 mean train loss:  1.56839435e-04, mean val. rec. loss:  1.61558898e-04\n",
      "Epoch: 4916 mean train loss:  1.56022241e-04, mean val. rec. loss:  1.62637634e-04\n",
      "Epoch: 4917 mean train loss:  1.56368997e-04, mean val. rec. loss:  1.62803563e-04\n",
      "Epoch: 4918 mean train loss:  1.57040774e-04, mean val. rec. loss:  1.61987434e-04\n",
      "Epoch: 4919 mean train loss:  1.57005727e-04, mean val. rec. loss:  1.63533375e-04\n",
      "Epoch: 4920 mean train loss:  1.56404878e-04, mean val. rec. loss:  1.63054139e-04\n",
      "Epoch: 4921 mean train loss:  1.56711264e-04, mean val. rec. loss:  1.61191966e-04\n",
      "Epoch: 4922 mean train loss:  1.54933938e-04, mean val. rec. loss:  1.63084796e-04\n",
      "Epoch: 4923 mean train loss:  1.55466163e-04, mean val. rec. loss:  1.60464972e-04\n",
      "Epoch: 4924 mean train loss:  1.55998807e-04, mean val. rec. loss:  1.64888363e-04\n",
      "Epoch: 4925 mean train loss:  1.56311972e-04, mean val. rec. loss:  1.61796645e-04\n",
      "Epoch: 4926 mean train loss:  1.57076416e-04, mean val. rec. loss:  1.61715561e-04\n",
      "Epoch: 4927 mean train loss:  1.55387426e-04, mean val. rec. loss:  1.62129667e-04\n",
      "Epoch: 4928 mean train loss:  1.55380503e-04, mean val. rec. loss:  1.60340456e-04\n",
      "Epoch: 4929 mean train loss:  1.55882048e-04, mean val. rec. loss:  1.61679562e-04\n",
      "Epoch: 4930 mean train loss:  1.55301992e-04, mean val. rec. loss:  1.61165381e-04\n",
      "Epoch: 4931 mean train loss:  1.54466731e-04, mean val. rec. loss:  1.62626803e-04\n",
      "Epoch: 4932 mean train loss:  1.54993282e-04, mean val. rec. loss:  1.59367029e-04\n",
      "Epoch: 4933 mean train loss:  1.54810561e-04, mean val. rec. loss:  1.62869219e-04\n",
      "Epoch: 4934 mean train loss:  1.55511886e-04, mean val. rec. loss:  1.59734015e-04\n",
      "Epoch: 4935 mean train loss:  1.54745513e-04, mean val. rec. loss:  1.60201403e-04\n",
      "Epoch: 4936 mean train loss:  1.54647644e-04, mean val. rec. loss:  1.61666260e-04\n",
      "Epoch: 4937 mean train loss:  1.54690411e-04, mean val. rec. loss:  1.62275753e-04\n",
      "Epoch: 4938 mean train loss:  1.54802945e-04, mean val. rec. loss:  1.60875371e-04\n",
      "Epoch: 4939 mean train loss:  1.54493306e-04, mean val. rec. loss:  1.61632532e-04\n",
      "Epoch: 4940 mean train loss:  1.55593700e-04, mean val. rec. loss:  1.63590981e-04\n",
      "Epoch: 4941 mean train loss:  1.55653490e-04, mean val. rec. loss:  1.58974583e-04\n",
      "Epoch: 4942 mean train loss:  1.54047952e-04, mean val. rec. loss:  1.61865590e-04\n",
      "Epoch: 4943 mean train loss:  1.54869847e-04, mean val. rec. loss:  1.60542403e-04\n",
      "Epoch: 4944 mean train loss:  1.54316272e-04, mean val. rec. loss:  1.59327813e-04\n",
      "Epoch: 4945 mean train loss:  1.54478830e-04, mean val. rec. loss:  1.65846744e-04\n",
      "Epoch: 4946 mean train loss:  1.54649750e-04, mean val. rec. loss:  1.59171224e-04\n",
      "Epoch: 4947 mean train loss:  1.53510557e-04, mean val. rec. loss:  1.59870488e-04\n",
      "Epoch: 4948 mean train loss:  1.52923047e-04, mean val. rec. loss:  1.59670013e-04\n",
      "Epoch: 4949 mean train loss:  1.54154184e-04, mean val. rec. loss:  1.58038935e-04\n",
      "Epoch: 4950 mean train loss:  1.53770708e-04, mean val. rec. loss:  1.62425274e-04\n",
      "Epoch: 4951 mean train loss:  1.53947810e-04, mean val. rec. loss:  1.57934082e-04\n",
      "Epoch: 4952 mean train loss:  1.53988105e-04, mean val. rec. loss:  1.62797258e-04\n",
      "Epoch: 4953 mean train loss:  1.54372356e-04, mean val. rec. loss:  1.57692592e-04\n",
      "Epoch: 4954 mean train loss:  1.52841197e-04, mean val. rec. loss:  1.62755153e-04\n",
      "Epoch: 4955 mean train loss:  1.54335139e-04, mean val. rec. loss:  1.58644686e-04\n",
      "Epoch: 4956 mean train loss:  1.53972546e-04, mean val. rec. loss:  1.58161615e-04\n",
      "Epoch: 4957 mean train loss:  1.53324729e-04, mean val. rec. loss:  1.60087263e-04\n",
      "Epoch: 4958 mean train loss:  1.52762261e-04, mean val. rec. loss:  1.58008697e-04\n",
      "Epoch: 4959 mean train loss:  1.52615407e-04, mean val. rec. loss:  1.57318137e-04\n",
      "Epoch: 4960 mean train loss:  1.52158121e-04, mean val. rec. loss:  1.59243749e-04\n",
      "Epoch: 4961 mean train loss:  1.52522211e-04, mean val. rec. loss:  1.57682416e-04\n",
      "Epoch: 4962 mean train loss:  1.53418662e-04, mean val. rec. loss:  1.58666583e-04\n",
      "Epoch: 4963 mean train loss:  1.52145445e-04, mean val. rec. loss:  1.58382589e-04\n",
      "Epoch: 4964 mean train loss:  1.51953400e-04, mean val. rec. loss:  1.56654617e-04\n",
      "Epoch: 4965 mean train loss:  1.52222255e-04, mean val. rec. loss:  1.60677077e-04\n",
      "Epoch: 4966 mean train loss:  1.52414115e-04, mean val. rec. loss:  1.56669282e-04\n",
      "Epoch: 4967 mean train loss:  1.51732541e-04, mean val. rec. loss:  1.58700801e-04\n",
      "Epoch: 4968 mean train loss:  1.51956027e-04, mean val. rec. loss:  1.57159149e-04\n",
      "Epoch: 4969 mean train loss:  1.52005335e-04, mean val. rec. loss:  1.58138464e-04\n",
      "Epoch: 4970 mean train loss:  1.51126623e-04, mean val. rec. loss:  1.58646049e-04\n",
      "Epoch: 4971 mean train loss:  1.51928982e-04, mean val. rec. loss:  1.60802010e-04\n",
      "Epoch: 4972 mean train loss:  1.52071396e-04, mean val. rec. loss:  1.57123949e-04\n",
      "Epoch: 4973 mean train loss:  1.51838882e-04, mean val. rec. loss:  1.57557083e-04\n",
      "Epoch: 4974 mean train loss:  1.51908655e-04, mean val. rec. loss:  1.56821274e-04\n",
      "Epoch: 4975 mean train loss:  1.51287370e-04, mean val. rec. loss:  1.55523547e-04\n",
      "Epoch: 4976 mean train loss:  1.50546578e-04, mean val. rec. loss:  1.59661363e-04\n",
      "Epoch: 4977 mean train loss:  1.51061039e-04, mean val. rec. loss:  1.55125776e-04\n",
      "Epoch: 4978 mean train loss:  1.50695925e-04, mean val. rec. loss:  1.57940915e-04\n",
      "Epoch: 4979 mean train loss:  1.50434309e-04, mean val. rec. loss:  1.56529575e-04\n",
      "Epoch: 4980 mean train loss:  1.50530547e-04, mean val. rec. loss:  1.56788637e-04\n",
      "Epoch: 4981 mean train loss:  1.50340413e-04, mean val. rec. loss:  1.56946771e-04\n",
      "Epoch: 4982 mean train loss:  1.50286776e-04, mean val. rec. loss:  1.56305730e-04\n",
      "Epoch: 4983 mean train loss:  1.49714933e-04, mean val. rec. loss:  1.55995859e-04\n",
      "Epoch: 4984 mean train loss:  1.49808769e-04, mean val. rec. loss:  1.57349393e-04\n",
      "Epoch: 4985 mean train loss:  1.49946878e-04, mean val. rec. loss:  1.56198133e-04\n",
      "Epoch: 4986 mean train loss:  1.49516064e-04, mean val. rec. loss:  1.56815168e-04\n",
      "Epoch: 4987 mean train loss:  1.50145613e-04, mean val. rec. loss:  1.55870871e-04\n",
      "Epoch: 4988 mean train loss:  1.49692649e-04, mean val. rec. loss:  1.56070128e-04\n",
      "Epoch: 4989 mean train loss:  1.49617090e-04, mean val. rec. loss:  1.56471624e-04\n",
      "Epoch: 4990 mean train loss:  1.50535938e-04, mean val. rec. loss:  1.57862611e-04\n",
      "Epoch: 4991 mean train loss:  1.50963607e-04, mean val. rec. loss:  1.57950455e-04\n",
      "Epoch: 4992 mean train loss:  1.51049672e-04, mean val. rec. loss:  1.56295536e-04\n",
      "Epoch: 4993 mean train loss:  1.50278995e-04, mean val. rec. loss:  1.57506909e-04\n",
      "Epoch: 4994 mean train loss:  1.49380143e-04, mean val. rec. loss:  1.55090486e-04\n",
      "Epoch: 4995 mean train loss:  1.49449017e-04, mean val. rec. loss:  1.54442122e-04\n",
      "Epoch: 4996 mean train loss:  1.49231148e-04, mean val. rec. loss:  1.56781732e-04\n",
      "Epoch: 4997 mean train loss:  1.49474549e-04, mean val. rec. loss:  1.53012319e-04\n",
      "Epoch: 4998 mean train loss:  1.49940422e-04, mean val. rec. loss:  1.55608520e-04\n",
      "Epoch: 4999 mean train loss:  1.49614544e-04, mean val. rec. loss:  1.58215714e-04\n",
      "Epoch: 5000 mean train loss:  1.49805272e-04, mean val. rec. loss:  1.54036102e-04\n",
      "Epoch: 5001 mean train loss:  1.48207486e-04, mean val. rec. loss:  1.55107477e-04\n",
      "Epoch: 5002 mean train loss:  1.48642430e-04, mean val. rec. loss:  1.56852639e-04\n",
      "Epoch: 5003 mean train loss:  1.48322988e-04, mean val. rec. loss:  1.52767650e-04\n",
      "Epoch: 5004 mean train loss:  1.47799235e-04, mean val. rec. loss:  1.56372767e-04\n",
      "Epoch: 5005 mean train loss:  1.48402926e-04, mean val. rec. loss:  1.53367876e-04\n",
      "Epoch: 5006 mean train loss:  1.48193774e-04, mean val. rec. loss:  1.55182619e-04\n",
      "Epoch: 5007 mean train loss:  1.48059677e-04, mean val. rec. loss:  1.53226478e-04\n",
      "Epoch: 5008 mean train loss:  1.47980234e-04, mean val. rec. loss:  1.53799191e-04\n",
      "Epoch: 5009 mean train loss:  1.47655541e-04, mean val. rec. loss:  1.54537453e-04\n",
      "Epoch: 5010 mean train loss:  1.48690301e-04, mean val. rec. loss:  1.56702447e-04\n",
      "Epoch: 5011 mean train loss:  1.47913658e-04, mean val. rec. loss:  1.55780337e-04\n",
      "Epoch: 5012 mean train loss:  1.48528085e-04, mean val. rec. loss:  1.56811788e-04\n",
      "Epoch: 5013 mean train loss:  1.48008177e-04, mean val. rec. loss:  1.53539711e-04\n",
      "Epoch: 5014 mean train loss:  1.47304164e-04, mean val. rec. loss:  1.53858669e-04\n",
      "Epoch: 5015 mean train loss:  1.47860734e-04, mean val. rec. loss:  1.53014809e-04\n",
      "Epoch: 5016 mean train loss:  1.47032987e-04, mean val. rec. loss:  1.53348522e-04\n",
      "Epoch: 5017 mean train loss:  1.46756320e-04, mean val. rec. loss:  1.52348127e-04\n",
      "Epoch: 5018 mean train loss:  1.47397325e-04, mean val. rec. loss:  1.53201855e-04\n",
      "Epoch: 5019 mean train loss:  1.47612830e-04, mean val. rec. loss:  1.53326934e-04\n",
      "Epoch: 5020 mean train loss:  1.47228838e-04, mean val. rec. loss:  1.51212722e-04\n",
      "Epoch: 5021 mean train loss:  1.47642367e-04, mean val. rec. loss:  1.54135612e-04\n",
      "Epoch: 5022 mean train loss:  1.46957045e-04, mean val. rec. loss:  1.52950425e-04\n",
      "Epoch: 5023 mean train loss:  1.46418352e-04, mean val. rec. loss:  1.53064001e-04\n",
      "Epoch: 5024 mean train loss:  1.46582741e-04, mean val. rec. loss:  1.51622568e-04\n",
      "Epoch: 5025 mean train loss:  1.46276506e-04, mean val. rec. loss:  1.51802245e-04\n",
      "Epoch: 5026 mean train loss:  1.46580206e-04, mean val. rec. loss:  1.51411099e-04\n",
      "Epoch: 5027 mean train loss:  1.45843164e-04, mean val. rec. loss:  1.53620305e-04\n",
      "Epoch: 5028 mean train loss:  1.46939209e-04, mean val. rec. loss:  1.51079611e-04\n",
      "Epoch: 5029 mean train loss:  1.46472549e-04, mean val. rec. loss:  1.52521889e-04\n",
      "Epoch: 5030 mean train loss:  1.46622491e-04, mean val. rec. loss:  1.53022823e-04\n",
      "Epoch: 5031 mean train loss:  1.46151247e-04, mean val. rec. loss:  1.51923444e-04\n",
      "Epoch: 5032 mean train loss:  1.45679845e-04, mean val. rec. loss:  1.50920178e-04\n",
      "Epoch: 5033 mean train loss:  1.45717732e-04, mean val. rec. loss:  1.52628669e-04\n",
      "Epoch: 5034 mean train loss:  1.46109991e-04, mean val. rec. loss:  1.51292998e-04\n",
      "Epoch: 5035 mean train loss:  1.46066520e-04, mean val. rec. loss:  1.54572216e-04\n",
      "Epoch: 5036 mean train loss:  1.45731744e-04, mean val. rec. loss:  1.50671474e-04\n",
      "Epoch: 5037 mean train loss:  1.46105662e-04, mean val. rec. loss:  1.52081306e-04\n",
      "Epoch: 5038 mean train loss:  1.46031789e-04, mean val. rec. loss:  1.52352607e-04\n",
      "Epoch: 5039 mean train loss:  1.46939767e-04, mean val. rec. loss:  1.53117827e-04\n",
      "Epoch: 5040 mean train loss:  1.45375406e-04, mean val. rec. loss:  1.50280173e-04\n",
      "Epoch: 5041 mean train loss:  1.45260156e-04, mean val. rec. loss:  1.52215243e-04\n",
      "Epoch: 5042 mean train loss:  1.44682523e-04, mean val. rec. loss:  1.50978420e-04\n",
      "Epoch: 5043 mean train loss:  1.45190208e-04, mean val. rec. loss:  1.50547949e-04\n",
      "Epoch: 5044 mean train loss:  1.45158933e-04, mean val. rec. loss:  1.50856021e-04\n",
      "Epoch: 5045 mean train loss:  1.44512605e-04, mean val. rec. loss:  1.50014015e-04\n",
      "Epoch: 5046 mean train loss:  1.44917854e-04, mean val. rec. loss:  1.50521736e-04\n",
      "Epoch: 5047 mean train loss:  1.44541868e-04, mean val. rec. loss:  1.51640268e-04\n",
      "Epoch: 5048 mean train loss:  1.44013138e-04, mean val. rec. loss:  1.48810501e-04\n",
      "Epoch: 5049 mean train loss:  1.44176731e-04, mean val. rec. loss:  1.50334962e-04\n",
      "Epoch: 5050 mean train loss:  1.44194560e-04, mean val. rec. loss:  1.50009445e-04\n",
      "Epoch: 5051 mean train loss:  1.44237556e-04, mean val. rec. loss:  1.51473556e-04\n",
      "Epoch: 5052 mean train loss:  1.45082991e-04, mean val. rec. loss:  1.49974618e-04\n",
      "Epoch: 5053 mean train loss:  1.44676766e-04, mean val. rec. loss:  1.50697415e-04\n",
      "Epoch: 5054 mean train loss:  1.44901051e-04, mean val. rec. loss:  1.50626344e-04\n",
      "Epoch: 5055 mean train loss:  1.43784869e-04, mean val. rec. loss:  1.49034273e-04\n",
      "Epoch: 5056 mean train loss:  1.44196546e-04, mean val. rec. loss:  1.52938468e-04\n",
      "Epoch: 5057 mean train loss:  1.43790511e-04, mean val. rec. loss:  1.49522358e-04\n",
      "Epoch: 5058 mean train loss:  1.43666111e-04, mean val. rec. loss:  1.50885288e-04\n",
      "Epoch: 5059 mean train loss:  1.43871969e-04, mean val. rec. loss:  1.49350359e-04\n",
      "Epoch: 5060 mean train loss:  1.43461546e-04, mean val. rec. loss:  1.50410286e-04\n",
      "Epoch: 5061 mean train loss:  1.43282036e-04, mean val. rec. loss:  1.48744618e-04\n",
      "Epoch: 5062 mean train loss:  1.43304082e-04, mean val. rec. loss:  1.48866180e-04\n",
      "Epoch: 5063 mean train loss:  1.42795363e-04, mean val. rec. loss:  1.50284925e-04\n",
      "Epoch: 5064 mean train loss:  1.42985642e-04, mean val. rec. loss:  1.48811373e-04\n",
      "Epoch: 5065 mean train loss:  1.42980282e-04, mean val. rec. loss:  1.48105085e-04\n",
      "Epoch: 5066 mean train loss:  1.43806156e-04, mean val. rec. loss:  1.50611997e-04\n",
      "Epoch: 5067 mean train loss:  1.43782460e-04, mean val. rec. loss:  1.47921583e-04\n",
      "Epoch: 5068 mean train loss:  1.43505158e-04, mean val. rec. loss:  1.48760427e-04\n",
      "Epoch: 5069 mean train loss:  1.43453040e-04, mean val. rec. loss:  1.49585052e-04\n",
      "Epoch: 5070 mean train loss:  1.43389340e-04, mean val. rec. loss:  1.48717114e-04\n",
      "Epoch: 5071 mean train loss:  1.42405200e-04, mean val. rec. loss:  1.47919411e-04\n",
      "Epoch: 5072 mean train loss:  1.42152133e-04, mean val. rec. loss:  1.48192702e-04\n",
      "Epoch: 5073 mean train loss:  1.42404037e-04, mean val. rec. loss:  1.48494305e-04\n",
      "Epoch: 5074 mean train loss:  1.42102837e-04, mean val. rec. loss:  1.47028140e-04\n",
      "Epoch: 5075 mean train loss:  1.41790128e-04, mean val. rec. loss:  1.49589941e-04\n",
      "Epoch: 5076 mean train loss:  1.43010750e-04, mean val. rec. loss:  1.48829900e-04\n",
      "Epoch: 5077 mean train loss:  1.42355067e-04, mean val. rec. loss:  1.47443274e-04\n",
      "Epoch: 5078 mean train loss:  1.42389551e-04, mean val. rec. loss:  1.50125237e-04\n",
      "Epoch: 5079 mean train loss:  1.42337046e-04, mean val. rec. loss:  1.50351871e-04\n",
      "Epoch: 5080 mean train loss:  1.43412617e-04, mean val. rec. loss:  1.46014770e-04\n",
      "Epoch: 5081 mean train loss:  1.41728904e-04, mean val. rec. loss:  1.49815366e-04\n",
      "Epoch: 5082 mean train loss:  1.42416249e-04, mean val. rec. loss:  1.48610807e-04\n",
      "Epoch: 5083 mean train loss:  1.41612940e-04, mean val. rec. loss:  1.46507572e-04\n",
      "Epoch: 5084 mean train loss:  1.41464947e-04, mean val. rec. loss:  1.49022606e-04\n",
      "Epoch: 5085 mean train loss:  1.42009568e-04, mean val. rec. loss:  1.47281805e-04\n",
      "Epoch: 5086 mean train loss:  1.40997283e-04, mean val. rec. loss:  1.51175551e-04\n",
      "Epoch: 5087 mean train loss:  1.42065211e-04, mean val. rec. loss:  1.48771876e-04\n",
      "Epoch: 5088 mean train loss:  1.41901775e-04, mean val. rec. loss:  1.52658471e-04\n",
      "Epoch: 5089 mean train loss:  1.42828225e-04, mean val. rec. loss:  1.47163159e-04\n",
      "Epoch: 5090 mean train loss:  1.40992815e-04, mean val. rec. loss:  1.48241549e-04\n",
      "Epoch: 5091 mean train loss:  1.41471738e-04, mean val. rec. loss:  1.46198027e-04\n",
      "Epoch: 5092 mean train loss:  1.41207803e-04, mean val. rec. loss:  1.47118928e-04\n",
      "Epoch: 5093 mean train loss:  1.40888487e-04, mean val. rec. loss:  1.46778437e-04\n",
      "Epoch: 5094 mean train loss:  1.40475950e-04, mean val. rec. loss:  1.46368028e-04\n",
      "Epoch: 5095 mean train loss:  1.40532296e-04, mean val. rec. loss:  1.45168512e-04\n",
      "Epoch: 5096 mean train loss:  1.39962952e-04, mean val. rec. loss:  1.48548913e-04\n",
      "Epoch: 5097 mean train loss:  1.40263010e-04, mean val. rec. loss:  1.44453083e-04\n",
      "Epoch: 5098 mean train loss:  1.39772920e-04, mean val. rec. loss:  1.46679235e-04\n",
      "Epoch: 5099 mean train loss:  1.39701170e-04, mean val. rec. loss:  1.46797481e-04\n",
      "Epoch: 5100 mean train loss:  1.40288801e-04, mean val. rec. loss:  1.44170306e-04\n",
      "Epoch: 5101 mean train loss:  1.39510990e-04, mean val. rec. loss:  1.47227343e-04\n",
      "Epoch: 5102 mean train loss:  1.40319578e-04, mean val. rec. loss:  1.45716293e-04\n",
      "Epoch: 5103 mean train loss:  1.41427577e-04, mean val. rec. loss:  1.45171992e-04\n",
      "Epoch: 5104 mean train loss:  1.40577036e-04, mean val. rec. loss:  1.46706848e-04\n",
      "Epoch: 5105 mean train loss:  1.40407733e-04, mean val. rec. loss:  1.44705858e-04\n",
      "Epoch: 5106 mean train loss:  1.39720587e-04, mean val. rec. loss:  1.46678290e-04\n",
      "Epoch: 5107 mean train loss:  1.39859504e-04, mean val. rec. loss:  1.44531769e-04\n",
      "Epoch: 5108 mean train loss:  1.39759714e-04, mean val. rec. loss:  1.45962489e-04\n",
      "Epoch: 5109 mean train loss:  1.39686355e-04, mean val. rec. loss:  1.45003100e-04\n",
      "Epoch: 5110 mean train loss:  1.39716297e-04, mean val. rec. loss:  1.44936662e-04\n",
      "Epoch: 5111 mean train loss:  1.39762947e-04, mean val. rec. loss:  1.45689652e-04\n",
      "Epoch: 5112 mean train loss:  1.39801455e-04, mean val. rec. loss:  1.43300942e-04\n",
      "Epoch: 5113 mean train loss:  1.39022670e-04, mean val. rec. loss:  1.47070736e-04\n",
      "Epoch: 5114 mean train loss:  1.40040260e-04, mean val. rec. loss:  1.44584886e-04\n",
      "Epoch: 5115 mean train loss:  1.39516170e-04, mean val. rec. loss:  1.46329276e-04\n",
      "Epoch: 5116 mean train loss:  1.39348499e-04, mean val. rec. loss:  1.46503383e-04\n",
      "Epoch: 5117 mean train loss:  1.39109047e-04, mean val. rec. loss:  1.46786932e-04\n",
      "Epoch: 5118 mean train loss:  1.39426678e-04, mean val. rec. loss:  1.44686841e-04\n",
      "Epoch: 5119 mean train loss:  1.38452693e-04, mean val. rec. loss:  1.46938551e-04\n",
      "Epoch: 5120 mean train loss:  1.38942472e-04, mean val. rec. loss:  1.42924706e-04\n",
      "Epoch: 5121 mean train loss:  1.39418011e-04, mean val. rec. loss:  1.45321285e-04\n",
      "Epoch: 5122 mean train loss:  1.38916407e-04, mean val. rec. loss:  1.44097591e-04\n",
      "Epoch: 5123 mean train loss:  1.38167355e-04, mean val. rec. loss:  1.43057144e-04\n",
      "Epoch: 5124 mean train loss:  1.39423944e-04, mean val. rec. loss:  1.46473154e-04\n",
      "Epoch: 5125 mean train loss:  1.38453600e-04, mean val. rec. loss:  1.43616647e-04\n",
      "Epoch: 5126 mean train loss:  1.38680457e-04, mean val. rec. loss:  1.45745986e-04\n",
      "Epoch: 5127 mean train loss:  1.37885497e-04, mean val. rec. loss:  1.45728595e-04\n",
      "Epoch: 5128 mean train loss:  1.38867013e-04, mean val. rec. loss:  1.43588970e-04\n",
      "Epoch: 5129 mean train loss:  1.37627577e-04, mean val. rec. loss:  1.41512821e-04\n",
      "Epoch: 5130 mean train loss:  1.37428410e-04, mean val. rec. loss:  1.43566928e-04\n",
      "Epoch: 5131 mean train loss:  1.37055420e-04, mean val. rec. loss:  1.43934623e-04\n",
      "Epoch: 5132 mean train loss:  1.37680944e-04, mean val. rec. loss:  1.42993006e-04\n",
      "Epoch: 5133 mean train loss:  1.38500965e-04, mean val. rec. loss:  1.47200094e-04\n",
      "Epoch: 5134 mean train loss:  1.38700791e-04, mean val. rec. loss:  1.41690035e-04\n",
      "Epoch: 5135 mean train loss:  1.37661691e-04, mean val. rec. loss:  1.46967672e-04\n",
      "Epoch: 5136 mean train loss:  1.37979701e-04, mean val. rec. loss:  1.42312213e-04\n",
      "Epoch: 5137 mean train loss:  1.37604505e-04, mean val. rec. loss:  1.44321871e-04\n",
      "Epoch: 5138 mean train loss:  1.38547294e-04, mean val. rec. loss:  1.43883132e-04\n",
      "Epoch: 5139 mean train loss:  1.39385944e-04, mean val. rec. loss:  1.43257165e-04\n",
      "Epoch: 5140 mean train loss:  1.37629834e-04, mean val. rec. loss:  1.47560112e-04\n",
      "Epoch: 5141 mean train loss:  1.38190292e-04, mean val. rec. loss:  1.40559001e-04\n",
      "Epoch: 5142 mean train loss:  1.36341605e-04, mean val. rec. loss:  1.43904966e-04\n",
      "Epoch: 5143 mean train loss:  1.36811542e-04, mean val. rec. loss:  1.42986200e-04\n",
      "Epoch: 5144 mean train loss:  1.35980483e-04, mean val. rec. loss:  1.42165346e-04\n",
      "Epoch: 5145 mean train loss:  1.35855975e-04, mean val. rec. loss:  1.44471146e-04\n",
      "Epoch: 5146 mean train loss:  1.36728334e-04, mean val. rec. loss:  1.40772306e-04\n",
      "Epoch: 5147 mean train loss:  1.37319469e-04, mean val. rec. loss:  1.45099003e-04\n",
      "Epoch: 5148 mean train loss:  1.36678470e-04, mean val. rec. loss:  1.42077575e-04\n",
      "Epoch: 5149 mean train loss:  1.36892021e-04, mean val. rec. loss:  1.41221957e-04\n",
      "Epoch: 5150 mean train loss:  1.36176395e-04, mean val. rec. loss:  1.43304349e-04\n",
      "Epoch: 5151 mean train loss:  1.36989931e-04, mean val. rec. loss:  1.42563080e-04\n",
      "Epoch: 5152 mean train loss:  1.35774111e-04, mean val. rec. loss:  1.42055650e-04\n",
      "Epoch: 5153 mean train loss:  1.35772477e-04, mean val. rec. loss:  1.40712119e-04\n",
      "Epoch: 5154 mean train loss:  1.35625848e-04, mean val. rec. loss:  1.43356149e-04\n",
      "Epoch: 5155 mean train loss:  1.35936367e-04, mean val. rec. loss:  1.40171389e-04\n",
      "Epoch: 5156 mean train loss:  1.36033934e-04, mean val. rec. loss:  1.44209258e-04\n",
      "Epoch: 5157 mean train loss:  1.36277444e-04, mean val. rec. loss:  1.39945891e-04\n",
      "Epoch: 5158 mean train loss:  1.36031563e-04, mean val. rec. loss:  1.42579289e-04\n",
      "Epoch: 5159 mean train loss:  1.35265203e-04, mean val. rec. loss:  1.41612822e-04\n",
      "Epoch: 5160 mean train loss:  1.35446916e-04, mean val. rec. loss:  1.39890248e-04\n",
      "Epoch: 5161 mean train loss:  1.36329493e-04, mean val. rec. loss:  1.42269954e-04\n",
      "Epoch: 5162 mean train loss:  1.35550988e-04, mean val. rec. loss:  1.41081650e-04\n",
      "Epoch: 5163 mean train loss:  1.35531015e-04, mean val. rec. loss:  1.42495116e-04\n",
      "Epoch: 5164 mean train loss:  1.35401047e-04, mean val. rec. loss:  1.40318865e-04\n",
      "Epoch: 5165 mean train loss:  1.35982100e-04, mean val. rec. loss:  1.44223360e-04\n",
      "Epoch: 5166 mean train loss:  1.35787253e-04, mean val. rec. loss:  1.39926565e-04\n",
      "Epoch: 5167 mean train loss:  1.34744685e-04, mean val. rec. loss:  1.41886131e-04\n",
      "Epoch: 5168 mean train loss:  1.34191941e-04, mean val. rec. loss:  1.39940167e-04\n",
      "Epoch: 5169 mean train loss:  1.34286097e-04, mean val. rec. loss:  1.40926714e-04\n",
      "Epoch: 5170 mean train loss:  1.34497727e-04, mean val. rec. loss:  1.39876019e-04\n",
      "Epoch: 5171 mean train loss:  1.34116629e-04, mean val. rec. loss:  1.40617642e-04\n",
      "Epoch: 5172 mean train loss:  1.34419438e-04, mean val. rec. loss:  1.40069098e-04\n",
      "Epoch: 5173 mean train loss:  1.34085768e-04, mean val. rec. loss:  1.40359916e-04\n",
      "Epoch: 5174 mean train loss:  1.33774297e-04, mean val. rec. loss:  1.39511286e-04\n",
      "Epoch: 5175 mean train loss:  1.34032504e-04, mean val. rec. loss:  1.41430410e-04\n",
      "Epoch: 5176 mean train loss:  1.34669808e-04, mean val. rec. loss:  1.38907871e-04\n",
      "Epoch: 5177 mean train loss:  1.35073770e-04, mean val. rec. loss:  1.43928790e-04\n",
      "Epoch: 5178 mean train loss:  1.35372012e-04, mean val. rec. loss:  1.41020937e-04\n",
      "Epoch: 5179 mean train loss:  1.36245008e-04, mean val. rec. loss:  1.40864075e-04\n",
      "Epoch: 5180 mean train loss:  1.35301093e-04, mean val. rec. loss:  1.39657990e-04\n",
      "Epoch: 5181 mean train loss:  1.34952130e-04, mean val. rec. loss:  1.43777425e-04\n",
      "Epoch: 5182 mean train loss:  1.34118045e-04, mean val. rec. loss:  1.39734113e-04\n",
      "Epoch: 5183 mean train loss:  1.34468974e-04, mean val. rec. loss:  1.40743757e-04\n",
      "Epoch: 5184 mean train loss:  1.33219737e-04, mean val. rec. loss:  1.38218546e-04\n",
      "Epoch: 5185 mean train loss:  1.34588007e-04, mean val. rec. loss:  1.39365372e-04\n",
      "Epoch: 5186 mean train loss:  1.33657535e-04, mean val. rec. loss:  1.39458514e-04\n",
      "Epoch: 5187 mean train loss:  1.33816466e-04, mean val. rec. loss:  1.37510232e-04\n",
      "Epoch: 5188 mean train loss:  1.33483042e-04, mean val. rec. loss:  1.41539270e-04\n",
      "Epoch: 5189 mean train loss:  1.32960939e-04, mean val. rec. loss:  1.36795313e-04\n",
      "Epoch: 5190 mean train loss:  1.33816531e-04, mean val. rec. loss:  1.41521352e-04\n",
      "Epoch: 5191 mean train loss:  1.32305493e-04, mean val. rec. loss:  1.37791419e-04\n",
      "Epoch: 5192 mean train loss:  1.33685387e-04, mean val. rec. loss:  1.40808177e-04\n",
      "Epoch: 5193 mean train loss:  1.32914126e-04, mean val. rec. loss:  1.38757514e-04\n",
      "Epoch: 5194 mean train loss:  1.32642589e-04, mean val. rec. loss:  1.39204785e-04\n",
      "Epoch: 5195 mean train loss:  1.32858472e-04, mean val. rec. loss:  1.39667775e-04\n",
      "Epoch: 5196 mean train loss:  1.33060325e-04, mean val. rec. loss:  1.39337769e-04\n",
      "Epoch: 5197 mean train loss:  1.33068271e-04, mean val. rec. loss:  1.38092977e-04\n",
      "Epoch: 5198 mean train loss:  1.32442797e-04, mean val. rec. loss:  1.40372064e-04\n",
      "Epoch: 5199 mean train loss:  1.33062786e-04, mean val. rec. loss:  1.37890694e-04\n",
      "Epoch: 5200 mean train loss:  1.33050427e-04, mean val. rec. loss:  1.37397420e-04\n",
      "Epoch: 5201 mean train loss:  1.32601658e-04, mean val. rec. loss:  1.37389279e-04\n",
      "Epoch: 5202 mean train loss:  1.32493283e-04, mean val. rec. loss:  1.39067449e-04\n",
      "Epoch: 5203 mean train loss:  1.32800871e-04, mean val. rec. loss:  1.38078721e-04\n",
      "Epoch: 5204 mean train loss:  1.31804859e-04, mean val. rec. loss:  1.37733922e-04\n",
      "Epoch: 5205 mean train loss:  1.32034922e-04, mean val. rec. loss:  1.38173289e-04\n",
      "Epoch: 5206 mean train loss:  1.33066476e-04, mean val. rec. loss:  1.39094835e-04\n",
      "Epoch: 5207 mean train loss:  1.33735752e-04, mean val. rec. loss:  1.39601247e-04\n",
      "Epoch: 5208 mean train loss:  1.32922748e-04, mean val. rec. loss:  1.35568420e-04\n",
      "Epoch: 5209 mean train loss:  1.31948357e-04, mean val. rec. loss:  1.37242929e-04\n",
      "Epoch: 5210 mean train loss:  1.31853462e-04, mean val. rec. loss:  1.37547603e-04\n",
      "Epoch: 5211 mean train loss:  1.31486639e-04, mean val. rec. loss:  1.36342408e-04\n",
      "Epoch: 5212 mean train loss:  1.31401803e-04, mean val. rec. loss:  1.36815811e-04\n",
      "Epoch: 5213 mean train loss:  1.30916436e-04, mean val. rec. loss:  1.36381497e-04\n",
      "Epoch: 5214 mean train loss:  1.31093075e-04, mean val. rec. loss:  1.39053602e-04\n",
      "Epoch: 5215 mean train loss:  1.31319642e-04, mean val. rec. loss:  1.35735377e-04\n",
      "Epoch: 5216 mean train loss:  1.30973127e-04, mean val. rec. loss:  1.37136840e-04\n",
      "Epoch: 5217 mean train loss:  1.31993115e-04, mean val. rec. loss:  1.37222340e-04\n",
      "Epoch: 5218 mean train loss:  1.30983925e-04, mean val. rec. loss:  1.36448879e-04\n",
      "Epoch: 5219 mean train loss:  1.31038215e-04, mean val. rec. loss:  1.38767382e-04\n",
      "Epoch: 5220 mean train loss:  1.30950107e-04, mean val. rec. loss:  1.36539749e-04\n",
      "Epoch: 5221 mean train loss:  1.30700008e-04, mean val. rec. loss:  1.36126887e-04\n",
      "Epoch: 5222 mean train loss:  1.30446959e-04, mean val. rec. loss:  1.37361984e-04\n",
      "Epoch: 5223 mean train loss:  1.30904957e-04, mean val. rec. loss:  1.37475569e-04\n",
      "Epoch: 5224 mean train loss:  1.31331462e-04, mean val. rec. loss:  1.35232036e-04\n",
      "Epoch: 5225 mean train loss:  1.31536030e-04, mean val. rec. loss:  1.37103422e-04\n",
      "Epoch: 5226 mean train loss:  1.30534477e-04, mean val. rec. loss:  1.37382528e-04\n",
      "Epoch: 5227 mean train loss:  1.29878034e-04, mean val. rec. loss:  1.34464108e-04\n",
      "Epoch: 5228 mean train loss:  1.29890408e-04, mean val. rec. loss:  1.37181307e-04\n",
      "Epoch: 5229 mean train loss:  1.30091264e-04, mean val. rec. loss:  1.34742533e-04\n",
      "Epoch: 5230 mean train loss:  1.30083538e-04, mean val. rec. loss:  1.36582590e-04\n",
      "Epoch: 5231 mean train loss:  1.29708545e-04, mean val. rec. loss:  1.35471999e-04\n",
      "Epoch: 5232 mean train loss:  1.30865344e-04, mean val. rec. loss:  1.36768618e-04\n",
      "Epoch: 5233 mean train loss:  1.30476041e-04, mean val. rec. loss:  1.35565931e-04\n",
      "Epoch: 5234 mean train loss:  1.29780650e-04, mean val. rec. loss:  1.35229146e-04\n",
      "Epoch: 5235 mean train loss:  1.29677450e-04, mean val. rec. loss:  1.35319444e-04\n",
      "Epoch: 5236 mean train loss:  1.29546922e-04, mean val. rec. loss:  1.34305329e-04\n",
      "Epoch: 5237 mean train loss:  1.29642888e-04, mean val. rec. loss:  1.36873090e-04\n",
      "Epoch: 5238 mean train loss:  1.29696570e-04, mean val. rec. loss:  1.33915282e-04\n",
      "Epoch: 5239 mean train loss:  1.28756136e-04, mean val. rec. loss:  1.35315137e-04\n",
      "Epoch: 5240 mean train loss:  1.29196157e-04, mean val. rec. loss:  1.35768041e-04\n",
      "Epoch: 5241 mean train loss:  1.29376228e-04, mean val. rec. loss:  1.33201862e-04\n",
      "Epoch: 5242 mean train loss:  1.29132026e-04, mean val. rec. loss:  1.35459079e-04\n",
      "Epoch: 5243 mean train loss:  1.28987529e-04, mean val. rec. loss:  1.34364497e-04\n",
      "Epoch: 5244 mean train loss:  1.28941956e-04, mean val. rec. loss:  1.33754004e-04\n",
      "Epoch: 5245 mean train loss:  1.29060575e-04, mean val. rec. loss:  1.36321938e-04\n",
      "Epoch: 5246 mean train loss:  1.28622022e-04, mean val. rec. loss:  1.33590400e-04\n",
      "Epoch: 5247 mean train loss:  1.29070686e-04, mean val. rec. loss:  1.37917025e-04\n",
      "Epoch: 5248 mean train loss:  1.29609387e-04, mean val. rec. loss:  1.32714430e-04\n",
      "Epoch: 5249 mean train loss:  1.28728622e-04, mean val. rec. loss:  1.37353007e-04\n",
      "Epoch: 5250 mean train loss:  1.29581201e-04, mean val. rec. loss:  1.35367127e-04\n",
      "Epoch: 5251 mean train loss:  1.29082654e-04, mean val. rec. loss:  1.34312779e-04\n",
      "Epoch: 5252 mean train loss:  1.28632419e-04, mean val. rec. loss:  1.34940300e-04\n",
      "Epoch: 5253 mean train loss:  1.28180349e-04, mean val. rec. loss:  1.33910248e-04\n",
      "Epoch: 5254 mean train loss:  1.28088469e-04, mean val. rec. loss:  1.33828901e-04\n",
      "Epoch: 5255 mean train loss:  1.28350909e-04, mean val. rec. loss:  1.33429413e-04\n",
      "Epoch: 5256 mean train loss:  1.28887966e-04, mean val. rec. loss:  1.35667558e-04\n",
      "Epoch: 5257 mean train loss:  1.28313370e-04, mean val. rec. loss:  1.33391097e-04\n",
      "Epoch: 5258 mean train loss:  1.27545571e-04, mean val. rec. loss:  1.33290869e-04\n",
      "Epoch: 5259 mean train loss:  1.27807909e-04, mean val. rec. loss:  1.32801956e-04\n",
      "Epoch: 5260 mean train loss:  1.27704029e-04, mean val. rec. loss:  1.34924418e-04\n",
      "Epoch: 5261 mean train loss:  1.27735553e-04, mean val. rec. loss:  1.32413445e-04\n",
      "Epoch: 5262 mean train loss:  1.27704957e-04, mean val. rec. loss:  1.32464072e-04\n",
      "Epoch: 5263 mean train loss:  1.27336255e-04, mean val. rec. loss:  1.34124525e-04\n",
      "Epoch: 5264 mean train loss:  1.27280084e-04, mean val. rec. loss:  1.34534543e-04\n",
      "Epoch: 5265 mean train loss:  1.27141216e-04, mean val. rec. loss:  1.34073652e-04\n",
      "Epoch: 5266 mean train loss:  1.28560457e-04, mean val. rec. loss:  1.33800507e-04\n",
      "Epoch: 5267 mean train loss:  1.26840115e-04, mean val. rec. loss:  1.33114762e-04\n",
      "Epoch: 5268 mean train loss:  1.26984335e-04, mean val. rec. loss:  1.32277572e-04\n",
      "Epoch: 5269 mean train loss:  1.27417178e-04, mean val. rec. loss:  1.33852688e-04\n",
      "Epoch: 5270 mean train loss:  1.27247286e-04, mean val. rec. loss:  1.32947161e-04\n",
      "Epoch: 5271 mean train loss:  1.27377995e-04, mean val. rec. loss:  1.31976695e-04\n",
      "Epoch: 5272 mean train loss:  1.26841882e-04, mean val. rec. loss:  1.32952104e-04\n",
      "Epoch: 5273 mean train loss:  1.26905221e-04, mean val. rec. loss:  1.31901890e-04\n",
      "Epoch: 5274 mean train loss:  1.27011823e-04, mean val. rec. loss:  1.33329012e-04\n",
      "Epoch: 5275 mean train loss:  1.27190345e-04, mean val. rec. loss:  1.32486842e-04\n",
      "Epoch: 5276 mean train loss:  1.26676912e-04, mean val. rec. loss:  1.31747908e-04\n",
      "Epoch: 5277 mean train loss:  1.26689587e-04, mean val. rec. loss:  1.31408525e-04\n",
      "Epoch: 5278 mean train loss:  1.26466235e-04, mean val. rec. loss:  1.32805164e-04\n",
      "Epoch: 5279 mean train loss:  1.26772516e-04, mean val. rec. loss:  1.30704463e-04\n",
      "Epoch: 5280 mean train loss:  1.26712452e-04, mean val. rec. loss:  1.33226685e-04\n",
      "Epoch: 5281 mean train loss:  1.26648584e-04, mean val. rec. loss:  1.32094233e-04\n",
      "Epoch: 5282 mean train loss:  1.28001107e-04, mean val. rec. loss:  1.33762281e-04\n",
      "Epoch: 5283 mean train loss:  1.25693712e-04, mean val. rec. loss:  1.30986404e-04\n",
      "Epoch: 5284 mean train loss:  1.25932966e-04, mean val. rec. loss:  1.31774821e-04\n",
      "Epoch: 5285 mean train loss:  1.26866913e-04, mean val. rec. loss:  1.33447622e-04\n",
      "Epoch: 5286 mean train loss:  1.26183561e-04, mean val. rec. loss:  1.31034424e-04\n",
      "Epoch: 5287 mean train loss:  1.25877901e-04, mean val. rec. loss:  1.31209385e-04\n",
      "Epoch: 5288 mean train loss:  1.26138873e-04, mean val. rec. loss:  1.33876148e-04\n",
      "Epoch: 5289 mean train loss:  1.25094174e-04, mean val. rec. loss:  1.30437651e-04\n",
      "Epoch: 5290 mean train loss:  1.27571472e-04, mean val. rec. loss:  1.35204596e-04\n",
      "Epoch: 5291 mean train loss:  1.26688641e-04, mean val. rec. loss:  1.30456368e-04\n",
      "Epoch: 5292 mean train loss:  1.25878612e-04, mean val. rec. loss:  1.32423548e-04\n",
      "Epoch: 5293 mean train loss:  1.25247128e-04, mean val. rec. loss:  1.30594140e-04\n",
      "Epoch: 5294 mean train loss:  1.25155194e-04, mean val. rec. loss:  1.31130991e-04\n",
      "Epoch: 5295 mean train loss:  1.25351462e-04, mean val. rec. loss:  1.32396672e-04\n",
      "Epoch: 5296 mean train loss:  1.25577192e-04, mean val. rec. loss:  1.29809848e-04\n",
      "Epoch: 5297 mean train loss:  1.25160551e-04, mean val. rec. loss:  1.31792920e-04\n",
      "Epoch: 5298 mean train loss:  1.24797821e-04, mean val. rec. loss:  1.31791012e-04\n",
      "Epoch: 5299 mean train loss:  1.24769966e-04, mean val. rec. loss:  1.29193631e-04\n",
      "Epoch: 5300 mean train loss:  1.24733275e-04, mean val. rec. loss:  1.31914637e-04\n",
      "Epoch: 5301 mean train loss:  1.24617110e-04, mean val. rec. loss:  1.29694146e-04\n",
      "Epoch: 5302 mean train loss:  1.25051004e-04, mean val. rec. loss:  1.32391929e-04\n",
      "Epoch: 5303 mean train loss:  1.25167799e-04, mean val. rec. loss:  1.30374902e-04\n",
      "Epoch: 5304 mean train loss:  1.24876155e-04, mean val. rec. loss:  1.29731317e-04\n",
      "Epoch: 5305 mean train loss:  1.25002111e-04, mean val. rec. loss:  1.31141703e-04\n",
      "Epoch: 5306 mean train loss:  1.23911497e-04, mean val. rec. loss:  1.29034007e-04\n",
      "Epoch: 5307 mean train loss:  1.24561747e-04, mean val. rec. loss:  1.31049280e-04\n",
      "Epoch: 5308 mean train loss:  1.24433169e-04, mean val. rec. loss:  1.29219145e-04\n",
      "Epoch: 5309 mean train loss:  1.25174009e-04, mean val. rec. loss:  1.29186326e-04\n",
      "Epoch: 5310 mean train loss:  1.23991314e-04, mean val. rec. loss:  1.30383870e-04\n",
      "Epoch: 5311 mean train loss:  1.24302954e-04, mean val. rec. loss:  1.29885863e-04\n",
      "Epoch: 5312 mean train loss:  1.23852091e-04, mean val. rec. loss:  1.30995490e-04\n",
      "Epoch: 5313 mean train loss:  1.24034524e-04, mean val. rec. loss:  1.29892441e-04\n",
      "Epoch: 5314 mean train loss:  1.24629469e-04, mean val. rec. loss:  1.31877802e-04\n",
      "Epoch: 5315 mean train loss:  1.24052161e-04, mean val. rec. loss:  1.29270599e-04\n",
      "Epoch: 5316 mean train loss:  1.24397488e-04, mean val. rec. loss:  1.29242696e-04\n",
      "Epoch: 5317 mean train loss:  1.24276098e-04, mean val. rec. loss:  1.33082907e-04\n",
      "Epoch: 5318 mean train loss:  1.24762412e-04, mean val. rec. loss:  1.28386224e-04\n",
      "Epoch: 5319 mean train loss:  1.24427123e-04, mean val. rec. loss:  1.28708080e-04\n",
      "Epoch: 5320 mean train loss:  1.23894970e-04, mean val. rec. loss:  1.32413517e-04\n",
      "Epoch: 5321 mean train loss:  1.25246263e-04, mean val. rec. loss:  1.28794616e-04\n",
      "Epoch: 5322 mean train loss:  1.24599185e-04, mean val. rec. loss:  1.30884195e-04\n",
      "Epoch: 5323 mean train loss:  1.23622111e-04, mean val. rec. loss:  1.28000738e-04\n",
      "Epoch: 5324 mean train loss:  1.23260208e-04, mean val. rec. loss:  1.30485362e-04\n",
      "Epoch: 5325 mean train loss:  1.23012635e-04, mean val. rec. loss:  1.28928845e-04\n",
      "Epoch: 5326 mean train loss:  1.23282859e-04, mean val. rec. loss:  1.28798723e-04\n",
      "Epoch: 5327 mean train loss:  1.22882389e-04, mean val. rec. loss:  1.28140092e-04\n",
      "Epoch: 5328 mean train loss:  1.23027685e-04, mean val. rec. loss:  1.31826121e-04\n",
      "Epoch: 5329 mean train loss:  1.23492377e-04, mean val. rec. loss:  1.29404882e-04\n",
      "Epoch: 5330 mean train loss:  1.24243987e-04, mean val. rec. loss:  1.31841204e-04\n",
      "Epoch: 5331 mean train loss:  1.24235183e-04, mean val. rec. loss:  1.28373195e-04\n",
      "Epoch: 5332 mean train loss:  1.23607709e-04, mean val. rec. loss:  1.29827212e-04\n",
      "Epoch: 5333 mean train loss:  1.23189239e-04, mean val. rec. loss:  1.29026538e-04\n",
      "Epoch: 5334 mean train loss:  1.23720515e-04, mean val. rec. loss:  1.29191759e-04\n",
      "Epoch: 5335 mean train loss:  1.24886148e-04, mean val. rec. loss:  1.30844089e-04\n",
      "Epoch: 5336 mean train loss:  1.23297771e-04, mean val. rec. loss:  1.27931103e-04\n",
      "Epoch: 5337 mean train loss:  1.23314775e-04, mean val. rec. loss:  1.28070165e-04\n",
      "Epoch: 5338 mean train loss:  1.23080155e-04, mean val. rec. loss:  1.28671173e-04\n",
      "Epoch: 5339 mean train loss:  1.22131688e-04, mean val. rec. loss:  1.26382645e-04\n",
      "Epoch: 5340 mean train loss:  1.22296983e-04, mean val. rec. loss:  1.29174768e-04\n",
      "Epoch: 5341 mean train loss:  1.22076769e-04, mean val. rec. loss:  1.26134822e-04\n",
      "Epoch: 5342 mean train loss:  1.21859550e-04, mean val. rec. loss:  1.28613812e-04\n",
      "Epoch: 5343 mean train loss:  1.22528223e-04, mean val. rec. loss:  1.30186530e-04\n",
      "Epoch: 5344 mean train loss:  1.22367714e-04, mean val. rec. loss:  1.26493186e-04\n",
      "Epoch: 5345 mean train loss:  1.22028000e-04, mean val. rec. loss:  1.27398950e-04\n",
      "Epoch: 5346 mean train loss:  1.22135472e-04, mean val. rec. loss:  1.31201962e-04\n",
      "Epoch: 5347 mean train loss:  1.22645180e-04, mean val. rec. loss:  1.28889783e-04\n",
      "Epoch: 5348 mean train loss:  1.23781434e-04, mean val. rec. loss:  1.34323019e-04\n",
      "Epoch: 5349 mean train loss:  1.24600727e-04, mean val. rec. loss:  1.27466913e-04\n",
      "Epoch: 5350 mean train loss:  1.21895182e-04, mean val. rec. loss:  1.26902123e-04\n",
      "Epoch: 5351 mean train loss:  1.21688101e-04, mean val. rec. loss:  1.28233287e-04\n",
      "Epoch: 5352 mean train loss:  1.21463828e-04, mean val. rec. loss:  1.26026852e-04\n",
      "Epoch: 5353 mean train loss:  1.22100505e-04, mean val. rec. loss:  1.28192436e-04\n",
      "Epoch: 5354 mean train loss:  1.21889358e-04, mean val. rec. loss:  1.26882434e-04\n",
      "Epoch: 5355 mean train loss:  1.22003005e-04, mean val. rec. loss:  1.24962065e-04\n",
      "Epoch: 5356 mean train loss:  1.22057595e-04, mean val. rec. loss:  1.29735742e-04\n",
      "Epoch: 5357 mean train loss:  1.21088730e-04, mean val. rec. loss:  1.25058195e-04\n",
      "Epoch: 5358 mean train loss:  1.21200519e-04, mean val. rec. loss:  1.27706649e-04\n",
      "Epoch: 5359 mean train loss:  1.20729782e-04, mean val. rec. loss:  1.27871598e-04\n",
      "Epoch: 5360 mean train loss:  1.20911018e-04, mean val. rec. loss:  1.28086411e-04\n",
      "Epoch: 5361 mean train loss:  1.21672125e-04, mean val. rec. loss:  1.27878849e-04\n",
      "Epoch: 5362 mean train loss:  1.21329419e-04, mean val. rec. loss:  1.29021613e-04\n",
      "Epoch: 5363 mean train loss:  1.21008299e-04, mean val. rec. loss:  1.27965348e-04\n",
      "Epoch: 5364 mean train loss:  1.21331931e-04, mean val. rec. loss:  1.29625919e-04\n",
      "Epoch: 5365 mean train loss:  1.21714402e-04, mean val. rec. loss:  1.26048941e-04\n",
      "Epoch: 5366 mean train loss:  1.20308724e-04, mean val. rec. loss:  1.24202796e-04\n",
      "Epoch: 5367 mean train loss:  1.20178828e-04, mean val. rec. loss:  1.27216419e-04\n",
      "Epoch: 5368 mean train loss:  1.20121293e-04, mean val. rec. loss:  1.25609375e-04\n",
      "Epoch: 5369 mean train loss:  1.20120609e-04, mean val. rec. loss:  1.27800972e-04\n",
      "Epoch: 5370 mean train loss:  1.19837305e-04, mean val. rec. loss:  1.25391591e-04\n",
      "Epoch: 5371 mean train loss:  1.20247465e-04, mean val. rec. loss:  1.28754701e-04\n",
      "Epoch: 5372 mean train loss:  1.21203383e-04, mean val. rec. loss:  1.26819476e-04\n",
      "Epoch: 5373 mean train loss:  1.20637030e-04, mean val. rec. loss:  1.25270255e-04\n",
      "Epoch: 5374 mean train loss:  1.20424325e-04, mean val. rec. loss:  1.25723986e-04\n",
      "Epoch: 5375 mean train loss:  1.19873888e-04, mean val. rec. loss:  1.25925207e-04\n",
      "Epoch: 5376 mean train loss:  1.19737312e-04, mean val. rec. loss:  1.25139861e-04\n",
      "Epoch: 5377 mean train loss:  1.19919364e-04, mean val. rec. loss:  1.26304732e-04\n",
      "Epoch: 5378 mean train loss:  1.19818874e-04, mean val. rec. loss:  1.25937909e-04\n",
      "Epoch: 5379 mean train loss:  1.20133769e-04, mean val. rec. loss:  1.27801844e-04\n",
      "Epoch: 5380 mean train loss:  1.21139911e-04, mean val. rec. loss:  1.25222035e-04\n",
      "Epoch: 5381 mean train loss:  1.21409781e-04, mean val. rec. loss:  1.24708681e-04\n",
      "Epoch: 5382 mean train loss:  1.19693478e-04, mean val. rec. loss:  1.24824710e-04\n",
      "Epoch: 5383 mean train loss:  1.19135816e-04, mean val. rec. loss:  1.25704751e-04\n",
      "Epoch: 5384 mean train loss:  1.19402710e-04, mean val. rec. loss:  1.24563131e-04\n",
      "Epoch: 5385 mean train loss:  1.19353169e-04, mean val. rec. loss:  1.24295256e-04\n",
      "Epoch: 5386 mean train loss:  1.19644403e-04, mean val. rec. loss:  1.27072641e-04\n",
      "Epoch: 5387 mean train loss:  1.19564484e-04, mean val. rec. loss:  1.24228800e-04\n",
      "Epoch: 5388 mean train loss:  1.19125214e-04, mean val. rec. loss:  1.24645978e-04\n",
      "Epoch: 5389 mean train loss:  1.19233443e-04, mean val. rec. loss:  1.23800246e-04\n",
      "Epoch: 5390 mean train loss:  1.19216013e-04, mean val. rec. loss:  1.26534700e-04\n",
      "Epoch: 5391 mean train loss:  1.19456488e-04, mean val. rec. loss:  1.22691128e-04\n",
      "Epoch: 5392 mean train loss:  1.18713397e-04, mean val. rec. loss:  1.23132366e-04\n",
      "Epoch: 5393 mean train loss:  1.18784023e-04, mean val. rec. loss:  1.27148065e-04\n",
      "Epoch: 5394 mean train loss:  1.18405021e-04, mean val. rec. loss:  1.23589795e-04\n",
      "Epoch: 5395 mean train loss:  1.18688436e-04, mean val. rec. loss:  1.25319238e-04\n",
      "Epoch: 5396 mean train loss:  1.19024722e-04, mean val. rec. loss:  1.23935284e-04\n",
      "Epoch: 5397 mean train loss:  1.18917556e-04, mean val. rec. loss:  1.23717500e-04\n",
      "Epoch: 5398 mean train loss:  1.19717258e-04, mean val. rec. loss:  1.27624720e-04\n",
      "Epoch: 5399 mean train loss:  1.20150742e-04, mean val. rec. loss:  1.23919474e-04\n",
      "Epoch: 5400 mean train loss:  1.19465132e-04, mean val. rec. loss:  1.30045896e-04\n",
      "Epoch: 5401 mean train loss:  1.19999890e-04, mean val. rec. loss:  1.24334707e-04\n",
      "Epoch: 5402 mean train loss:  1.18286753e-04, mean val. rec. loss:  1.25038097e-04\n",
      "Epoch: 5403 mean train loss:  1.17849303e-04, mean val. rec. loss:  1.24255104e-04\n",
      "Epoch: 5404 mean train loss:  1.17924777e-04, mean val. rec. loss:  1.24332436e-04\n",
      "Epoch: 5405 mean train loss:  1.17839985e-04, mean val. rec. loss:  1.22610807e-04\n",
      "Epoch: 5406 mean train loss:  1.17590378e-04, mean val. rec. loss:  1.24169595e-04\n",
      "Epoch: 5407 mean train loss:  1.17564652e-04, mean val. rec. loss:  1.22283308e-04\n",
      "Epoch: 5408 mean train loss:  1.17481224e-04, mean val. rec. loss:  1.23037143e-04\n",
      "Epoch: 5409 mean train loss:  1.17831655e-04, mean val. rec. loss:  1.22628606e-04\n",
      "Epoch: 5410 mean train loss:  1.18105505e-04, mean val. rec. loss:  1.25573812e-04\n",
      "Epoch: 5411 mean train loss:  1.17996184e-04, mean val. rec. loss:  1.22642862e-04\n",
      "Epoch: 5412 mean train loss:  1.18082655e-04, mean val. rec. loss:  1.24625062e-04\n",
      "Epoch: 5413 mean train loss:  1.18326874e-04, mean val. rec. loss:  1.22213900e-04\n",
      "Epoch: 5414 mean train loss:  1.17376852e-04, mean val. rec. loss:  1.24093145e-04\n",
      "Epoch: 5415 mean train loss:  1.17298668e-04, mean val. rec. loss:  1.23133501e-04\n",
      "Epoch: 5416 mean train loss:  1.19048263e-04, mean val. rec. loss:  1.23875652e-04\n",
      "Epoch: 5417 mean train loss:  1.17260923e-04, mean val. rec. loss:  1.22820804e-04\n",
      "Epoch: 5418 mean train loss:  1.17383466e-04, mean val. rec. loss:  1.22703939e-04\n",
      "Epoch: 5419 mean train loss:  1.17245820e-04, mean val. rec. loss:  1.23542529e-04\n",
      "Epoch: 5420 mean train loss:  1.17288511e-04, mean val. rec. loss:  1.23931795e-04\n",
      "Epoch: 5421 mean train loss:  1.17248671e-04, mean val. rec. loss:  1.23116483e-04\n",
      "Epoch: 5422 mean train loss:  1.18753333e-04, mean val. rec. loss:  1.27107750e-04\n",
      "Epoch: 5423 mean train loss:  1.18016251e-04, mean val. rec. loss:  1.22143274e-04\n",
      "Epoch: 5424 mean train loss:  1.16648616e-04, mean val. rec. loss:  1.24944456e-04\n",
      "Epoch: 5425 mean train loss:  1.17668333e-04, mean val. rec. loss:  1.24168105e-04\n",
      "Epoch: 5426 mean train loss:  1.18257259e-04, mean val. rec. loss:  1.23325681e-04\n",
      "Epoch: 5427 mean train loss:  1.16992294e-04, mean val. rec. loss:  1.22338715e-04\n",
      "Epoch: 5428 mean train loss:  1.16774173e-04, mean val. rec. loss:  1.22013007e-04\n",
      "Epoch: 5429 mean train loss:  1.16159935e-04, mean val. rec. loss:  1.23963641e-04\n",
      "Epoch: 5430 mean train loss:  1.17930415e-04, mean val. rec. loss:  1.25055106e-04\n",
      "Epoch: 5431 mean train loss:  1.17258836e-04, mean val. rec. loss:  1.22638701e-04\n",
      "Epoch: 5432 mean train loss:  1.16675259e-04, mean val. rec. loss:  1.22406661e-04\n",
      "Epoch: 5433 mean train loss:  1.16551537e-04, mean val. rec. loss:  1.23151419e-04\n",
      "Epoch: 5434 mean train loss:  1.16388856e-04, mean val. rec. loss:  1.20742728e-04\n",
      "Epoch: 5435 mean train loss:  1.16372026e-04, mean val. rec. loss:  1.21420740e-04\n",
      "Epoch: 5436 mean train loss:  1.16382962e-04, mean val. rec. loss:  1.22179600e-04\n",
      "Epoch: 5437 mean train loss:  1.16468764e-04, mean val. rec. loss:  1.22615840e-04\n",
      "Epoch: 5438 mean train loss:  1.16394970e-04, mean val. rec. loss:  1.20857694e-04\n",
      "Epoch: 5439 mean train loss:  1.15851231e-04, mean val. rec. loss:  1.23084146e-04\n",
      "Epoch: 5440 mean train loss:  1.15583905e-04, mean val. rec. loss:  1.22296828e-04\n",
      "Epoch: 5441 mean train loss:  1.16309642e-04, mean val. rec. loss:  1.21773271e-04\n",
      "Epoch: 5442 mean train loss:  1.15922233e-04, mean val. rec. loss:  1.21352304e-04\n",
      "Epoch: 5443 mean train loss:  1.15591044e-04, mean val. rec. loss:  1.20630170e-04\n",
      "Epoch: 5444 mean train loss:  1.15498201e-04, mean val. rec. loss:  1.22196182e-04\n",
      "Epoch: 5445 mean train loss:  1.16001442e-04, mean val. rec. loss:  1.20034223e-04\n",
      "Epoch: 5446 mean train loss:  1.15644730e-04, mean val. rec. loss:  1.21903692e-04\n",
      "Epoch: 5447 mean train loss:  1.16314900e-04, mean val. rec. loss:  1.21137409e-04\n",
      "Epoch: 5448 mean train loss:  1.15745650e-04, mean val. rec. loss:  1.20959367e-04\n",
      "Epoch: 5449 mean train loss:  1.15747233e-04, mean val. rec. loss:  1.20395268e-04\n",
      "Epoch: 5450 mean train loss:  1.15329756e-04, mean val. rec. loss:  1.20590028e-04\n",
      "Epoch: 5451 mean train loss:  1.15774138e-04, mean val. rec. loss:  1.21634190e-04\n",
      "Epoch: 5452 mean train loss:  1.16333980e-04, mean val. rec. loss:  1.24793591e-04\n",
      "Epoch: 5453 mean train loss:  1.16389388e-04, mean val. rec. loss:  1.20589646e-04\n",
      "Epoch: 5454 mean train loss:  1.14875670e-04, mean val. rec. loss:  1.19669890e-04\n",
      "Epoch: 5455 mean train loss:  1.14575845e-04, mean val. rec. loss:  1.20832698e-04\n",
      "Epoch: 5456 mean train loss:  1.14506636e-04, mean val. rec. loss:  1.20781471e-04\n",
      "Epoch: 5457 mean train loss:  1.14342699e-04, mean val. rec. loss:  1.19493656e-04\n",
      "Epoch: 5458 mean train loss:  1.14980287e-04, mean val. rec. loss:  1.19946334e-04\n",
      "Epoch: 5459 mean train loss:  1.15400283e-04, mean val. rec. loss:  1.21781666e-04\n",
      "Epoch: 5460 mean train loss:  1.14944555e-04, mean val. rec. loss:  1.19253003e-04\n",
      "Epoch: 5461 mean train loss:  1.14571293e-04, mean val. rec. loss:  1.22516084e-04\n",
      "Epoch: 5462 mean train loss:  1.15474639e-04, mean val. rec. loss:  1.18788259e-04\n",
      "Epoch: 5463 mean train loss:  1.14426766e-04, mean val. rec. loss:  1.19625850e-04\n",
      "Epoch: 5464 mean train loss:  1.14477850e-04, mean val. rec. loss:  1.19884857e-04\n",
      "Epoch: 5465 mean train loss:  1.14015105e-04, mean val. rec. loss:  1.19145542e-04\n",
      "Epoch: 5466 mean train loss:  1.14033729e-04, mean val. rec. loss:  1.23303529e-04\n",
      "Epoch: 5467 mean train loss:  1.15104637e-04, mean val. rec. loss:  1.18507908e-04\n",
      "Epoch: 5468 mean train loss:  1.14650717e-04, mean val. rec. loss:  1.20146972e-04\n",
      "Epoch: 5469 mean train loss:  1.14535844e-04, mean val. rec. loss:  1.19881241e-04\n",
      "Epoch: 5470 mean train loss:  1.14419579e-04, mean val. rec. loss:  1.17668073e-04\n",
      "Epoch: 5471 mean train loss:  1.14367380e-04, mean val. rec. loss:  1.22187877e-04\n",
      "Epoch: 5472 mean train loss:  1.13699789e-04, mean val. rec. loss:  1.18454509e-04\n",
      "Epoch: 5473 mean train loss:  1.13814477e-04, mean val. rec. loss:  1.19123026e-04\n",
      "Epoch: 5474 mean train loss:  1.13858205e-04, mean val. rec. loss:  1.20865663e-04\n",
      "Epoch: 5475 mean train loss:  1.14334094e-04, mean val. rec. loss:  1.19559712e-04\n",
      "Epoch: 5476 mean train loss:  1.14577655e-04, mean val. rec. loss:  1.18920743e-04\n",
      "Epoch: 5477 mean train loss:  1.14973553e-04, mean val. rec. loss:  1.18779973e-04\n",
      "Epoch: 5478 mean train loss:  1.14188852e-04, mean val. rec. loss:  1.18534830e-04\n",
      "Epoch: 5479 mean train loss:  1.13578401e-04, mean val. rec. loss:  1.19590632e-04\n",
      "Epoch: 5480 mean train loss:  1.13246368e-04, mean val. rec. loss:  1.18022330e-04\n",
      "Epoch: 5481 mean train loss:  1.14292937e-04, mean val. rec. loss:  1.18910776e-04\n",
      "Epoch: 5482 mean train loss:  1.13223522e-04, mean val. rec. loss:  1.18717778e-04\n",
      "Epoch: 5483 mean train loss:  1.13247708e-04, mean val. rec. loss:  1.19445182e-04\n",
      "Epoch: 5484 mean train loss:  1.13442383e-04, mean val. rec. loss:  1.19160452e-04\n",
      "Epoch: 5485 mean train loss:  1.13592439e-04, mean val. rec. loss:  1.20342287e-04\n",
      "Epoch: 5486 mean train loss:  1.12743534e-04, mean val. rec. loss:  1.17839473e-04\n",
      "Epoch: 5487 mean train loss:  1.13002821e-04, mean val. rec. loss:  1.17373176e-04\n",
      "Epoch: 5488 mean train loss:  1.12951360e-04, mean val. rec. loss:  1.18667605e-04\n",
      "Epoch: 5489 mean train loss:  1.12820686e-04, mean val. rec. loss:  1.18864791e-04\n",
      "Epoch: 5490 mean train loss:  1.12664989e-04, mean val. rec. loss:  1.17949651e-04\n",
      "Epoch: 5491 mean train loss:  1.13904778e-04, mean val. rec. loss:  1.18614570e-04\n",
      "Epoch: 5492 mean train loss:  1.12013569e-04, mean val. rec. loss:  1.17574532e-04\n",
      "Epoch: 5493 mean train loss:  1.12776345e-04, mean val. rec. loss:  1.17398353e-04\n",
      "Epoch: 5494 mean train loss:  1.12799938e-04, mean val. rec. loss:  1.19126425e-04\n",
      "Epoch: 5495 mean train loss:  1.13095352e-04, mean val. rec. loss:  1.16680799e-04\n",
      "Epoch: 5496 mean train loss:  1.13288813e-04, mean val. rec. loss:  1.19477592e-04\n",
      "Epoch: 5497 mean train loss:  1.12634108e-04, mean val. rec. loss:  1.16981811e-04\n",
      "Epoch: 5498 mean train loss:  1.12712229e-04, mean val. rec. loss:  1.17709079e-04\n",
      "Epoch: 5499 mean train loss:  1.13357793e-04, mean val. rec. loss:  1.23973463e-04\n",
      "Epoch: 5500 mean train loss:  1.13006007e-04, mean val. rec. loss:  1.16877903e-04\n",
      "Epoch: 5501 mean train loss:  1.13144477e-04, mean val. rec. loss:  1.20466502e-04\n",
      "Epoch: 5502 mean train loss:  1.13982910e-04, mean val. rec. loss:  1.17559449e-04\n",
      "Epoch: 5503 mean train loss:  1.12146680e-04, mean val. rec. loss:  1.16298529e-04\n",
      "Epoch: 5504 mean train loss:  1.12601556e-04, mean val. rec. loss:  1.18329285e-04\n",
      "Epoch: 5505 mean train loss:  1.12075810e-04, mean val. rec. loss:  1.17940474e-04\n",
      "Epoch: 5506 mean train loss:  1.12765030e-04, mean val. rec. loss:  1.16701697e-04\n",
      "Epoch: 5507 mean train loss:  1.13530885e-04, mean val. rec. loss:  1.17987503e-04\n",
      "Epoch: 5508 mean train loss:  1.12080154e-04, mean val. rec. loss:  1.18414113e-04\n",
      "Epoch: 5509 mean train loss:  1.11845870e-04, mean val. rec. loss:  1.15697159e-04\n",
      "Epoch: 5510 mean train loss:  1.11341746e-04, mean val. rec. loss:  1.18310250e-04\n",
      "Epoch: 5511 mean train loss:  1.11332599e-04, mean val. rec. loss:  1.18696671e-04\n",
      "Epoch: 5512 mean train loss:  1.11642761e-04, mean val. rec. loss:  1.15762278e-04\n",
      "Epoch: 5513 mean train loss:  1.11906938e-04, mean val. rec. loss:  1.18785061e-04\n",
      "Epoch: 5514 mean train loss:  1.11561086e-04, mean val. rec. loss:  1.15604353e-04\n",
      "Epoch: 5515 mean train loss:  1.11058721e-04, mean val. rec. loss:  1.18568776e-04\n",
      "Epoch: 5516 mean train loss:  1.11495770e-04, mean val. rec. loss:  1.15027897e-04\n",
      "Epoch: 5517 mean train loss:  1.11426795e-04, mean val. rec. loss:  1.16832500e-04\n",
      "Epoch: 5518 mean train loss:  1.11701387e-04, mean val. rec. loss:  1.17303894e-04\n",
      "Epoch: 5519 mean train loss:  1.11192065e-04, mean val. rec. loss:  1.16869862e-04\n",
      "Epoch: 5520 mean train loss:  1.11490053e-04, mean val. rec. loss:  1.18248074e-04\n",
      "Epoch: 5521 mean train loss:  1.12139784e-04, mean val. rec. loss:  1.17304503e-04\n",
      "Epoch: 5522 mean train loss:  1.12110599e-04, mean val. rec. loss:  1.18252081e-04\n",
      "Epoch: 5523 mean train loss:  1.11814697e-04, mean val. rec. loss:  1.16194876e-04\n",
      "Epoch: 5524 mean train loss:  1.10861151e-04, mean val. rec. loss:  1.15297226e-04\n",
      "Epoch: 5525 mean train loss:  1.10385425e-04, mean val. rec. loss:  1.16791340e-04\n",
      "Epoch: 5526 mean train loss:  1.10745284e-04, mean val. rec. loss:  1.16022149e-04\n",
      "Epoch: 5527 mean train loss:  1.11337927e-04, mean val. rec. loss:  1.18222687e-04\n",
      "Epoch: 5528 mean train loss:  1.10890209e-04, mean val. rec. loss:  1.15519789e-04\n",
      "Epoch: 5529 mean train loss:  1.11352891e-04, mean val. rec. loss:  1.21426337e-04\n",
      "Epoch: 5530 mean train loss:  1.12470690e-04, mean val. rec. loss:  1.14922480e-04\n",
      "Epoch: 5531 mean train loss:  1.10790473e-04, mean val. rec. loss:  1.16050007e-04\n",
      "Epoch: 5532 mean train loss:  1.10512033e-04, mean val. rec. loss:  1.17243790e-04\n",
      "Epoch: 5533 mean train loss:  1.10689936e-04, mean val. rec. loss:  1.16528789e-04\n",
      "Epoch: 5534 mean train loss:  1.10529569e-04, mean val. rec. loss:  1.15115632e-04\n",
      "Epoch: 5535 mean train loss:  1.10191005e-04, mean val. rec. loss:  1.15631821e-04\n",
      "Epoch: 5536 mean train loss:  1.09864530e-04, mean val. rec. loss:  1.15596639e-04\n",
      "Epoch: 5537 mean train loss:  1.11152756e-04, mean val. rec. loss:  1.14799473e-04\n",
      "Epoch: 5538 mean train loss:  1.10529777e-04, mean val. rec. loss:  1.15484308e-04\n",
      "Epoch: 5539 mean train loss:  1.10024092e-04, mean val. rec. loss:  1.15470188e-04\n",
      "Epoch: 5540 mean train loss:  1.10167619e-04, mean val. rec. loss:  1.15415109e-04\n",
      "Epoch: 5541 mean train loss:  1.10400662e-04, mean val. rec. loss:  1.16432549e-04\n",
      "Epoch: 5542 mean train loss:  1.10099550e-04, mean val. rec. loss:  1.16061219e-04\n",
      "Epoch: 5543 mean train loss:  1.10135370e-04, mean val. rec. loss:  1.14941797e-04\n",
      "Epoch: 5544 mean train loss:  1.10225412e-04, mean val. rec. loss:  1.15137356e-04\n",
      "Epoch: 5545 mean train loss:  1.09637381e-04, mean val. rec. loss:  1.16671513e-04\n",
      "Epoch: 5546 mean train loss:  1.09411897e-04, mean val. rec. loss:  1.15995145e-04\n",
      "Epoch: 5547 mean train loss:  1.10304135e-04, mean val. rec. loss:  1.13996527e-04\n",
      "Epoch: 5548 mean train loss:  1.10063036e-04, mean val. rec. loss:  1.16444216e-04\n",
      "Epoch: 5549 mean train loss:  1.09177881e-04, mean val. rec. loss:  1.13147297e-04\n",
      "Epoch: 5550 mean train loss:  1.08933680e-04, mean val. rec. loss:  1.15552726e-04\n",
      "Epoch: 5551 mean train loss:  1.09669215e-04, mean val. rec. loss:  1.14613099e-04\n",
      "Epoch: 5552 mean train loss:  1.09260093e-04, mean val. rec. loss:  1.13479384e-04\n",
      "Epoch: 5553 mean train loss:  1.09163438e-04, mean val. rec. loss:  1.16851835e-04\n",
      "Epoch: 5554 mean train loss:  1.09347228e-04, mean val. rec. loss:  1.14283284e-04\n",
      "Epoch: 5555 mean train loss:  1.09462030e-04, mean val. rec. loss:  1.14477017e-04\n",
      "Epoch: 5556 mean train loss:  1.09371554e-04, mean val. rec. loss:  1.14012346e-04\n",
      "Epoch: 5557 mean train loss:  1.09038083e-04, mean val. rec. loss:  1.14865419e-04\n",
      "Epoch: 5558 mean train loss:  1.09329306e-04, mean val. rec. loss:  1.14577400e-04\n",
      "Epoch: 5559 mean train loss:  1.09168593e-04, mean val. rec. loss:  1.17530201e-04\n",
      "Epoch: 5560 mean train loss:  1.09756584e-04, mean val. rec. loss:  1.13007581e-04\n",
      "Epoch: 5561 mean train loss:  1.09965850e-04, mean val. rec. loss:  1.14818844e-04\n",
      "Epoch: 5562 mean train loss:  1.08936389e-04, mean val. rec. loss:  1.13757091e-04\n",
      "Epoch: 5563 mean train loss:  1.08409050e-04, mean val. rec. loss:  1.14569050e-04\n",
      "Epoch: 5564 mean train loss:  1.08095431e-04, mean val. rec. loss:  1.13195490e-04\n",
      "Epoch: 5565 mean train loss:  1.08026632e-04, mean val. rec. loss:  1.13759708e-04\n",
      "Epoch: 5566 mean train loss:  1.08148597e-04, mean val. rec. loss:  1.13226455e-04\n",
      "Epoch: 5567 mean train loss:  1.08136927e-04, mean val. rec. loss:  1.15120047e-04\n",
      "Epoch: 5568 mean train loss:  1.08775690e-04, mean val. rec. loss:  1.14038459e-04\n",
      "Epoch: 5569 mean train loss:  1.08324020e-04, mean val. rec. loss:  1.14374726e-04\n",
      "Epoch: 5570 mean train loss:  1.08111154e-04, mean val. rec. loss:  1.13133232e-04\n",
      "Epoch: 5571 mean train loss:  1.08500378e-04, mean val. rec. loss:  1.13632339e-04\n",
      "Epoch: 5572 mean train loss:  1.09230056e-04, mean val. rec. loss:  1.13113842e-04\n",
      "Epoch: 5573 mean train loss:  1.07806037e-04, mean val. rec. loss:  1.14319274e-04\n",
      "Epoch: 5574 mean train loss:  1.08445504e-04, mean val. rec. loss:  1.14172406e-04\n",
      "Epoch: 5575 mean train loss:  1.08199865e-04, mean val. rec. loss:  1.12493772e-04\n",
      "Epoch: 5576 mean train loss:  1.07932081e-04, mean val. rec. loss:  1.14187235e-04\n",
      "Epoch: 5577 mean train loss:  1.08114815e-04, mean val. rec. loss:  1.13927591e-04\n",
      "Epoch: 5578 mean train loss:  1.08184261e-04, mean val. rec. loss:  1.14586422e-04\n",
      "Epoch: 5579 mean train loss:  1.08642353e-04, mean val. rec. loss:  1.12896622e-04\n",
      "Epoch: 5580 mean train loss:  1.08042238e-04, mean val. rec. loss:  1.14008848e-04\n",
      "Epoch: 5581 mean train loss:  1.08675449e-04, mean val. rec. loss:  1.12572131e-04\n",
      "Epoch: 5582 mean train loss:  1.08373365e-04, mean val. rec. loss:  1.11995674e-04\n",
      "Epoch: 5583 mean train loss:  1.07568778e-04, mean val. rec. loss:  1.13059126e-04\n",
      "Epoch: 5584 mean train loss:  1.07387340e-04, mean val. rec. loss:  1.12284129e-04\n",
      "Epoch: 5585 mean train loss:  1.07041847e-04, mean val. rec. loss:  1.13504689e-04\n",
      "Epoch: 5586 mean train loss:  1.07346100e-04, mean val. rec. loss:  1.12092313e-04\n",
      "Epoch: 5587 mean train loss:  1.07455366e-04, mean val. rec. loss:  1.12851418e-04\n",
      "Epoch: 5588 mean train loss:  1.07610407e-04, mean val. rec. loss:  1.14618369e-04\n",
      "Epoch: 5589 mean train loss:  1.07600671e-04, mean val. rec. loss:  1.12499242e-04\n",
      "Epoch: 5590 mean train loss:  1.07362601e-04, mean val. rec. loss:  1.14710593e-04\n",
      "Epoch: 5591 mean train loss:  1.07879421e-04, mean val. rec. loss:  1.11844082e-04\n",
      "Epoch: 5592 mean train loss:  1.07563676e-04, mean val. rec. loss:  1.14177131e-04\n",
      "Epoch: 5593 mean train loss:  1.07611370e-04, mean val. rec. loss:  1.10926043e-04\n",
      "Epoch: 5594 mean train loss:  1.07301490e-04, mean val. rec. loss:  1.13190075e-04\n",
      "Epoch: 5595 mean train loss:  1.06887890e-04, mean val. rec. loss:  1.11374477e-04\n",
      "Epoch: 5596 mean train loss:  1.07045885e-04, mean val. rec. loss:  1.18651514e-04\n",
      "Epoch: 5597 mean train loss:  1.08934839e-04, mean val. rec. loss:  1.14046428e-04\n",
      "Epoch: 5598 mean train loss:  1.09157862e-04, mean val. rec. loss:  1.13324548e-04\n",
      "Epoch: 5599 mean train loss:  1.07565581e-04, mean val. rec. loss:  1.12172598e-04\n",
      "Epoch: 5600 mean train loss:  1.07094662e-04, mean val. rec. loss:  1.13206657e-04\n",
      "Epoch: 5601 mean train loss:  1.07002775e-04, mean val. rec. loss:  1.12390146e-04\n",
      "Epoch: 5602 mean train loss:  1.06485201e-04, mean val. rec. loss:  1.12856143e-04\n",
      "Epoch: 5603 mean train loss:  1.06421277e-04, mean val. rec. loss:  1.11031487e-04\n",
      "Epoch: 5604 mean train loss:  1.07193943e-04, mean val. rec. loss:  1.12624176e-04\n",
      "Epoch: 5605 mean train loss:  1.06999482e-04, mean val. rec. loss:  1.12404456e-04\n",
      "Epoch: 5606 mean train loss:  1.06419883e-04, mean val. rec. loss:  1.13054701e-04\n",
      "Epoch: 5607 mean train loss:  1.06839195e-04, mean val. rec. loss:  1.12864375e-04\n",
      "Epoch: 5608 mean train loss:  1.06857598e-04, mean val. rec. loss:  1.11408704e-04\n",
      "Epoch: 5609 mean train loss:  1.05696067e-04, mean val. rec. loss:  1.11846717e-04\n",
      "Epoch: 5610 mean train loss:  1.06608970e-04, mean val. rec. loss:  1.10730965e-04\n",
      "Epoch: 5611 mean train loss:  1.05558422e-04, mean val. rec. loss:  1.11448401e-04\n",
      "Epoch: 5612 mean train loss:  1.06364096e-04, mean val. rec. loss:  1.12147375e-04\n",
      "Epoch: 5613 mean train loss:  1.06242765e-04, mean val. rec. loss:  1.11541752e-04\n",
      "Epoch: 5614 mean train loss:  1.05996080e-04, mean val. rec. loss:  1.09795681e-04\n",
      "Epoch: 5615 mean train loss:  1.05231084e-04, mean val. rec. loss:  1.10760168e-04\n",
      "Epoch: 5616 mean train loss:  1.05319086e-04, mean val. rec. loss:  1.10887046e-04\n",
      "Epoch: 5617 mean train loss:  1.05681270e-04, mean val. rec. loss:  1.11627270e-04\n",
      "Epoch: 5618 mean train loss:  1.05401532e-04, mean val. rec. loss:  1.10682945e-04\n",
      "Epoch: 5619 mean train loss:  1.05942892e-04, mean val. rec. loss:  1.11091918e-04\n",
      "Epoch: 5620 mean train loss:  1.06725863e-04, mean val. rec. loss:  1.10061812e-04\n",
      "Epoch: 5621 mean train loss:  1.05100549e-04, mean val. rec. loss:  1.10989573e-04\n",
      "Epoch: 5622 mean train loss:  1.05268464e-04, mean val. rec. loss:  1.10786490e-04\n",
      "Epoch: 5623 mean train loss:  1.04753655e-04, mean val. rec. loss:  1.09797689e-04\n",
      "Epoch: 5624 mean train loss:  1.06174071e-04, mean val. rec. loss:  1.10701381e-04\n",
      "Epoch: 5625 mean train loss:  1.05387462e-04, mean val. rec. loss:  1.09109410e-04\n",
      "Epoch: 5626 mean train loss:  1.05333607e-04, mean val. rec. loss:  1.12780202e-04\n",
      "Epoch: 5627 mean train loss:  1.05427172e-04, mean val. rec. loss:  1.10214403e-04\n",
      "Epoch: 5628 mean train loss:  1.05434172e-04, mean val. rec. loss:  1.10079530e-04\n",
      "Epoch: 5629 mean train loss:  1.05303187e-04, mean val. rec. loss:  1.08912424e-04\n",
      "Epoch: 5630 mean train loss:  1.04985237e-04, mean val. rec. loss:  1.10844677e-04\n",
      "Epoch: 5631 mean train loss:  1.05170283e-04, mean val. rec. loss:  1.10365596e-04\n",
      "Epoch: 5632 mean train loss:  1.05321302e-04, mean val. rec. loss:  1.10036498e-04\n",
      "Epoch: 5633 mean train loss:  1.05022023e-04, mean val. rec. loss:  1.09447212e-04\n",
      "Epoch: 5634 mean train loss:  1.04548303e-04, mean val. rec. loss:  1.12282494e-04\n",
      "Epoch: 5635 mean train loss:  1.05339170e-04, mean val. rec. loss:  1.09587928e-04\n",
      "Epoch: 5636 mean train loss:  1.04679044e-04, mean val. rec. loss:  1.10204609e-04\n",
      "Epoch: 5637 mean train loss:  1.04676101e-04, mean val. rec. loss:  1.09333618e-04\n",
      "Epoch: 5638 mean train loss:  1.04507070e-04, mean val. rec. loss:  1.10503232e-04\n",
      "Epoch: 5639 mean train loss:  1.04699108e-04, mean val. rec. loss:  1.09407687e-04\n",
      "Epoch: 5640 mean train loss:  1.04954840e-04, mean val. rec. loss:  1.08933049e-04\n",
      "Epoch: 5641 mean train loss:  1.04244774e-04, mean val. rec. loss:  1.10872471e-04\n",
      "Epoch: 5642 mean train loss:  1.03872296e-04, mean val. rec. loss:  1.08383214e-04\n",
      "Epoch: 5643 mean train loss:  1.04136365e-04, mean val. rec. loss:  1.09190676e-04\n",
      "Epoch: 5644 mean train loss:  1.04798211e-04, mean val. rec. loss:  1.08957336e-04\n",
      "Epoch: 5645 mean train loss:  1.04375323e-04, mean val. rec. loss:  1.10510437e-04\n",
      "Epoch: 5646 mean train loss:  1.05101371e-04, mean val. rec. loss:  1.10181630e-04\n",
      "Epoch: 5647 mean train loss:  1.04843471e-04, mean val. rec. loss:  1.10096939e-04\n",
      "Epoch: 5648 mean train loss:  1.04192399e-04, mean val. rec. loss:  1.08910843e-04\n",
      "Epoch: 5649 mean train loss:  1.03775408e-04, mean val. rec. loss:  1.09016986e-04\n",
      "Epoch: 5650 mean train loss:  1.04499908e-04, mean val. rec. loss:  1.10438284e-04\n",
      "Epoch: 5651 mean train loss:  1.04136136e-04, mean val. rec. loss:  1.09116706e-04\n",
      "Epoch: 5652 mean train loss:  1.04148466e-04, mean val. rec. loss:  1.10640213e-04\n",
      "Epoch: 5653 mean train loss:  1.03625725e-04, mean val. rec. loss:  1.08271674e-04\n",
      "Epoch: 5654 mean train loss:  1.03892528e-04, mean val. rec. loss:  1.09032087e-04\n",
      "Epoch: 5655 mean train loss:  1.03448267e-04, mean val. rec. loss:  1.09353880e-04\n",
      "Epoch: 5656 mean train loss:  1.03730890e-04, mean val. rec. loss:  1.08733528e-04\n",
      "Epoch: 5657 mean train loss:  1.03866403e-04, mean val. rec. loss:  1.09631377e-04\n",
      "Epoch: 5658 mean train loss:  1.05642213e-04, mean val. rec. loss:  1.07530468e-04\n",
      "Epoch: 5659 mean train loss:  1.04001874e-04, mean val. rec. loss:  1.10636161e-04\n",
      "Epoch: 5660 mean train loss:  1.04118124e-04, mean val. rec. loss:  1.07765788e-04\n",
      "Epoch: 5661 mean train loss:  1.04051784e-04, mean val. rec. loss:  1.07885715e-04\n",
      "Epoch: 5662 mean train loss:  1.03727839e-04, mean val. rec. loss:  1.10348614e-04\n",
      "Epoch: 5663 mean train loss:  1.02888312e-04, mean val. rec. loss:  1.07760155e-04\n",
      "Epoch: 5664 mean train loss:  1.03187920e-04, mean val. rec. loss:  1.13092127e-04\n",
      "Epoch: 5665 mean train loss:  1.04366762e-04, mean val. rec. loss:  1.08261443e-04\n",
      "Epoch: 5666 mean train loss:  1.04251939e-04, mean val. rec. loss:  1.07543334e-04\n",
      "Epoch: 5667 mean train loss:  1.03757413e-04, mean val. rec. loss:  1.09125256e-04\n",
      "Epoch: 5668 mean train loss:  1.02985812e-04, mean val. rec. loss:  1.07815571e-04\n",
      "Epoch: 5669 mean train loss:  1.02844244e-04, mean val. rec. loss:  1.06532953e-04\n",
      "Epoch: 5670 mean train loss:  1.03849567e-04, mean val. rec. loss:  1.08879178e-04\n",
      "Epoch: 5671 mean train loss:  1.04302484e-04, mean val. rec. loss:  1.11110836e-04\n",
      "Epoch: 5672 mean train loss:  1.03719462e-04, mean val. rec. loss:  1.07600504e-04\n",
      "Epoch: 5673 mean train loss:  1.03108983e-04, mean val. rec. loss:  1.07631778e-04\n",
      "Epoch: 5674 mean train loss:  1.03141506e-04, mean val. rec. loss:  1.07731761e-04\n",
      "Epoch: 5675 mean train loss:  1.02619513e-04, mean val. rec. loss:  1.06898186e-04\n",
      "Epoch: 5676 mean train loss:  1.02583701e-04, mean val. rec. loss:  1.08660403e-04\n",
      "Epoch: 5677 mean train loss:  1.02712491e-04, mean val. rec. loss:  1.07231773e-04\n",
      "Epoch: 5678 mean train loss:  1.02711368e-04, mean val. rec. loss:  1.08393100e-04\n",
      "Epoch: 5679 mean train loss:  1.02899539e-04, mean val. rec. loss:  1.06877470e-04\n",
      "Epoch: 5680 mean train loss:  1.02421987e-04, mean val. rec. loss:  1.08608876e-04\n",
      "Epoch: 5681 mean train loss:  1.02348111e-04, mean val. rec. loss:  1.07531540e-04\n",
      "Epoch: 5682 mean train loss:  1.02615477e-04, mean val. rec. loss:  1.07804468e-04\n",
      "Epoch: 5683 mean train loss:  1.01865123e-04, mean val. rec. loss:  1.08450906e-04\n",
      "Epoch: 5684 mean train loss:  1.02696769e-04, mean val. rec. loss:  1.07928347e-04\n",
      "Epoch: 5685 mean train loss:  1.02219384e-04, mean val. rec. loss:  1.05938343e-04\n",
      "Epoch: 5686 mean train loss:  1.02481127e-04, mean val. rec. loss:  1.08752454e-04\n",
      "Epoch: 5687 mean train loss:  1.02307331e-04, mean val. rec. loss:  1.08629810e-04\n",
      "Epoch: 5688 mean train loss:  1.02049494e-04, mean val. rec. loss:  1.06589769e-04\n",
      "Epoch: 5689 mean train loss:  1.02094618e-04, mean val. rec. loss:  1.08916476e-04\n",
      "Epoch: 5690 mean train loss:  1.01857934e-04, mean val. rec. loss:  1.06358537e-04\n",
      "Epoch: 5691 mean train loss:  1.01865364e-04, mean val. rec. loss:  1.07626163e-04\n",
      "Epoch: 5692 mean train loss:  1.02343118e-04, mean val. rec. loss:  1.09969915e-04\n",
      "Epoch: 5693 mean train loss:  1.02503155e-04, mean val. rec. loss:  1.06294544e-04\n",
      "Epoch: 5694 mean train loss:  1.03380976e-04, mean val. rec. loss:  1.09092391e-04\n",
      "Epoch: 5695 mean train loss:  1.02265388e-04, mean val. rec. loss:  1.07015288e-04\n",
      "Epoch: 5696 mean train loss:  1.01818803e-04, mean val. rec. loss:  1.06456040e-04\n",
      "Epoch: 5697 mean train loss:  1.02667178e-04, mean val. rec. loss:  1.07471082e-04\n",
      "Epoch: 5698 mean train loss:  1.01993817e-04, mean val. rec. loss:  1.07779408e-04\n",
      "Epoch: 5699 mean train loss:  1.02499549e-04, mean val. rec. loss:  1.08469732e-04\n",
      "Epoch: 5700 mean train loss:  1.01914048e-04, mean val. rec. loss:  1.07844665e-04\n",
      "Epoch: 5701 mean train loss:  1.01942890e-04, mean val. rec. loss:  1.06995244e-04\n",
      "Epoch: 5702 mean train loss:  1.02504415e-04, mean val. rec. loss:  1.08554560e-04\n",
      "Epoch: 5703 mean train loss:  1.01840987e-04, mean val. rec. loss:  1.06788472e-04\n",
      "Epoch: 5704 mean train loss:  1.01046873e-04, mean val. rec. loss:  1.05547760e-04\n",
      "Epoch: 5705 mean train loss:  1.00725772e-04, mean val. rec. loss:  1.06631256e-04\n",
      "Epoch: 5706 mean train loss:  1.00803120e-04, mean val. rec. loss:  1.05559517e-04\n",
      "Epoch: 5707 mean train loss:  1.01483377e-04, mean val. rec. loss:  1.05542335e-04\n",
      "Epoch: 5708 mean train loss:  1.01736559e-04, mean val. rec. loss:  1.07410041e-04\n",
      "Epoch: 5709 mean train loss:  1.01144309e-04, mean val. rec. loss:  1.08402967e-04\n",
      "Epoch: 5710 mean train loss:  1.01212049e-04, mean val. rec. loss:  1.06221528e-04\n",
      "Epoch: 5711 mean train loss:  1.01442109e-04, mean val. rec. loss:  1.06710177e-04\n",
      "Epoch: 5712 mean train loss:  1.01061394e-04, mean val. rec. loss:  1.05020931e-04\n",
      "Epoch: 5713 mean train loss:  1.00861708e-04, mean val. rec. loss:  1.06130241e-04\n",
      "Epoch: 5714 mean train loss:  1.01008268e-04, mean val. rec. loss:  1.05431294e-04\n",
      "Epoch: 5715 mean train loss:  1.00378829e-04, mean val. rec. loss:  1.06502379e-04\n",
      "Epoch: 5716 mean train loss:  1.01076759e-04, mean val. rec. loss:  1.08336439e-04\n",
      "Epoch: 5717 mean train loss:  1.00979090e-04, mean val. rec. loss:  1.07433783e-04\n",
      "Epoch: 5718 mean train loss:  1.01540780e-04, mean val. rec. loss:  1.10689096e-04\n",
      "Epoch: 5719 mean train loss:  1.02186572e-04, mean val. rec. loss:  1.07751405e-04\n",
      "Epoch: 5720 mean train loss:  1.01292162e-04, mean val. rec. loss:  1.09044553e-04\n",
      "Epoch: 5721 mean train loss:  1.00927751e-04, mean val. rec. loss:  1.04924010e-04\n",
      "Epoch: 5722 mean train loss:  1.01185130e-04, mean val. rec. loss:  1.05091148e-04\n",
      "Epoch: 5723 mean train loss:  1.01525371e-04, mean val. rec. loss:  1.07595334e-04\n",
      "Epoch: 5724 mean train loss:  1.00324721e-04, mean val. rec. loss:  1.05159948e-04\n",
      "Epoch: 5725 mean train loss:  1.00126771e-04, mean val. rec. loss:  1.06258418e-04\n",
      "Epoch: 5726 mean train loss:  1.00527655e-04, mean val. rec. loss:  1.06743178e-04\n",
      "Epoch: 5727 mean train loss:  1.01087911e-04, mean val. rec. loss:  1.04850031e-04\n",
      "Epoch: 5728 mean train loss:  1.00910658e-04, mean val. rec. loss:  1.06221238e-04\n",
      "Epoch: 5729 mean train loss:  1.00653231e-04, mean val. rec. loss:  1.05631506e-04\n",
      "Epoch: 5730 mean train loss:  9.96777938e-05, mean val. rec. loss:  1.05129528e-04\n",
      "Epoch: 5731 mean train loss:  9.99083206e-05, mean val. rec. loss:  1.04800375e-04\n",
      "Epoch: 5732 mean train loss:  1.00012324e-04, mean val. rec. loss:  1.07017732e-04\n",
      "Epoch: 5733 mean train loss:  1.01073382e-04, mean val. rec. loss:  1.04450607e-04\n",
      "Epoch: 5734 mean train loss:  1.00344359e-04, mean val. rec. loss:  1.06250004e-04\n",
      "Epoch: 5735 mean train loss:  1.00657657e-04, mean val. rec. loss:  1.09072693e-04\n",
      "Epoch: 5736 mean train loss:  1.01407647e-04, mean val. rec. loss:  1.06007224e-04\n",
      "Epoch: 5737 mean train loss:  1.00091730e-04, mean val. rec. loss:  1.04637589e-04\n",
      "Epoch: 5738 mean train loss:  9.99331888e-05, mean val. rec. loss:  1.05661190e-04\n",
      "Epoch: 5739 mean train loss:  9.94778028e-05, mean val. rec. loss:  1.03716334e-04\n",
      "Epoch: 5740 mean train loss:  1.00008756e-04, mean val. rec. loss:  1.09589145e-04\n",
      "Epoch: 5741 mean train loss:  1.02066370e-04, mean val. rec. loss:  1.04247161e-04\n",
      "Epoch: 5742 mean train loss:  1.01297648e-04, mean val. rec. loss:  1.05102433e-04\n",
      "Epoch: 5743 mean train loss:  1.02783771e-04, mean val. rec. loss:  1.11317653e-04\n",
      "Epoch: 5744 mean train loss:  1.01368465e-04, mean val. rec. loss:  1.04377637e-04\n",
      "Epoch: 5745 mean train loss:  9.94485656e-05, mean val. rec. loss:  1.04753201e-04\n",
      "Epoch: 5746 mean train loss:  9.90877071e-05, mean val. rec. loss:  1.05285708e-04\n",
      "Epoch: 5747 mean train loss:  9.88201094e-05, mean val. rec. loss:  1.03532005e-04\n",
      "Epoch: 5748 mean train loss:  9.86062867e-05, mean val. rec. loss:  1.05308223e-04\n",
      "Epoch: 5749 mean train loss:  9.89481588e-05, mean val. rec. loss:  1.03658892e-04\n",
      "Epoch: 5750 mean train loss:  9.87620740e-05, mean val. rec. loss:  1.04788309e-04\n",
      "Epoch: 5751 mean train loss:  9.86330147e-05, mean val. rec. loss:  1.04899341e-04\n",
      "Epoch: 5752 mean train loss:  9.92455538e-05, mean val. rec. loss:  1.04473849e-04\n",
      "Epoch: 5753 mean train loss:  9.91355190e-05, mean val. rec. loss:  1.04508413e-04\n",
      "Epoch: 5754 mean train loss:  9.84910546e-05, mean val. rec. loss:  1.04804819e-04\n",
      "Epoch: 5755 mean train loss:  9.90042923e-05, mean val. rec. loss:  1.04186657e-04\n",
      "Epoch: 5756 mean train loss:  9.98356251e-05, mean val. rec. loss:  1.04771200e-04\n",
      "Epoch: 5757 mean train loss:  9.89753605e-05, mean val. rec. loss:  1.06322048e-04\n",
      "Epoch: 5758 mean train loss:  9.90989996e-05, mean val. rec. loss:  1.02110589e-04\n",
      "Epoch: 5759 mean train loss:  9.84011075e-05, mean val. rec. loss:  1.04102056e-04\n",
      "Epoch: 5760 mean train loss:  9.87416497e-05, mean val. rec. loss:  1.03424662e-04\n",
      "Epoch: 5761 mean train loss:  9.81332505e-05, mean val. rec. loss:  1.03781272e-04\n",
      "Epoch: 5762 mean train loss:  9.82105036e-05, mean val. rec. loss:  1.03217245e-04\n",
      "Epoch: 5763 mean train loss:  9.85469800e-05, mean val. rec. loss:  1.03462088e-04\n",
      "Epoch: 5764 mean train loss:  9.82258005e-05, mean val. rec. loss:  1.02800949e-04\n",
      "Epoch: 5765 mean train loss:  9.89478473e-05, mean val. rec. loss:  1.06580664e-04\n",
      "Epoch: 5766 mean train loss:  9.91569388e-05, mean val. rec. loss:  1.04871965e-04\n",
      "Epoch: 5767 mean train loss:  1.00137494e-04, mean val. rec. loss:  1.08507021e-04\n",
      "Epoch: 5768 mean train loss:  9.94951272e-05, mean val. rec. loss:  1.03750507e-04\n",
      "Epoch: 5769 mean train loss:  9.92717519e-05, mean val. rec. loss:  1.04490631e-04\n",
      "Epoch: 5770 mean train loss:  9.92415272e-05, mean val. rec. loss:  1.02514383e-04\n",
      "Epoch: 5771 mean train loss:  9.86537859e-05, mean val. rec. loss:  1.03295049e-04\n",
      "Epoch: 5772 mean train loss:  9.80376187e-05, mean val. rec. loss:  1.03037959e-04\n",
      "Epoch: 5773 mean train loss:  9.77395034e-05, mean val. rec. loss:  1.03890096e-04\n",
      "Epoch: 5774 mean train loss:  9.88131006e-05, mean val. rec. loss:  1.03203989e-04\n",
      "Epoch: 5775 mean train loss:  9.81346179e-05, mean val. rec. loss:  1.02446419e-04\n",
      "Epoch: 5776 mean train loss:  9.77607328e-05, mean val. rec. loss:  1.03012690e-04\n",
      "Epoch: 5777 mean train loss:  9.78928123e-05, mean val. rec. loss:  1.02074853e-04\n",
      "Epoch: 5778 mean train loss:  9.89636559e-05, mean val. rec. loss:  1.04803383e-04\n",
      "Epoch: 5779 mean train loss:  9.78159788e-05, mean val. rec. loss:  1.01503139e-04\n",
      "Epoch: 5780 mean train loss:  9.74861859e-05, mean val. rec. loss:  1.03513170e-04\n",
      "Epoch: 5781 mean train loss:  9.78084918e-05, mean val. rec. loss:  1.03507291e-04\n",
      "Epoch: 5782 mean train loss:  9.78435557e-05, mean val. rec. loss:  1.01453393e-04\n",
      "Epoch: 5783 mean train loss:  9.71469048e-05, mean val. rec. loss:  1.03123377e-04\n",
      "Epoch: 5784 mean train loss:  9.73273033e-05, mean val. rec. loss:  1.03175858e-04\n",
      "Epoch: 5785 mean train loss:  9.76613189e-05, mean val. rec. loss:  1.01948766e-04\n",
      "Epoch: 5786 mean train loss:  9.69934219e-05, mean val. rec. loss:  1.02398018e-04\n",
      "Epoch: 5787 mean train loss:  9.68517534e-05, mean val. rec. loss:  1.02211653e-04\n",
      "Epoch: 5788 mean train loss:  9.69978909e-05, mean val. rec. loss:  1.02265079e-04\n",
      "Epoch: 5789 mean train loss:  9.78783080e-05, mean val. rec. loss:  1.03317946e-04\n",
      "Epoch: 5790 mean train loss:  9.73855775e-05, mean val. rec. loss:  1.01667879e-04\n",
      "Epoch: 5791 mean train loss:  9.73490797e-05, mean val. rec. loss:  1.06900522e-04\n",
      "Epoch: 5792 mean train loss:  9.88430695e-05, mean val. rec. loss:  1.03311604e-04\n",
      "Epoch: 5793 mean train loss:  9.80254700e-05, mean val. rec. loss:  1.01227350e-04\n",
      "Epoch: 5794 mean train loss:  9.70848740e-05, mean val. rec. loss:  1.03230284e-04\n",
      "Epoch: 5795 mean train loss:  9.67295262e-05, mean val. rec. loss:  1.02225019e-04\n",
      "Epoch: 5796 mean train loss:  9.65912850e-05, mean val. rec. loss:  1.03425671e-04\n",
      "Epoch: 5797 mean train loss:  9.69058026e-05, mean val. rec. loss:  1.01192913e-04\n",
      "Epoch: 5798 mean train loss:  9.65344220e-05, mean val. rec. loss:  1.01254708e-04\n",
      "Epoch: 5799 mean train loss:  9.63364836e-05, mean val. rec. loss:  1.01596917e-04\n",
      "Epoch: 5800 mean train loss:  9.66566993e-05, mean val. rec. loss:  1.02132014e-04\n",
      "Epoch: 5801 mean train loss:  9.66025701e-05, mean val. rec. loss:  1.00190665e-04\n",
      "Epoch: 5802 mean train loss:  9.67917804e-05, mean val. rec. loss:  1.03615897e-04\n",
      "Epoch: 5803 mean train loss:  9.70509979e-05, mean val. rec. loss:  1.02676197e-04\n",
      "Epoch: 5804 mean train loss:  9.74544619e-05, mean val. rec. loss:  1.05856904e-04\n",
      "Epoch: 5805 mean train loss:  9.82313475e-05, mean val. rec. loss:  1.02021218e-04\n",
      "Epoch: 5806 mean train loss:  9.71175789e-05, mean val. rec. loss:  1.01865401e-04\n",
      "Epoch: 5807 mean train loss:  9.71003671e-05, mean val. rec. loss:  1.01680527e-04\n",
      "Epoch: 5808 mean train loss:  9.64660881e-05, mean val. rec. loss:  1.02333279e-04\n",
      "Epoch: 5809 mean train loss:  9.58220632e-05, mean val. rec. loss:  1.00861463e-04\n",
      "Epoch: 5810 mean train loss:  9.63715873e-05, mean val. rec. loss:  1.01969146e-04\n",
      "Epoch: 5811 mean train loss:  9.60666343e-05, mean val. rec. loss:  1.01028174e-04\n",
      "Epoch: 5812 mean train loss:  9.55724773e-05, mean val. rec. loss:  1.01304072e-04\n",
      "Epoch: 5813 mean train loss:  9.61372909e-05, mean val. rec. loss:  1.00607952e-04\n",
      "Epoch: 5814 mean train loss:  9.62530273e-05, mean val. rec. loss:  1.00879089e-04\n",
      "Epoch: 5815 mean train loss:  9.59843652e-05, mean val. rec. loss:  1.00691744e-04\n",
      "Epoch: 5816 mean train loss:  9.58694537e-05, mean val. rec. loss:  1.01401184e-04\n",
      "Epoch: 5817 mean train loss:  9.56665295e-05, mean val. rec. loss:  1.00183969e-04\n",
      "Epoch: 5818 mean train loss:  9.58940280e-05, mean val. rec. loss:  1.00971958e-04\n",
      "Epoch: 5819 mean train loss:  9.56205672e-05, mean val. rec. loss:  9.96917668e-05\n",
      "Epoch: 5820 mean train loss:  9.61904575e-05, mean val. rec. loss:  1.02361764e-04\n",
      "Epoch: 5821 mean train loss:  9.60269900e-05, mean val. rec. loss:  1.00975965e-04\n",
      "Epoch: 5822 mean train loss:  9.57094073e-05, mean val. rec. loss:  1.00507751e-04\n",
      "Epoch: 5823 mean train loss:  9.59667360e-05, mean val. rec. loss:  1.02809262e-04\n",
      "Epoch: 5824 mean train loss:  9.62502510e-05, mean val. rec. loss:  1.00961764e-04\n",
      "Epoch: 5825 mean train loss:  9.67890887e-05, mean val. rec. loss:  1.02601064e-04\n",
      "Epoch: 5826 mean train loss:  9.60040316e-05, mean val. rec. loss:  1.01324371e-04\n",
      "Epoch: 5827 mean train loss:  9.60768908e-05, mean val. rec. loss:  1.00648176e-04\n",
      "Epoch: 5828 mean train loss:  9.50834334e-05, mean val. rec. loss:  9.90519164e-05\n",
      "Epoch: 5829 mean train loss:  9.54864675e-05, mean val. rec. loss:  1.01394879e-04\n",
      "Epoch: 5830 mean train loss:  9.57595491e-05, mean val. rec. loss:  9.97825094e-05\n",
      "Epoch: 5831 mean train loss:  9.57874678e-05, mean val. rec. loss:  9.92510922e-05\n",
      "Epoch: 5832 mean train loss:  9.51255038e-05, mean val. rec. loss:  1.04155119e-04\n",
      "Epoch: 5833 mean train loss:  9.63329448e-05, mean val. rec. loss:  9.89270101e-05\n",
      "Epoch: 5834 mean train loss:  9.51300439e-05, mean val. rec. loss:  1.00689954e-04\n",
      "Epoch: 5835 mean train loss:  9.52056499e-05, mean val. rec. loss:  1.00262827e-04\n",
      "Epoch: 5836 mean train loss:  9.56465134e-05, mean val. rec. loss:  9.92737347e-05\n",
      "Epoch: 5837 mean train loss:  9.56669536e-05, mean val. rec. loss:  1.00706772e-04\n",
      "Epoch: 5838 mean train loss:  9.52997100e-05, mean val. rec. loss:  9.88661334e-05\n",
      "Epoch: 5839 mean train loss:  9.49391051e-05, mean val. rec. loss:  1.00861381e-04\n",
      "Epoch: 5840 mean train loss:  9.47830449e-05, mean val. rec. loss:  1.00587754e-04\n",
      "Epoch: 5841 mean train loss:  9.54808438e-05, mean val. rec. loss:  1.01191024e-04\n",
      "Epoch: 5842 mean train loss:  9.57600188e-05, mean val. rec. loss:  9.97901962e-05\n",
      "Epoch: 5843 mean train loss:  9.50851295e-05, mean val. rec. loss:  1.00046905e-04\n",
      "Epoch: 5844 mean train loss:  9.56993213e-05, mean val. rec. loss:  9.92193273e-05\n",
      "Epoch: 5845 mean train loss:  9.57971023e-05, mean val. rec. loss:  1.00343547e-04\n",
      "Epoch: 5846 mean train loss:  9.50141050e-05, mean val. rec. loss:  1.01144921e-04\n",
      "Epoch: 5847 mean train loss:  9.46753737e-05, mean val. rec. loss:  9.84511906e-05\n",
      "Epoch: 5848 mean train loss:  9.43705458e-05, mean val. rec. loss:  9.85235521e-05\n",
      "Epoch: 5849 mean train loss:  9.45215372e-05, mean val. rec. loss:  1.00361865e-04\n",
      "Epoch: 5850 mean train loss:  9.46681568e-05, mean val. rec. loss:  9.88988160e-05\n",
      "Epoch: 5851 mean train loss:  9.45983593e-05, mean val. rec. loss:  9.84380431e-05\n",
      "Epoch: 5852 mean train loss:  9.42026610e-05, mean val. rec. loss:  1.00569173e-04\n",
      "Epoch: 5853 mean train loss:  9.43345011e-05, mean val. rec. loss:  9.83735047e-05\n",
      "Epoch: 5854 mean train loss:  9.40108398e-05, mean val. rec. loss:  9.96349334e-05\n",
      "Epoch: 5855 mean train loss:  9.48628163e-05, mean val. rec. loss:  1.00465573e-04\n",
      "Epoch: 5856 mean train loss:  9.50494868e-05, mean val. rec. loss:  9.92208083e-05\n",
      "Epoch: 5857 mean train loss:  9.45020749e-05, mean val. rec. loss:  9.90463284e-05\n",
      "Epoch: 5858 mean train loss:  9.45007831e-05, mean val. rec. loss:  9.87349850e-05\n",
      "Epoch: 5859 mean train loss:  9.40016374e-05, mean val. rec. loss:  9.95257915e-05\n",
      "Epoch: 5860 mean train loss:  9.39118188e-05, mean val. rec. loss:  9.90303642e-05\n",
      "Epoch: 5861 mean train loss:  9.39647416e-05, mean val. rec. loss:  9.84009174e-05\n",
      "Epoch: 5862 mean train loss:  9.38070615e-05, mean val. rec. loss:  9.98121573e-05\n",
      "Epoch: 5863 mean train loss:  9.36525344e-05, mean val. rec. loss:  9.78397433e-05\n",
      "Epoch: 5864 mean train loss:  9.34816410e-05, mean val. rec. loss:  9.78086053e-05\n",
      "Epoch: 5865 mean train loss:  9.32182234e-05, mean val. rec. loss:  9.90933580e-05\n",
      "Epoch: 5866 mean train loss:  9.33341014e-05, mean val. rec. loss:  9.89755752e-05\n",
      "Epoch: 5867 mean train loss:  9.36089235e-05, mean val. rec. loss:  9.74146422e-05\n",
      "Epoch: 5868 mean train loss:  9.33084611e-05, mean val. rec. loss:  9.86831126e-05\n",
      "Epoch: 5869 mean train loss:  9.35418330e-05, mean val. rec. loss:  9.79150214e-05\n",
      "Epoch: 5870 mean train loss:  9.33335221e-05, mean val. rec. loss:  9.86836033e-05\n",
      "Epoch: 5871 mean train loss:  9.33849933e-05, mean val. rec. loss:  9.83989184e-05\n",
      "Epoch: 5872 mean train loss:  9.38359325e-05, mean val. rec. loss:  9.83873700e-05\n",
      "Epoch: 5873 mean train loss:  9.35509233e-05, mean val. rec. loss:  9.87640786e-05\n",
      "Epoch: 5874 mean train loss:  9.31730821e-05, mean val. rec. loss:  9.94704209e-05\n",
      "Epoch: 5875 mean train loss:  9.32000297e-05, mean val. rec. loss:  9.72833939e-05\n",
      "Epoch: 5876 mean train loss:  9.33590368e-05, mean val. rec. loss:  9.73570456e-05\n",
      "Epoch: 5877 mean train loss:  9.34632693e-05, mean val. rec. loss:  1.01074713e-04\n",
      "Epoch: 5878 mean train loss:  9.45315253e-05, mean val. rec. loss:  9.79372459e-05\n",
      "Epoch: 5879 mean train loss:  9.35123894e-05, mean val. rec. loss:  9.73288424e-05\n",
      "Epoch: 5880 mean train loss:  9.34020584e-05, mean val. rec. loss:  9.89960189e-05\n",
      "Epoch: 5881 mean train loss:  9.38734879e-05, mean val. rec. loss:  9.89511246e-05\n",
      "Epoch: 5882 mean train loss:  9.46879528e-05, mean val. rec. loss:  9.70739235e-05\n",
      "Epoch: 5883 mean train loss:  9.40555040e-05, mean val. rec. loss:  9.91524174e-05\n",
      "Epoch: 5884 mean train loss:  9.30783675e-05, mean val. rec. loss:  9.74243189e-05\n",
      "Epoch: 5885 mean train loss:  9.29292428e-05, mean val. rec. loss:  9.87177215e-05\n",
      "Epoch: 5886 mean train loss:  9.27477787e-05, mean val. rec. loss:  9.86164845e-05\n",
      "Epoch: 5887 mean train loss:  9.39481171e-05, mean val. rec. loss:  9.78653024e-05\n",
      "Epoch: 5888 mean train loss:  9.29509179e-05, mean val. rec. loss:  9.83489632e-05\n",
      "Epoch: 5889 mean train loss:  9.35683763e-05, mean val. rec. loss:  1.00760889e-04\n",
      "Epoch: 5890 mean train loss:  9.47187865e-05, mean val. rec. loss:  9.74456348e-05\n",
      "Epoch: 5891 mean train loss:  9.29587864e-05, mean val. rec. loss:  9.80358026e-05\n",
      "Epoch: 5892 mean train loss:  9.22829464e-05, mean val. rec. loss:  9.69144266e-05\n",
      "Epoch: 5893 mean train loss:  9.31370647e-05, mean val. rec. loss:  9.80344487e-05\n",
      "Epoch: 5894 mean train loss:  9.25484103e-05, mean val. rec. loss:  9.74892843e-05\n",
      "Epoch: 5895 mean train loss:  9.24286950e-05, mean val. rec. loss:  9.78465215e-05\n",
      "Epoch: 5896 mean train loss:  9.29435253e-05, mean val. rec. loss:  9.73741001e-05\n",
      "Epoch: 5897 mean train loss:  9.21180176e-05, mean val. rec. loss:  9.80957161e-05\n",
      "Epoch: 5898 mean train loss:  9.28150665e-05, mean val. rec. loss:  9.79217451e-05\n",
      "Epoch: 5899 mean train loss:  9.35005438e-05, mean val. rec. loss:  9.84440671e-05\n",
      "Epoch: 5900 mean train loss:  9.29791129e-05, mean val. rec. loss:  9.68274093e-05\n",
      "Epoch: 5901 mean train loss:  9.27602026e-05, mean val. rec. loss:  9.80978332e-05\n",
      "Epoch: 5902 mean train loss:  9.22932456e-05, mean val. rec. loss:  9.88327512e-05\n",
      "Epoch: 5903 mean train loss:  9.33280808e-05, mean val. rec. loss:  9.81238466e-05\n",
      "Epoch: 5904 mean train loss:  9.26139440e-05, mean val. rec. loss:  9.69370782e-05\n",
      "Epoch: 5905 mean train loss:  9.22835997e-05, mean val. rec. loss:  9.78333558e-05\n",
      "Epoch: 5906 mean train loss:  9.15523688e-05, mean val. rec. loss:  9.71305843e-05\n",
      "Epoch: 5907 mean train loss:  9.26731598e-05, mean val. rec. loss:  9.72699283e-05\n",
      "Epoch: 5908 mean train loss:  9.21166235e-05, mean val. rec. loss:  9.63334085e-05\n",
      "Epoch: 5909 mean train loss:  9.28978513e-05, mean val. rec. loss:  9.80715381e-05\n",
      "Epoch: 5910 mean train loss:  9.36060977e-05, mean val. rec. loss:  9.89563945e-05\n",
      "Epoch: 5911 mean train loss:  9.27844716e-05, mean val. rec. loss:  9.54958451e-05\n",
      "Epoch: 5912 mean train loss:  9.20750778e-05, mean val. rec. loss:  9.68003418e-05\n",
      "Epoch: 5913 mean train loss:  9.13926947e-05, mean val. rec. loss:  9.68015594e-05\n",
      "Epoch: 5914 mean train loss:  9.25842060e-05, mean val. rec. loss:  9.65385720e-05\n",
      "Epoch: 5915 mean train loss:  9.12756756e-05, mean val. rec. loss:  9.65868736e-05\n",
      "Epoch: 5916 mean train loss:  9.16934841e-05, mean val. rec. loss:  9.71998929e-05\n",
      "Epoch: 5917 mean train loss:  9.15273646e-05, mean val. rec. loss:  9.67325052e-05\n",
      "Epoch: 5918 mean train loss:  9.12027822e-05, mean val. rec. loss:  9.59982596e-05\n",
      "Epoch: 5919 mean train loss:  9.13245313e-05, mean val. rec. loss:  9.57971666e-05\n",
      "Epoch: 5920 mean train loss:  9.16758628e-05, mean val. rec. loss:  9.91766046e-05\n",
      "Epoch: 5921 mean train loss:  9.15285291e-05, mean val. rec. loss:  9.60958985e-05\n",
      "Epoch: 5922 mean train loss:  9.21459027e-05, mean val. rec. loss:  9.72942063e-05\n",
      "Epoch: 5923 mean train loss:  9.13640659e-05, mean val. rec. loss:  9.88327148e-05\n",
      "Epoch: 5924 mean train loss:  9.25413589e-05, mean val. rec. loss:  9.61357137e-05\n",
      "Epoch: 5925 mean train loss:  9.20970083e-05, mean val. rec. loss:  9.89366050e-05\n",
      "Epoch: 5926 mean train loss:  9.19957016e-05, mean val. rec. loss:  9.60002313e-05\n",
      "Epoch: 5927 mean train loss:  9.17721871e-05, mean val. rec. loss:  9.90837086e-05\n",
      "Epoch: 5928 mean train loss:  9.24778948e-05, mean val. rec. loss:  9.68282361e-05\n",
      "Epoch: 5929 mean train loss:  9.20141291e-05, mean val. rec. loss:  9.61245106e-05\n",
      "Epoch: 5930 mean train loss:  9.17655132e-05, mean val. rec. loss:  9.58773512e-05\n",
      "Epoch: 5931 mean train loss:  9.17974732e-05, mean val. rec. loss:  9.65992034e-05\n",
      "Epoch: 5932 mean train loss:  9.12597902e-05, mean val. rec. loss:  9.70550063e-05\n",
      "Epoch: 5933 mean train loss:  9.16665559e-05, mean val. rec. loss:  9.63965567e-05\n",
      "Epoch: 5934 mean train loss:  9.19176900e-05, mean val. rec. loss:  9.71189905e-05\n",
      "Epoch: 5935 mean train loss:  9.07771815e-05, mean val. rec. loss:  9.56012163e-05\n",
      "Epoch: 5936 mean train loss:  9.08209644e-05, mean val. rec. loss:  9.58362095e-05\n",
      "Epoch: 5937 mean train loss:  9.05761727e-05, mean val. rec. loss:  9.49264300e-05\n",
      "Epoch: 5938 mean train loss:  9.10366253e-05, mean val. rec. loss:  9.69178611e-05\n",
      "Epoch: 5939 mean train loss:  9.07438892e-05, mean val. rec. loss:  9.62468364e-05\n",
      "Epoch: 5940 mean train loss:  9.14382909e-05, mean val. rec. loss:  9.50824378e-05\n",
      "Epoch: 5941 mean train loss:  9.06916488e-05, mean val. rec. loss:  9.62492896e-05\n",
      "Epoch: 5942 mean train loss:  9.13620270e-05, mean val. rec. loss:  9.65945241e-05\n",
      "Epoch: 5943 mean train loss:  9.18601265e-05, mean val. rec. loss:  9.41743211e-05\n",
      "Epoch: 5944 mean train loss:  9.00856995e-05, mean val. rec. loss:  9.68126807e-05\n",
      "Epoch: 5945 mean train loss:  9.05226450e-05, mean val. rec. loss:  9.43771223e-05\n",
      "Epoch: 5946 mean train loss:  9.03501126e-05, mean val. rec. loss:  9.67577645e-05\n",
      "Epoch: 5947 mean train loss:  9.01035851e-05, mean val. rec. loss:  9.31038635e-05\n",
      "Epoch: 5948 mean train loss:  9.03789541e-05, mean val. rec. loss:  9.51578977e-05\n",
      "Epoch: 5949 mean train loss:  9.01264423e-05, mean val. rec. loss:  9.47196400e-05\n",
      "Epoch: 5950 mean train loss:  8.97170714e-05, mean val. rec. loss:  9.47989160e-05\n",
      "Epoch: 5951 mean train loss:  9.04986887e-05, mean val. rec. loss:  9.53445893e-05\n",
      "Epoch: 5952 mean train loss:  9.03746511e-05, mean val. rec. loss:  9.36454571e-05\n",
      "Epoch: 5953 mean train loss:  8.94231994e-05, mean val. rec. loss:  9.51119676e-05\n",
      "Epoch: 5954 mean train loss:  8.97702455e-05, mean val. rec. loss:  9.47983527e-05\n",
      "Epoch: 5955 mean train loss:  8.98575567e-05, mean val. rec. loss:  9.49018612e-05\n",
      "Epoch: 5956 mean train loss:  8.99295244e-05, mean val. rec. loss:  9.62202869e-05\n",
      "Epoch: 5957 mean train loss:  8.99380200e-05, mean val. rec. loss:  9.41012872e-05\n",
      "Epoch: 5958 mean train loss:  8.97032352e-05, mean val. rec. loss:  9.52892005e-05\n",
      "Epoch: 5959 mean train loss:  9.01752560e-05, mean val. rec. loss:  9.47345502e-05\n",
      "Epoch: 5960 mean train loss:  8.97260958e-05, mean val. rec. loss:  9.43740694e-05\n",
      "Epoch: 5961 mean train loss:  8.99459539e-05, mean val. rec. loss:  9.36695443e-05\n",
      "Epoch: 5962 mean train loss:  9.00173422e-05, mean val. rec. loss:  9.51266689e-05\n",
      "Epoch: 5963 mean train loss:  9.00298491e-05, mean val. rec. loss:  9.55268741e-05\n",
      "Epoch: 5964 mean train loss:  9.05600560e-05, mean val. rec. loss:  9.46640423e-05\n",
      "Epoch: 5965 mean train loss:  9.06594203e-05, mean val. rec. loss:  9.59564273e-05\n",
      "Epoch: 5966 mean train loss:  8.98033569e-05, mean val. rec. loss:  9.38387906e-05\n",
      "Epoch: 5967 mean train loss:  8.95071850e-05, mean val. rec. loss:  9.48230304e-05\n",
      "Epoch: 5968 mean train loss:  8.97732572e-05, mean val. rec. loss:  9.38562177e-05\n",
      "Epoch: 5969 mean train loss:  8.91372191e-05, mean val. rec. loss:  9.39121334e-05\n",
      "Epoch: 5970 mean train loss:  8.87665265e-05, mean val. rec. loss:  9.40563112e-05\n",
      "Epoch: 5971 mean train loss:  8.94140197e-05, mean val. rec. loss:  9.41491527e-05\n",
      "Epoch: 5972 mean train loss:  8.94814792e-05, mean val. rec. loss:  9.43113391e-05\n",
      "Epoch: 5973 mean train loss:  8.93593947e-05, mean val. rec. loss:  9.53175854e-05\n",
      "Epoch: 5974 mean train loss:  8.94561237e-05, mean val. rec. loss:  9.34118723e-05\n",
      "Epoch: 5975 mean train loss:  8.95663933e-05, mean val. rec. loss:  9.34975358e-05\n",
      "Epoch: 5976 mean train loss:  8.92206526e-05, mean val. rec. loss:  9.59873199e-05\n",
      "Epoch: 5977 mean train loss:  8.93831263e-05, mean val. rec. loss:  9.22101663e-05\n",
      "Epoch: 5978 mean train loss:  8.91262922e-05, mean val. rec. loss:  9.53022481e-05\n",
      "Epoch: 5979 mean train loss:  8.97745968e-05, mean val. rec. loss:  9.46889200e-05\n",
      "Epoch: 5980 mean train loss:  8.87849767e-05, mean val. rec. loss:  9.30876267e-05\n",
      "Epoch: 5981 mean train loss:  8.88443210e-05, mean val. rec. loss:  9.37370811e-05\n",
      "Epoch: 5982 mean train loss:  8.88749170e-05, mean val. rec. loss:  9.37750972e-05\n",
      "Epoch: 5983 mean train loss:  8.86694620e-05, mean val. rec. loss:  9.30520819e-05\n",
      "Epoch: 5984 mean train loss:  8.88539856e-05, mean val. rec. loss:  9.41090377e-05\n",
      "Epoch: 5985 mean train loss:  8.89034253e-05, mean val. rec. loss:  9.51264871e-05\n",
      "Epoch: 5986 mean train loss:  8.99476465e-05, mean val. rec. loss:  9.22728057e-05\n",
      "Epoch: 5987 mean train loss:  8.89186050e-05, mean val. rec. loss:  9.45355743e-05\n",
      "Epoch: 5988 mean train loss:  8.88883342e-05, mean val. rec. loss:  9.24621595e-05\n",
      "Epoch: 5989 mean train loss:  8.90034202e-05, mean val. rec. loss:  9.45961148e-05\n",
      "Epoch: 5990 mean train loss:  8.89740170e-05, mean val. rec. loss:  9.23495649e-05\n",
      "Epoch: 5991 mean train loss:  8.82024305e-05, mean val. rec. loss:  9.44680375e-05\n",
      "Epoch: 5992 mean train loss:  8.81134545e-05, mean val. rec. loss:  9.21929119e-05\n",
      "Epoch: 5993 mean train loss:  8.86920048e-05, mean val. rec. loss:  9.39150954e-05\n",
      "Epoch: 5994 mean train loss:  8.89251459e-05, mean val. rec. loss:  9.34009327e-05\n",
      "Epoch: 5995 mean train loss:  8.83890157e-05, mean val. rec. loss:  9.39091895e-05\n",
      "Epoch: 5996 mean train loss:  8.86288943e-05, mean val. rec. loss:  9.29416316e-05\n",
      "Epoch: 5997 mean train loss:  8.85376463e-05, mean val. rec. loss:  9.36047424e-05\n",
      "Epoch: 5998 mean train loss:  8.89415043e-05, mean val. rec. loss:  9.50197076e-05\n",
      "Epoch: 5999 mean train loss:  8.91604419e-05, mean val. rec. loss:  9.35367695e-05\n",
      "Epoch: 6000 mean train loss:  8.86678041e-05, mean val. rec. loss:  9.23301661e-05\n",
      "Epoch: 6001 mean train loss:  8.87443741e-05, mean val. rec. loss:  9.58285771e-05\n",
      "Epoch: 6002 mean train loss:  8.97649777e-05, mean val. rec. loss:  9.18678212e-05\n",
      "Epoch: 6003 mean train loss:  8.89127556e-05, mean val. rec. loss:  9.44442865e-05\n",
      "Epoch: 6004 mean train loss:  8.81578981e-05, mean val. rec. loss:  9.50109214e-05\n",
      "Epoch: 6005 mean train loss:  8.82109068e-05, mean val. rec. loss:  9.22614300e-05\n",
      "Epoch: 6006 mean train loss:  8.85319981e-05, mean val. rec. loss:  9.35254301e-05\n",
      "Epoch: 6007 mean train loss:  8.78401261e-05, mean val. rec. loss:  9.25051366e-05\n",
      "Epoch: 6008 mean train loss:  8.82856764e-05, mean val. rec. loss:  9.21753031e-05\n",
      "Epoch: 6009 mean train loss:  8.79167976e-05, mean val. rec. loss:  9.18297324e-05\n",
      "Epoch: 6010 mean train loss:  8.72896161e-05, mean val. rec. loss:  9.28570585e-05\n",
      "Epoch: 6011 mean train loss:  8.77230018e-05, mean val. rec. loss:  9.32192475e-05\n",
      "Epoch: 6012 mean train loss:  8.74732106e-05, mean val. rec. loss:  9.11965603e-05\n",
      "Epoch: 6013 mean train loss:  8.75304511e-05, mean val. rec. loss:  9.32004030e-05\n",
      "Epoch: 6014 mean train loss:  8.79569946e-05, mean val. rec. loss:  9.17938152e-05\n",
      "Epoch: 6015 mean train loss:  8.74513546e-05, mean val. rec. loss:  9.26271990e-05\n",
      "Epoch: 6016 mean train loss:  8.73563017e-05, mean val. rec. loss:  9.22092759e-05\n",
      "Epoch: 6017 mean train loss:  8.76127225e-05, mean val. rec. loss:  9.03695549e-05\n",
      "Epoch: 6018 mean train loss:  8.77750262e-05, mean val. rec. loss:  9.30528270e-05\n",
      "Epoch: 6019 mean train loss:  8.72434821e-05, mean val. rec. loss:  9.14139173e-05\n",
      "Epoch: 6020 mean train loss:  8.75212533e-05, mean val. rec. loss:  9.09690632e-05\n",
      "Epoch: 6021 mean train loss:  8.77354746e-05, mean val. rec. loss:  9.30981665e-05\n",
      "Epoch: 6022 mean train loss:  8.75944531e-05, mean val. rec. loss:  9.32160220e-05\n",
      "Epoch: 6023 mean train loss:  8.77736394e-05, mean val. rec. loss:  9.21396402e-05\n",
      "Epoch: 6024 mean train loss:  8.68208208e-05, mean val. rec. loss:  9.09095857e-05\n",
      "Epoch: 6025 mean train loss:  8.69851111e-05, mean val. rec. loss:  9.19232190e-05\n",
      "Epoch: 6026 mean train loss:  8.70114247e-05, mean val. rec. loss:  9.20521141e-05\n",
      "Epoch: 6027 mean train loss:  8.70823121e-05, mean val. rec. loss:  9.16163732e-05\n",
      "Epoch: 6028 mean train loss:  8.68519837e-05, mean val. rec. loss:  9.27472169e-05\n",
      "Epoch: 6029 mean train loss:  8.70207061e-05, mean val. rec. loss:  9.12362119e-05\n",
      "Epoch: 6030 mean train loss:  8.72087457e-05, mean val. rec. loss:  9.24272054e-05\n",
      "Epoch: 6031 mean train loss:  8.73931731e-05, mean val. rec. loss:  9.37515733e-05\n",
      "Epoch: 6032 mean train loss:  8.73592913e-05, mean val. rec. loss:  9.15193249e-05\n",
      "Epoch: 6033 mean train loss:  8.67554952e-05, mean val. rec. loss:  9.12830597e-05\n",
      "Epoch: 6034 mean train loss:  8.65044009e-05, mean val. rec. loss:  9.29623024e-05\n",
      "Epoch: 6035 mean train loss:  8.76709978e-05, mean val. rec. loss:  9.08280200e-05\n",
      "Epoch: 6036 mean train loss:  8.69319518e-05, mean val. rec. loss:  9.22125105e-05\n",
      "Epoch: 6037 mean train loss:  8.66215633e-05, mean val. rec. loss:  9.11271881e-05\n",
      "Epoch: 6038 mean train loss:  8.72066596e-05, mean val. rec. loss:  9.19215199e-05\n",
      "Epoch: 6039 mean train loss:  8.69635725e-05, mean val. rec. loss:  9.24086789e-05\n",
      "Epoch: 6040 mean train loss:  8.66540777e-05, mean val. rec. loss:  9.18372466e-05\n",
      "Epoch: 6041 mean train loss:  8.72995070e-05, mean val. rec. loss:  9.28309996e-05\n",
      "Epoch: 6042 mean train loss:  8.76718865e-05, mean val. rec. loss:  9.35222681e-05\n",
      "Epoch: 6043 mean train loss:  8.73662978e-05, mean val. rec. loss:  9.18323310e-05\n",
      "Epoch: 6044 mean train loss:  8.71870535e-05, mean val. rec. loss:  9.01030695e-05\n",
      "Epoch: 6045 mean train loss:  8.65921885e-05, mean val. rec. loss:  9.18723734e-05\n",
      "Epoch: 6046 mean train loss:  8.65274571e-05, mean val. rec. loss:  9.00961640e-05\n",
      "Epoch: 6047 mean train loss:  8.65104067e-05, mean val. rec. loss:  9.03515736e-05\n",
      "Epoch: 6048 mean train loss:  8.63565514e-05, mean val. rec. loss:  9.12143872e-05\n",
      "Epoch: 6049 mean train loss:  8.64639071e-05, mean val. rec. loss:  9.09335275e-05\n",
      "Epoch: 6050 mean train loss:  8.72957447e-05, mean val. rec. loss:  9.50808932e-05\n",
      "Epoch: 6051 mean train loss:  8.80923178e-05, mean val. rec. loss:  9.20092006e-05\n",
      "Epoch: 6052 mean train loss:  8.62930338e-05, mean val. rec. loss:  9.07598563e-05\n",
      "Epoch: 6053 mean train loss:  8.62116699e-05, mean val. rec. loss:  9.14539051e-05\n",
      "Epoch: 6054 mean train loss:  8.68171433e-05, mean val. rec. loss:  9.07912669e-05\n",
      "Epoch: 6055 mean train loss:  8.66670341e-05, mean val. rec. loss:  9.15705703e-05\n",
      "Epoch: 6056 mean train loss:  8.63810876e-05, mean val. rec. loss:  9.06440361e-05\n",
      "Epoch: 6057 mean train loss:  8.61442372e-05, mean val. rec. loss:  9.21780743e-05\n",
      "Epoch: 6058 mean train loss:  8.62595335e-05, mean val. rec. loss:  9.01708879e-05\n",
      "Epoch: 6059 mean train loss:  8.58209949e-05, mean val. rec. loss:  8.94024241e-05\n",
      "Epoch: 6060 mean train loss:  8.51951967e-05, mean val. rec. loss:  9.06909839e-05\n",
      "Epoch: 6061 mean train loss:  8.55513073e-05, mean val. rec. loss:  9.03202085e-05\n",
      "Epoch: 6062 mean train loss:  8.59093892e-05, mean val. rec. loss:  9.09367894e-05\n",
      "Epoch: 6063 mean train loss:  8.61552908e-05, mean val. rec. loss:  9.01429210e-05\n",
      "Epoch: 6064 mean train loss:  8.55402870e-05, mean val. rec. loss:  8.92749283e-05\n",
      "Epoch: 6065 mean train loss:  8.57346690e-05, mean val. rec. loss:  9.07738034e-05\n",
      "Epoch: 6066 mean train loss:  8.55218413e-05, mean val. rec. loss:  9.15842721e-05\n",
      "Epoch: 6067 mean train loss:  8.66377432e-05, mean val. rec. loss:  9.01112651e-05\n",
      "Epoch: 6068 mean train loss:  8.71710578e-05, mean val. rec. loss:  9.03273864e-05\n",
      "Epoch: 6069 mean train loss:  8.66162062e-05, mean val. rec. loss:  9.17607873e-05\n",
      "Epoch: 6070 mean train loss:  8.59436469e-05, mean val. rec. loss:  8.92235557e-05\n",
      "Epoch: 6071 mean train loss:  8.53808364e-05, mean val. rec. loss:  9.11107696e-05\n",
      "Epoch: 6072 mean train loss:  8.57060721e-05, mean val. rec. loss:  8.91216826e-05\n",
      "Epoch: 6073 mean train loss:  8.53995436e-05, mean val. rec. loss:  9.11422437e-05\n",
      "Epoch: 6074 mean train loss:  8.59929035e-05, mean val. rec. loss:  8.90935521e-05\n",
      "Epoch: 6075 mean train loss:  8.58086165e-05, mean val. rec. loss:  9.15711882e-05\n",
      "Epoch: 6076 mean train loss:  8.57011465e-05, mean val. rec. loss:  8.92305520e-05\n",
      "Epoch: 6077 mean train loss:  8.52073119e-05, mean val. rec. loss:  9.08208693e-05\n",
      "Epoch: 6078 mean train loss:  8.52219293e-05, mean val. rec. loss:  9.01175799e-05\n",
      "Epoch: 6079 mean train loss:  8.53416310e-05, mean val. rec. loss:  8.94969647e-05\n",
      "Epoch: 6080 mean train loss:  8.56548732e-05, mean val. rec. loss:  9.00196048e-05\n",
      "Epoch: 6081 mean train loss:  8.58503810e-05, mean val. rec. loss:  9.36057146e-05\n",
      "Epoch: 6082 mean train loss:  8.64365400e-05, mean val. rec. loss:  8.88083766e-05\n",
      "Epoch: 6083 mean train loss:  8.65302135e-05, mean val. rec. loss:  9.10142028e-05\n",
      "Epoch: 6084 mean train loss:  8.52132768e-05, mean val. rec. loss:  8.99026579e-05\n",
      "Epoch: 6085 mean train loss:  8.46769231e-05, mean val. rec. loss:  8.99155511e-05\n",
      "Epoch: 6086 mean train loss:  8.52090073e-05, mean val. rec. loss:  8.85656331e-05\n",
      "Epoch: 6087 mean train loss:  8.47150266e-05, mean val. rec. loss:  8.96126759e-05\n",
      "Epoch: 6088 mean train loss:  8.47204792e-05, mean val. rec. loss:  8.91616432e-05\n",
      "Epoch: 6089 mean train loss:  8.49042113e-05, mean val. rec. loss:  8.99950451e-05\n",
      "Epoch: 6090 mean train loss:  8.56822363e-05, mean val. rec. loss:  9.01150631e-05\n",
      "Epoch: 6091 mean train loss:  8.52156506e-05, mean val. rec. loss:  8.84543196e-05\n",
      "Epoch: 6092 mean train loss:  8.46398578e-05, mean val. rec. loss:  9.09125114e-05\n",
      "Epoch: 6093 mean train loss:  8.50766032e-05, mean val. rec. loss:  8.87362695e-05\n",
      "Epoch: 6094 mean train loss:  8.41991677e-05, mean val. rec. loss:  8.87105196e-05\n",
      "Epoch: 6095 mean train loss:  8.43959934e-05, mean val. rec. loss:  8.82064606e-05\n",
      "Epoch: 6096 mean train loss:  8.39626783e-05, mean val. rec. loss:  8.82018358e-05\n",
      "Epoch: 6097 mean train loss:  8.41190930e-05, mean val. rec. loss:  8.77450243e-05\n",
      "Epoch: 6098 mean train loss:  8.43933894e-05, mean val. rec. loss:  9.04047998e-05\n",
      "Epoch: 6099 mean train loss:  8.42087083e-05, mean val. rec. loss:  8.80722774e-05\n",
      "Epoch: 6100 mean train loss:  8.38068699e-05, mean val. rec. loss:  8.85305154e-05\n",
      "Epoch: 6101 mean train loss:  8.43249139e-05, mean val. rec. loss:  8.98041195e-05\n",
      "Epoch: 6102 mean train loss:  8.45364978e-05, mean val. rec. loss:  8.80664442e-05\n",
      "Epoch: 6103 mean train loss:  8.45335381e-05, mean val. rec. loss:  8.93938923e-05\n",
      "Epoch: 6104 mean train loss:  8.42301157e-05, mean val. rec. loss:  8.83409163e-05\n",
      "Epoch: 6105 mean train loss:  8.43613952e-05, mean val. rec. loss:  8.98124060e-05\n",
      "Epoch: 6106 mean train loss:  8.42948358e-05, mean val. rec. loss:  8.95107574e-05\n",
      "Epoch: 6107 mean train loss:  8.44357850e-05, mean val. rec. loss:  8.95786849e-05\n",
      "Epoch: 6108 mean train loss:  8.41328141e-05, mean val. rec. loss:  8.83848111e-05\n",
      "Epoch: 6109 mean train loss:  8.40219384e-05, mean val. rec. loss:  8.87813910e-05\n",
      "Epoch: 6110 mean train loss:  8.38538398e-05, mean val. rec. loss:  8.79805626e-05\n",
      "Epoch: 6111 mean train loss:  8.38451810e-05, mean val. rec. loss:  8.83330659e-05\n",
      "Epoch: 6112 mean train loss:  8.45069851e-05, mean val. rec. loss:  8.98473692e-05\n",
      "Epoch: 6113 mean train loss:  8.40847657e-05, mean val. rec. loss:  8.90433425e-05\n",
      "Epoch: 6114 mean train loss:  8.37458973e-05, mean val. rec. loss:  8.75680003e-05\n",
      "Epoch: 6115 mean train loss:  8.42478853e-05, mean val. rec. loss:  8.87431931e-05\n",
      "Epoch: 6116 mean train loss:  8.34183504e-05, mean val. rec. loss:  8.73852249e-05\n",
      "Epoch: 6117 mean train loss:  8.40356603e-05, mean val. rec. loss:  8.94378053e-05\n",
      "Epoch: 6118 mean train loss:  8.40733635e-05, mean val. rec. loss:  8.88072681e-05\n",
      "Epoch: 6119 mean train loss:  8.34659718e-05, mean val. rec. loss:  8.83214721e-05\n",
      "Epoch: 6120 mean train loss:  8.34350698e-05, mean val. rec. loss:  8.78664233e-05\n",
      "Epoch: 6121 mean train loss:  8.38036240e-05, mean val. rec. loss:  8.94014065e-05\n",
      "Epoch: 6122 mean train loss:  8.55596634e-05, mean val. rec. loss:  9.06209757e-05\n",
      "Epoch: 6123 mean train loss:  8.39345817e-05, mean val. rec. loss:  8.74452111e-05\n",
      "Epoch: 6124 mean train loss:  8.35878386e-05, mean val. rec. loss:  8.74001442e-05\n",
      "Epoch: 6125 mean train loss:  8.38575190e-05, mean val. rec. loss:  8.78413458e-05\n",
      "Epoch: 6126 mean train loss:  8.37717628e-05, mean val. rec. loss:  9.00684515e-05\n",
      "Epoch: 6127 mean train loss:  8.40575737e-05, mean val. rec. loss:  8.81597400e-05\n",
      "Epoch: 6128 mean train loss:  8.48491541e-05, mean val. rec. loss:  8.91300327e-05\n",
      "Epoch: 6129 mean train loss:  8.34869026e-05, mean val. rec. loss:  8.62804491e-05\n",
      "Epoch: 6130 mean train loss:  8.34636349e-05, mean val. rec. loss:  8.75949860e-05\n",
      "Epoch: 6131 mean train loss:  8.30489928e-05, mean val. rec. loss:  8.93681515e-05\n",
      "Epoch: 6132 mean train loss:  8.37504499e-05, mean val. rec. loss:  8.90838573e-05\n",
      "Epoch: 6133 mean train loss:  8.38251592e-05, mean val. rec. loss:  8.87043592e-05\n",
      "Epoch: 6134 mean train loss:  8.40706773e-05, mean val. rec. loss:  8.80250934e-05\n",
      "Epoch: 6135 mean train loss:  8.31404791e-05, mean val. rec. loss:  8.71217923e-05\n",
      "Epoch: 6136 mean train loss:  8.29396722e-05, mean val. rec. loss:  8.86521870e-05\n",
      "Epoch: 6137 mean train loss:  8.29436419e-05, mean val. rec. loss:  8.81787480e-05\n",
      "Epoch: 6138 mean train loss:  8.31384123e-05, mean val. rec. loss:  8.84589626e-05\n",
      "Epoch: 6139 mean train loss:  8.26262423e-05, mean val. rec. loss:  8.69483392e-05\n",
      "Epoch: 6140 mean train loss:  8.35354753e-05, mean val. rec. loss:  8.84310138e-05\n",
      "Epoch: 6141 mean train loss:  8.32148410e-05, mean val. rec. loss:  8.78288978e-05\n",
      "Epoch: 6142 mean train loss:  8.27390876e-05, mean val. rec. loss:  8.64300968e-05\n",
      "Epoch: 6143 mean train loss:  8.31021442e-05, mean val. rec. loss:  8.91162310e-05\n",
      "Epoch: 6144 mean train loss:  8.30533094e-05, mean val. rec. loss:  8.70349840e-05\n",
      "Epoch: 6145 mean train loss:  8.29897180e-05, mean val. rec. loss:  8.69485481e-05\n",
      "Epoch: 6146 mean train loss:  8.23621157e-05, mean val. rec. loss:  8.68933593e-05\n",
      "Epoch: 6147 mean train loss:  8.23246229e-05, mean val. rec. loss:  8.70488947e-05\n",
      "Epoch: 6148 mean train loss:  8.21189700e-05, mean val. rec. loss:  8.69358640e-05\n",
      "Epoch: 6149 mean train loss:  8.26842771e-05, mean val. rec. loss:  8.74074312e-05\n",
      "Epoch: 6150 mean train loss:  8.22011902e-05, mean val. rec. loss:  8.72821615e-05\n",
      "Epoch: 6151 mean train loss:  8.22139752e-05, mean val. rec. loss:  8.67530158e-05\n",
      "Epoch: 6152 mean train loss:  8.23807751e-05, mean val. rec. loss:  8.63396722e-05\n",
      "Epoch: 6153 mean train loss:  8.19084991e-05, mean val. rec. loss:  8.71772991e-05\n",
      "Epoch: 6154 mean train loss:  8.19644455e-05, mean val. rec. loss:  8.73003428e-05\n",
      "Epoch: 6155 mean train loss:  8.28173789e-05, mean val. rec. loss:  8.63747444e-05\n",
      "Epoch: 6156 mean train loss:  8.30633140e-05, mean val. rec. loss:  8.99896207e-05\n",
      "Epoch: 6157 mean train loss:  8.26373778e-05, mean val. rec. loss:  8.60496083e-05\n",
      "Epoch: 6158 mean train loss:  8.24217943e-05, mean val. rec. loss:  8.68086044e-05\n",
      "Epoch: 6159 mean train loss:  8.28111934e-05, mean val. rec. loss:  8.73207683e-05\n",
      "Epoch: 6160 mean train loss:  8.23619457e-05, mean val. rec. loss:  8.61933955e-05\n",
      "Epoch: 6161 mean train loss:  8.20396552e-05, mean val. rec. loss:  8.67477459e-05\n",
      "Epoch: 6162 mean train loss:  8.27672314e-05, mean val. rec. loss:  8.77650227e-05\n",
      "Epoch: 6163 mean train loss:  8.30946629e-05, mean val. rec. loss:  8.90651854e-05\n",
      "Epoch: 6164 mean train loss:  8.29575185e-05, mean val. rec. loss:  8.52112454e-05\n",
      "Epoch: 6165 mean train loss:  8.23094715e-05, mean val. rec. loss:  8.62490022e-05\n",
      "Epoch: 6166 mean train loss:  8.23838045e-05, mean val. rec. loss:  8.62787228e-05\n",
      "Epoch: 6167 mean train loss:  8.16854661e-05, mean val. rec. loss:  8.65924377e-05\n",
      "Epoch: 6168 mean train loss:  8.24962682e-05, mean val. rec. loss:  8.60951023e-05\n",
      "Epoch: 6169 mean train loss:  8.18426359e-05, mean val. rec. loss:  8.69923249e-05\n",
      "Epoch: 6170 mean train loss:  8.15940594e-05, mean val. rec. loss:  8.55766782e-05\n",
      "Epoch: 6171 mean train loss:  8.20348383e-05, mean val. rec. loss:  8.82480930e-05\n",
      "Epoch: 6172 mean train loss:  8.25009135e-05, mean val. rec. loss:  8.52412749e-05\n",
      "Epoch: 6173 mean train loss:  8.24179183e-05, mean val. rec. loss:  8.89654021e-05\n",
      "Epoch: 6174 mean train loss:  8.29751102e-05, mean val. rec. loss:  8.62416516e-05\n",
      "Epoch: 6175 mean train loss:  8.22692797e-05, mean val. rec. loss:  8.59108095e-05\n",
      "Epoch: 6176 mean train loss:  8.17979577e-05, mean val. rec. loss:  8.73890410e-05\n",
      "Epoch: 6177 mean train loss:  8.19777388e-05, mean val. rec. loss:  8.71178126e-05\n",
      "Epoch: 6178 mean train loss:  8.16138816e-05, mean val. rec. loss:  8.51061195e-05\n",
      "Epoch: 6179 mean train loss:  8.10894230e-05, mean val. rec. loss:  8.57801245e-05\n",
      "Epoch: 6180 mean train loss:  8.13560878e-05, mean val. rec. loss:  8.55187454e-05\n",
      "Epoch: 6181 mean train loss:  8.09912930e-05, mean val. rec. loss:  8.59797183e-05\n",
      "Epoch: 6182 mean train loss:  8.17934722e-05, mean val. rec. loss:  8.63326305e-05\n",
      "Epoch: 6183 mean train loss:  8.15167773e-05, mean val. rec. loss:  8.64369840e-05\n",
      "Epoch: 6184 mean train loss:  8.18326105e-05, mean val. rec. loss:  8.79925562e-05\n",
      "Epoch: 6185 mean train loss:  8.21845856e-05, mean val. rec. loss:  8.77082348e-05\n",
      "Epoch: 6186 mean train loss:  8.22386290e-05, mean val. rec. loss:  8.69175283e-05\n",
      "Epoch: 6187 mean train loss:  8.13020540e-05, mean val. rec. loss:  8.72915111e-05\n",
      "Epoch: 6188 mean train loss:  8.15780157e-05, mean val. rec. loss:  8.56740264e-05\n",
      "Epoch: 6189 mean train loss:  8.13833793e-05, mean val. rec. loss:  8.65507962e-05\n",
      "Epoch: 6190 mean train loss:  8.06640491e-05, mean val. rec. loss:  8.48702541e-05\n",
      "Epoch: 6191 mean train loss:  8.14746011e-05, mean val. rec. loss:  8.81329906e-05\n",
      "Epoch: 6192 mean train loss:  8.14185506e-05, mean val. rec. loss:  8.47257855e-05\n",
      "Epoch: 6193 mean train loss:  8.12661941e-05, mean val. rec. loss:  8.58625443e-05\n",
      "Epoch: 6194 mean train loss:  8.19171471e-05, mean val. rec. loss:  8.48329467e-05\n",
      "Epoch: 6195 mean train loss:  8.07152577e-05, mean val. rec. loss:  8.55843468e-05\n",
      "Epoch: 6196 mean train loss:  8.07602568e-05, mean val. rec. loss:  8.67981918e-05\n",
      "Epoch: 6197 mean train loss:  8.14478938e-05, mean val. rec. loss:  8.45101003e-05\n",
      "Epoch: 6198 mean train loss:  8.05019498e-05, mean val. rec. loss:  8.51247096e-05\n",
      "Epoch: 6199 mean train loss:  8.09298019e-05, mean val. rec. loss:  8.49741625e-05\n",
      "Epoch: 6200 mean train loss:  8.10094282e-05, mean val. rec. loss:  8.50377832e-05\n",
      "Epoch: 6201 mean train loss:  8.09931295e-05, mean val. rec. loss:  8.63457780e-05\n",
      "Epoch: 6202 mean train loss:  8.04175423e-05, mean val. rec. loss:  8.34548800e-05\n",
      "Epoch: 6203 mean train loss:  8.08273297e-05, mean val. rec. loss:  8.72920744e-05\n",
      "Epoch: 6204 mean train loss:  8.11126561e-05, mean val. rec. loss:  8.45531774e-05\n",
      "Epoch: 6205 mean train loss:  8.12200950e-05, mean val. rec. loss:  8.43535109e-05\n",
      "Epoch: 6206 mean train loss:  8.11570360e-05, mean val. rec. loss:  8.53830176e-05\n",
      "Epoch: 6207 mean train loss:  8.11132155e-05, mean val. rec. loss:  8.52369499e-05\n",
      "Epoch: 6208 mean train loss:  8.08909092e-05, mean val. rec. loss:  8.43320496e-05\n",
      "Epoch: 6209 mean train loss:  8.06653290e-05, mean val. rec. loss:  8.56950061e-05\n",
      "Epoch: 6210 mean train loss:  8.03460025e-05, mean val. rec. loss:  8.52852969e-05\n",
      "Epoch: 6211 mean train loss:  8.04689848e-05, mean val. rec. loss:  8.36422349e-05\n",
      "Epoch: 6212 mean train loss:  8.08214193e-05, mean val. rec. loss:  8.43401635e-05\n",
      "Epoch: 6213 mean train loss:  8.05074277e-05, mean val. rec. loss:  8.47782758e-05\n",
      "Epoch: 6214 mean train loss:  8.04598299e-05, mean val. rec. loss:  8.48058520e-05\n",
      "Epoch: 6215 mean train loss:  8.03996223e-05, mean val. rec. loss:  8.42845022e-05\n",
      "Epoch: 6216 mean train loss:  8.03627489e-05, mean val. rec. loss:  8.46836898e-05\n",
      "Epoch: 6217 mean train loss:  8.03346037e-05, mean val. rec. loss:  8.40618298e-05\n",
      "Epoch: 6218 mean train loss:  8.08842847e-05, mean val. rec. loss:  8.45871866e-05\n",
      "Epoch: 6219 mean train loss:  8.08552892e-05, mean val. rec. loss:  8.45034857e-05\n",
      "Epoch: 6220 mean train loss:  8.03522989e-05, mean val. rec. loss:  8.70195286e-05\n",
      "Epoch: 6221 mean train loss:  8.10487399e-05, mean val. rec. loss:  8.51399651e-05\n",
      "Epoch: 6222 mean train loss:  8.02049156e-05, mean val. rec. loss:  8.35314211e-05\n",
      "Epoch: 6223 mean train loss:  7.97283554e-05, mean val. rec. loss:  8.36541376e-05\n",
      "Epoch: 6224 mean train loss:  7.94915009e-05, mean val. rec. loss:  8.49150303e-05\n",
      "Epoch: 6225 mean train loss:  7.93275734e-05, mean val. rec. loss:  8.38821981e-05\n",
      "Epoch: 6226 mean train loss:  8.01789648e-05, mean val. rec. loss:  8.45693234e-05\n",
      "Epoch: 6227 mean train loss:  7.97167843e-05, mean val. rec. loss:  8.38052118e-05\n",
      "Epoch: 6228 mean train loss:  8.01357749e-05, mean val. rec. loss:  8.53867611e-05\n",
      "Epoch: 6229 mean train loss:  8.01290330e-05, mean val. rec. loss:  8.41340550e-05\n",
      "Epoch: 6230 mean train loss:  7.99177533e-05, mean val. rec. loss:  8.42764338e-05\n",
      "Epoch: 6231 mean train loss:  8.09407156e-05, mean val. rec. loss:  8.48743247e-05\n",
      "Epoch: 6232 mean train loss:  8.01124543e-05, mean val. rec. loss:  8.42587159e-05\n",
      "Epoch: 6233 mean train loss:  8.06671547e-05, mean val. rec. loss:  8.44813520e-05\n",
      "Epoch: 6234 mean train loss:  8.16976625e-05, mean val. rec. loss:  8.52439643e-05\n",
      "Epoch: 6235 mean train loss:  8.03417359e-05, mean val. rec. loss:  8.52395849e-05\n",
      "Epoch: 6236 mean train loss:  8.01841172e-05, mean val. rec. loss:  8.30607443e-05\n",
      "Epoch: 6237 mean train loss:  7.92942556e-05, mean val. rec. loss:  8.47441394e-05\n",
      "Epoch: 6238 mean train loss:  7.96876074e-05, mean val. rec. loss:  8.44186762e-05\n",
      "Epoch: 6239 mean train loss:  8.00199475e-05, mean val. rec. loss:  8.48980484e-05\n",
      "Epoch: 6240 mean train loss:  8.05929855e-05, mean val. rec. loss:  8.43456515e-05\n",
      "Epoch: 6241 mean train loss:  7.96196687e-05, mean val. rec. loss:  8.41527087e-05\n",
      "Epoch: 6242 mean train loss:  7.91421162e-05, mean val. rec. loss:  8.43015568e-05\n",
      "Epoch: 6243 mean train loss:  7.90329030e-05, mean val. rec. loss:  8.43777980e-05\n",
      "Epoch: 6244 mean train loss:  7.92627245e-05, mean val. rec. loss:  8.32827716e-05\n",
      "Epoch: 6245 mean train loss:  7.92908151e-05, mean val. rec. loss:  8.34720527e-05\n",
      "Epoch: 6246 mean train loss:  7.96439388e-05, mean val. rec. loss:  8.35408161e-05\n",
      "Epoch: 6247 mean train loss:  7.95177531e-05, mean val. rec. loss:  8.38259190e-05\n",
      "Epoch: 6248 mean train loss:  7.89015314e-05, mean val. rec. loss:  8.28791319e-05\n",
      "Epoch: 6249 mean train loss:  7.91392211e-05, mean val. rec. loss:  8.33473191e-05\n",
      "Epoch: 6250 mean train loss:  7.93996883e-05, mean val. rec. loss:  8.54452572e-05\n",
      "Epoch: 6251 mean train loss:  7.94281085e-05, mean val. rec. loss:  8.38766556e-05\n",
      "Epoch: 6252 mean train loss:  7.95882959e-05, mean val. rec. loss:  8.24041029e-05\n",
      "Epoch: 6253 mean train loss:  7.87571923e-05, mean val. rec. loss:  8.37533031e-05\n",
      "Epoch: 6254 mean train loss:  7.90807786e-05, mean val. rec. loss:  8.42612237e-05\n",
      "Epoch: 6255 mean train loss:  7.88498765e-05, mean val. rec. loss:  8.28626316e-05\n",
      "Epoch: 6256 mean train loss:  7.89719219e-05, mean val. rec. loss:  8.46089386e-05\n",
      "Epoch: 6257 mean train loss:  7.95472109e-05, mean val. rec. loss:  8.30323776e-05\n",
      "Epoch: 6258 mean train loss:  7.89222889e-05, mean val. rec. loss:  8.32743579e-05\n",
      "Epoch: 6259 mean train loss:  7.87378881e-05, mean val. rec. loss:  8.43932262e-05\n",
      "Epoch: 6260 mean train loss:  7.85261504e-05, mean val. rec. loss:  8.26942303e-05\n",
      "Epoch: 6261 mean train loss:  7.87203704e-05, mean val. rec. loss:  8.35735987e-05\n",
      "Epoch: 6262 mean train loss:  7.90302646e-05, mean val. rec. loss:  8.28200451e-05\n",
      "Epoch: 6263 mean train loss:  7.88478746e-05, mean val. rec. loss:  8.34709806e-05\n",
      "Epoch: 6264 mean train loss:  7.84807436e-05, mean val. rec. loss:  8.21277590e-05\n",
      "Epoch: 6265 mean train loss:  7.83097060e-05, mean val. rec. loss:  8.25897859e-05\n",
      "Epoch: 6266 mean train loss:  7.82144587e-05, mean val. rec. loss:  8.26085032e-05\n",
      "Epoch: 6267 mean train loss:  7.83700276e-05, mean val. rec. loss:  8.27273399e-05\n",
      "Epoch: 6268 mean train loss:  7.85048949e-05, mean val. rec. loss:  8.36024379e-05\n",
      "Epoch: 6269 mean train loss:  7.82950977e-05, mean val. rec. loss:  8.25418205e-05\n",
      "Epoch: 6270 mean train loss:  7.90686452e-05, mean val. rec. loss:  8.64921092e-05\n",
      "Epoch: 6271 mean train loss:  7.85838435e-05, mean val. rec. loss:  8.34560703e-05\n",
      "Epoch: 6272 mean train loss:  7.87869639e-05, mean val. rec. loss:  8.17892573e-05\n",
      "Epoch: 6273 mean train loss:  7.85863276e-05, mean val. rec. loss:  8.33421491e-05\n",
      "Epoch: 6274 mean train loss:  7.84183626e-05, mean val. rec. loss:  8.19537243e-05\n",
      "Epoch: 6275 mean train loss:  7.83052825e-05, mean val. rec. loss:  8.21926517e-05\n",
      "Epoch: 6276 mean train loss:  7.81663927e-05, mean val. rec. loss:  8.37465612e-05\n",
      "Epoch: 6277 mean train loss:  7.88389045e-05, mean val. rec. loss:  8.22682025e-05\n",
      "Epoch: 6278 mean train loss:  7.82475124e-05, mean val. rec. loss:  8.22243167e-05\n",
      "Epoch: 6279 mean train loss:  7.85693119e-05, mean val. rec. loss:  8.37763090e-05\n",
      "Epoch: 6280 mean train loss:  7.84077206e-05, mean val. rec. loss:  8.28304305e-05\n",
      "Epoch: 6281 mean train loss:  7.84568971e-05, mean val. rec. loss:  8.16345124e-05\n",
      "Epoch: 6282 mean train loss:  7.82764331e-05, mean val. rec. loss:  8.47371613e-05\n",
      "Epoch: 6283 mean train loss:  7.88726263e-05, mean val. rec. loss:  8.27128295e-05\n",
      "Epoch: 6284 mean train loss:  7.90252301e-05, mean val. rec. loss:  8.21184458e-05\n",
      "Epoch: 6285 mean train loss:  7.99688851e-05, mean val. rec. loss:  8.58612540e-05\n",
      "Epoch: 6286 mean train loss:  7.90907201e-05, mean val. rec. loss:  8.16808786e-05\n",
      "Epoch: 6287 mean train loss:  7.86410926e-05, mean val. rec. loss:  8.31975443e-05\n",
      "Epoch: 6288 mean train loss:  7.91028563e-05, mean val. rec. loss:  8.21168285e-05\n",
      "Epoch: 6289 mean train loss:  7.77627724e-05, mean val. rec. loss:  8.39472544e-05\n",
      "Epoch: 6290 mean train loss:  7.80190555e-05, mean val. rec. loss:  8.07912248e-05\n",
      "Epoch: 6291 mean train loss:  7.76936959e-05, mean val. rec. loss:  8.15600520e-05\n",
      "Epoch: 6292 mean train loss:  7.77761810e-05, mean val. rec. loss:  8.15804593e-05\n",
      "Epoch: 6293 mean train loss:  7.78553690e-05, mean val. rec. loss:  8.19160626e-05\n",
      "Epoch: 6294 mean train loss:  7.75377113e-05, mean val. rec. loss:  8.29340754e-05\n",
      "Epoch: 6295 mean train loss:  7.76826766e-05, mean val. rec. loss:  8.18609828e-05\n",
      "Epoch: 6296 mean train loss:  7.71984528e-05, mean val. rec. loss:  8.17335233e-05\n",
      "Epoch: 6297 mean train loss:  7.74663602e-05, mean val. rec. loss:  8.22673484e-05\n",
      "Epoch: 6298 mean train loss:  7.76904595e-05, mean val. rec. loss:  8.08773153e-05\n",
      "Epoch: 6299 mean train loss:  7.74305210e-05, mean val. rec. loss:  8.25206136e-05\n",
      "Epoch: 6300 mean train loss:  7.89284880e-05, mean val. rec. loss:  8.27967212e-05\n",
      "Epoch: 6301 mean train loss:  7.79383369e-05, mean val. rec. loss:  8.06592677e-05\n",
      "Epoch: 6302 mean train loss:  7.76192736e-05, mean val. rec. loss:  8.15163480e-05\n",
      "Epoch: 6303 mean train loss:  7.72772787e-05, mean val. rec. loss:  8.20519448e-05\n",
      "Epoch: 6304 mean train loss:  7.77918554e-05, mean val. rec. loss:  8.22875285e-05\n",
      "Epoch: 6305 mean train loss:  7.82610575e-05, mean val. rec. loss:  8.19243854e-05\n",
      "Epoch: 6306 mean train loss:  7.88972205e-05, mean val. rec. loss:  8.39745853e-05\n",
      "Epoch: 6307 mean train loss:  7.79135275e-05, mean val. rec. loss:  8.27787762e-05\n",
      "Epoch: 6308 mean train loss:  7.73593630e-05, mean val. rec. loss:  8.17326420e-05\n",
      "Epoch: 6309 mean train loss:  7.74000643e-05, mean val. rec. loss:  8.20056603e-05\n",
      "Epoch: 6310 mean train loss:  7.73215097e-05, mean val. rec. loss:  8.17840964e-05\n",
      "Epoch: 6311 mean train loss:  7.72532678e-05, mean val. rec. loss:  8.03507501e-05\n",
      "Epoch: 6312 mean train loss:  7.70573353e-05, mean val. rec. loss:  8.33807831e-05\n",
      "Epoch: 6313 mean train loss:  7.76215607e-05, mean val. rec. loss:  8.15763252e-05\n",
      "Epoch: 6314 mean train loss:  7.74198964e-05, mean val. rec. loss:  8.11595651e-05\n",
      "Epoch: 6315 mean train loss:  7.70213523e-05, mean val. rec. loss:  8.03112439e-05\n",
      "Epoch: 6316 mean train loss:  7.70546977e-05, mean val. rec. loss:  8.31627991e-05\n",
      "Epoch: 6317 mean train loss:  7.74512950e-05, mean val. rec. loss:  8.07258323e-05\n",
      "Epoch: 6318 mean train loss:  7.69837563e-05, mean val. rec. loss:  8.03425363e-05\n",
      "Epoch: 6319 mean train loss:  7.71633253e-05, mean val. rec. loss:  8.19345436e-05\n",
      "Epoch: 6320 mean train loss:  7.68083401e-05, mean val. rec. loss:  8.13130380e-05\n",
      "Epoch: 6321 mean train loss:  7.65094058e-05, mean val. rec. loss:  8.05146901e-05\n",
      "Epoch: 6322 mean train loss:  7.74621274e-05, mean val. rec. loss:  8.14262505e-05\n",
      "Epoch: 6323 mean train loss:  7.69889002e-05, mean val. rec. loss:  8.15361738e-05\n",
      "Epoch: 6324 mean train loss:  7.70506540e-05, mean val. rec. loss:  8.08012649e-05\n",
      "Epoch: 6325 mean train loss:  7.59798391e-05, mean val. rec. loss:  8.10129977e-05\n",
      "Epoch: 6326 mean train loss:  7.64461532e-05, mean val. rec. loss:  8.14283857e-05\n",
      "Epoch: 6327 mean train loss:  7.69654357e-05, mean val. rec. loss:  8.19141000e-05\n",
      "Epoch: 6328 mean train loss:  7.68727492e-05, mean val. rec. loss:  8.03522220e-05\n",
      "Epoch: 6329 mean train loss:  7.78121580e-05, mean val. rec. loss:  8.12383596e-05\n",
      "Epoch: 6330 mean train loss:  7.69458177e-05, mean val. rec. loss:  8.08259790e-05\n",
      "Epoch: 6331 mean train loss:  7.65015091e-05, mean val. rec. loss:  8.09061455e-05\n",
      "Epoch: 6332 mean train loss:  7.61992756e-05, mean val. rec. loss:  8.10797440e-05\n",
      "Epoch: 6333 mean train loss:  7.62224751e-05, mean val. rec. loss:  7.98858339e-05\n",
      "Epoch: 6334 mean train loss:  7.64392970e-05, mean val. rec. loss:  8.17365854e-05\n",
      "Epoch: 6335 mean train loss:  7.64564651e-05, mean val. rec. loss:  8.17284897e-05\n",
      "Epoch: 6336 mean train loss:  7.62191774e-05, mean val. rec. loss:  8.03463524e-05\n",
      "Epoch: 6337 mean train loss:  7.68836891e-05, mean val. rec. loss:  8.11229301e-05\n",
      "Epoch: 6338 mean train loss:  7.60839923e-05, mean val. rec. loss:  7.95536652e-05\n",
      "Epoch: 6339 mean train loss:  7.59595152e-05, mean val. rec. loss:  8.07478024e-05\n",
      "Epoch: 6340 mean train loss:  7.60011419e-05, mean val. rec. loss:  8.09468602e-05\n",
      "Epoch: 6341 mean train loss:  7.64128015e-05, mean val. rec. loss:  7.96362213e-05\n",
      "Epoch: 6342 mean train loss:  7.58421208e-05, mean val. rec. loss:  7.98522881e-05\n",
      "Epoch: 6343 mean train loss:  7.63737099e-05, mean val. rec. loss:  8.08165477e-05\n",
      "Epoch: 6344 mean train loss:  7.61022623e-05, mean val. rec. loss:  8.04188775e-05\n",
      "Epoch: 6345 mean train loss:  7.67663274e-05, mean val. rec. loss:  8.10140971e-05\n",
      "Epoch: 6346 mean train loss:  7.62906618e-05, mean val. rec. loss:  7.94989852e-05\n",
      "Epoch: 6347 mean train loss:  7.60657957e-05, mean val. rec. loss:  8.03134518e-05\n",
      "Epoch: 6348 mean train loss:  7.64651324e-05, mean val. rec. loss:  8.11740211e-05\n",
      "Epoch: 6349 mean train loss:  7.57730989e-05, mean val. rec. loss:  7.99665818e-05\n",
      "Epoch: 6350 mean train loss:  7.56759844e-05, mean val. rec. loss:  8.04373222e-05\n",
      "Epoch: 6351 mean train loss:  7.60606246e-05, mean val. rec. loss:  8.20375070e-05\n",
      "Epoch: 6352 mean train loss:  7.65326266e-05, mean val. rec. loss:  8.01365459e-05\n",
      "Epoch: 6353 mean train loss:  7.58525502e-05, mean val. rec. loss:  7.95426529e-05\n",
      "Epoch: 6354 mean train loss:  7.62503051e-05, mean val. rec. loss:  8.16190116e-05\n",
      "Epoch: 6355 mean train loss:  7.65693837e-05, mean val. rec. loss:  7.99963387e-05\n",
      "Epoch: 6356 mean train loss:  7.56161978e-05, mean val. rec. loss:  8.29702107e-05\n",
      "Epoch: 6357 mean train loss:  7.62643852e-05, mean val. rec. loss:  7.89763179e-05\n",
      "Epoch: 6358 mean train loss:  7.56564678e-05, mean val. rec. loss:  8.02686302e-05\n",
      "Epoch: 6359 mean train loss:  7.61000633e-05, mean val. rec. loss:  8.16916456e-05\n",
      "Epoch: 6360 mean train loss:  7.58381300e-05, mean val. rec. loss:  7.95903548e-05\n",
      "Epoch: 6361 mean train loss:  7.61582713e-05, mean val. rec. loss:  7.89008762e-05\n",
      "Epoch: 6362 mean train loss:  7.60458104e-05, mean val. rec. loss:  8.25267558e-05\n",
      "Epoch: 6363 mean train loss:  7.56589607e-05, mean val. rec. loss:  7.92063137e-05\n",
      "Epoch: 6364 mean train loss:  7.55167794e-05, mean val. rec. loss:  7.92291015e-05\n",
      "Epoch: 6365 mean train loss:  7.51426117e-05, mean val. rec. loss:  7.89020392e-05\n",
      "Epoch: 6366 mean train loss:  7.55298523e-05, mean val. rec. loss:  7.92550786e-05\n",
      "Epoch: 6367 mean train loss:  7.56833002e-05, mean val. rec. loss:  8.28743708e-05\n",
      "Epoch: 6368 mean train loss:  7.70765565e-05, mean val. rec. loss:  7.90060566e-05\n",
      "Epoch: 6369 mean train loss:  7.57976590e-05, mean val. rec. loss:  7.98804640e-05\n",
      "Epoch: 6370 mean train loss:  7.53749116e-05, mean val. rec. loss:  8.01599971e-05\n",
      "Epoch: 6371 mean train loss:  7.59672529e-05, mean val. rec. loss:  8.06514355e-05\n",
      "Epoch: 6372 mean train loss:  7.57718728e-05, mean val. rec. loss:  8.01768881e-05\n",
      "Epoch: 6373 mean train loss:  7.54709541e-05, mean val. rec. loss:  7.83988252e-05\n",
      "Epoch: 6374 mean train loss:  7.60272232e-05, mean val. rec. loss:  8.31154062e-05\n",
      "Epoch: 6375 mean train loss:  7.56153984e-05, mean val. rec. loss:  7.80247697e-05\n",
      "Epoch: 6376 mean train loss:  7.52289040e-05, mean val. rec. loss:  7.89776717e-05\n",
      "Epoch: 6377 mean train loss:  7.53945367e-05, mean val. rec. loss:  8.03158596e-05\n",
      "Epoch: 6378 mean train loss:  7.52224109e-05, mean val. rec. loss:  7.91609469e-05\n",
      "Epoch: 6379 mean train loss:  7.49782958e-05, mean val. rec. loss:  7.90103907e-05\n",
      "Epoch: 6380 mean train loss:  7.55212364e-05, mean val. rec. loss:  8.03926914e-05\n",
      "Epoch: 6381 mean train loss:  7.52220743e-05, mean val. rec. loss:  7.85229046e-05\n",
      "Epoch: 6382 mean train loss:  7.47588073e-05, mean val. rec. loss:  8.18809994e-05\n",
      "Epoch: 6383 mean train loss:  7.50333149e-05, mean val. rec. loss:  7.89262264e-05\n",
      "Epoch: 6384 mean train loss:  7.51110320e-05, mean val. rec. loss:  7.91062669e-05\n",
      "Epoch: 6385 mean train loss:  7.51912167e-05, mean val. rec. loss:  7.96610626e-05\n",
      "Epoch: 6386 mean train loss:  7.56643610e-05, mean val. rec. loss:  8.03145421e-05\n",
      "Epoch: 6387 mean train loss:  7.59532737e-05, mean val. rec. loss:  7.91843890e-05\n",
      "Epoch: 6388 mean train loss:  7.50410083e-05, mean val. rec. loss:  7.97234839e-05\n",
      "Epoch: 6389 mean train loss:  7.52488138e-05, mean val. rec. loss:  7.96910103e-05\n",
      "Epoch: 6390 mean train loss:  7.59394725e-05, mean val. rec. loss:  8.08209635e-05\n",
      "Epoch: 6391 mean train loss:  7.56971867e-05, mean val. rec. loss:  7.83038666e-05\n",
      "Epoch: 6392 mean train loss:  7.47997719e-05, mean val. rec. loss:  7.95632237e-05\n",
      "Epoch: 6393 mean train loss:  7.44993348e-05, mean val. rec. loss:  7.83246101e-05\n",
      "Epoch: 6394 mean train loss:  7.49956558e-05, mean val. rec. loss:  7.86919238e-05\n",
      "Epoch: 6395 mean train loss:  7.42090819e-05, mean val. rec. loss:  7.93762505e-05\n",
      "Epoch: 6396 mean train loss:  7.50275305e-05, mean val. rec. loss:  7.95628421e-05\n",
      "Epoch: 6397 mean train loss:  7.51805611e-05, mean val. rec. loss:  7.90158060e-05\n",
      "Epoch: 6398 mean train loss:  7.43034685e-05, mean val. rec. loss:  7.80786138e-05\n",
      "Epoch: 6399 mean train loss:  7.47386920e-05, mean val. rec. loss:  7.83369309e-05\n",
      "Epoch: 6400 mean train loss:  7.52323900e-05, mean val. rec. loss:  7.98528060e-05\n",
      "Epoch: 6401 mean train loss:  7.61706815e-05, mean val. rec. loss:  8.26333536e-05\n",
      "Epoch: 6402 mean train loss:  7.66023657e-05, mean val. rec. loss:  7.88346842e-05\n",
      "Epoch: 6403 mean train loss:  7.54612963e-05, mean val. rec. loss:  7.94942059e-05\n",
      "Epoch: 6404 mean train loss:  7.51678620e-05, mean val. rec. loss:  7.95290328e-05\n",
      "Epoch: 6405 mean train loss:  7.44222869e-05, mean val. rec. loss:  7.82162678e-05\n",
      "Epoch: 6406 mean train loss:  7.44824757e-05, mean val. rec. loss:  8.05469639e-05\n",
      "Epoch: 6407 mean train loss:  7.44411797e-05, mean val. rec. loss:  7.71844623e-05\n",
      "Epoch: 6408 mean train loss:  7.40051257e-05, mean val. rec. loss:  7.85972469e-05\n",
      "Epoch: 6409 mean train loss:  7.44811123e-05, mean val. rec. loss:  7.85427304e-05\n",
      "Epoch: 6410 mean train loss:  7.39984040e-05, mean val. rec. loss:  7.75989508e-05\n",
      "Epoch: 6411 mean train loss:  7.37677914e-05, mean val. rec. loss:  7.81792875e-05\n",
      "Epoch: 6412 mean train loss:  7.41808790e-05, mean val. rec. loss:  7.83828519e-05\n",
      "Epoch: 6413 mean train loss:  7.39563635e-05, mean val. rec. loss:  7.93207891e-05\n",
      "Epoch: 6414 mean train loss:  7.45920534e-05, mean val. rec. loss:  7.87321387e-05\n",
      "Epoch: 6415 mean train loss:  7.39453482e-05, mean val. rec. loss:  7.77360143e-05\n",
      "Epoch: 6416 mean train loss:  7.37405761e-05, mean val. rec. loss:  7.87384626e-05\n",
      "Epoch: 6417 mean train loss:  7.36320071e-05, mean val. rec. loss:  7.76244373e-05\n",
      "Epoch: 6418 mean train loss:  7.42683210e-05, mean val. rec. loss:  8.06271848e-05\n",
      "Epoch: 6419 mean train loss:  7.57197383e-05, mean val. rec. loss:  7.86841824e-05\n",
      "Epoch: 6420 mean train loss:  7.41206202e-05, mean val. rec. loss:  7.76283806e-05\n",
      "Epoch: 6421 mean train loss:  7.41917154e-05, mean val. rec. loss:  7.91571126e-05\n",
      "Epoch: 6422 mean train loss:  7.40461255e-05, mean val. rec. loss:  7.76650157e-05\n",
      "Epoch: 6423 mean train loss:  7.40006334e-05, mean val. rec. loss:  7.66924060e-05\n",
      "Epoch: 6424 mean train loss:  7.33029619e-05, mean val. rec. loss:  7.78727143e-05\n",
      "Epoch: 6425 mean train loss:  7.44989532e-05, mean val. rec. loss:  7.89923457e-05\n",
      "Epoch: 6426 mean train loss:  7.38097191e-05, mean val. rec. loss:  7.75425990e-05\n",
      "Epoch: 6427 mean train loss:  7.33209340e-05, mean val. rec. loss:  7.82153683e-05\n",
      "Epoch: 6428 mean train loss:  7.35133402e-05, mean val. rec. loss:  7.74481311e-05\n",
      "Epoch: 6429 mean train loss:  7.36689632e-05, mean val. rec. loss:  7.77836798e-05\n",
      "Epoch: 6430 mean train loss:  7.27723298e-05, mean val. rec. loss:  7.70103732e-05\n",
      "Epoch: 6431 mean train loss:  7.33065442e-05, mean val. rec. loss:  7.75255626e-05\n",
      "Epoch: 6432 mean train loss:  7.33507340e-05, mean val. rec. loss:  7.71187791e-05\n",
      "Epoch: 6433 mean train loss:  7.34701193e-05, mean val. rec. loss:  7.72637020e-05\n",
      "Epoch: 6434 mean train loss:  7.35936346e-05, mean val. rec. loss:  7.65377519e-05\n",
      "Epoch: 6435 mean train loss:  7.34062006e-05, mean val. rec. loss:  7.84858243e-05\n",
      "Epoch: 6436 mean train loss:  7.37650725e-05, mean val. rec. loss:  7.86943043e-05\n",
      "Epoch: 6437 mean train loss:  7.32180529e-05, mean val. rec. loss:  7.69222564e-05\n",
      "Epoch: 6438 mean train loss:  7.29014345e-05, mean val. rec. loss:  7.88600070e-05\n",
      "Epoch: 6439 mean train loss:  7.30956523e-05, mean val. rec. loss:  7.75032472e-05\n",
      "Epoch: 6440 mean train loss:  7.28919684e-05, mean val. rec. loss:  7.67944244e-05\n",
      "Epoch: 6441 mean train loss:  7.32583756e-05, mean val. rec. loss:  7.72260947e-05\n",
      "Epoch: 6442 mean train loss:  7.41399267e-05, mean val. rec. loss:  7.70793637e-05\n",
      "Epoch: 6443 mean train loss:  7.37278668e-05, mean val. rec. loss:  7.93181178e-05\n",
      "Epoch: 6444 mean train loss:  7.35864376e-05, mean val. rec. loss:  7.85682805e-05\n",
      "Epoch: 6445 mean train loss:  7.37292311e-05, mean val. rec. loss:  7.61118468e-05\n",
      "Epoch: 6446 mean train loss:  7.32641392e-05, mean val. rec. loss:  7.93429864e-05\n",
      "Epoch: 6447 mean train loss:  7.34323151e-05, mean val. rec. loss:  7.70489072e-05\n",
      "Epoch: 6448 mean train loss:  7.26825558e-05, mean val. rec. loss:  7.80405522e-05\n",
      "Epoch: 6449 mean train loss:  7.31865375e-05, mean val. rec. loss:  7.66696181e-05\n",
      "Epoch: 6450 mean train loss:  7.25981116e-05, mean val. rec. loss:  7.81146673e-05\n",
      "Epoch: 6451 mean train loss:  7.32010276e-05, mean val. rec. loss:  7.69036845e-05\n",
      "Epoch: 6452 mean train loss:  7.29279300e-05, mean val. rec. loss:  7.56982260e-05\n",
      "Epoch: 6453 mean train loss:  7.28782026e-05, mean val. rec. loss:  7.78919313e-05\n",
      "Epoch: 6454 mean train loss:  7.31683051e-05, mean val. rec. loss:  7.53389717e-05\n",
      "Epoch: 6455 mean train loss:  7.25042445e-05, mean val. rec. loss:  7.76762097e-05\n",
      "Epoch: 6456 mean train loss:  7.29175234e-05, mean val. rec. loss:  7.55562697e-05\n",
      "Epoch: 6457 mean train loss:  7.34480416e-05, mean val. rec. loss:  7.90106178e-05\n",
      "Epoch: 6458 mean train loss:  7.30409561e-05, mean val. rec. loss:  7.70868052e-05\n",
      "Epoch: 6459 mean train loss:  7.25440000e-05, mean val. rec. loss:  7.58742823e-05\n",
      "Epoch: 6460 mean train loss:  7.22277602e-05, mean val. rec. loss:  7.70301535e-05\n",
      "Epoch: 6461 mean train loss:  7.22595964e-05, mean val. rec. loss:  7.64989998e-05\n",
      "Epoch: 6462 mean train loss:  7.28976575e-05, mean val. rec. loss:  7.68957705e-05\n",
      "Epoch: 6463 mean train loss:  7.28357510e-05, mean val. rec. loss:  7.59508915e-05\n",
      "Epoch: 6464 mean train loss:  7.25809430e-05, mean val. rec. loss:  7.68047644e-05\n",
      "Epoch: 6465 mean train loss:  7.24250187e-05, mean val. rec. loss:  7.72763225e-05\n",
      "Epoch: 6466 mean train loss:  7.27713763e-05, mean val. rec. loss:  7.76819794e-05\n",
      "Epoch: 6467 mean train loss:  7.25990242e-05, mean val. rec. loss:  7.55973615e-05\n",
      "Epoch: 6468 mean train loss:  7.28567396e-05, mean val. rec. loss:  7.88873834e-05\n",
      "Epoch: 6469 mean train loss:  7.28576311e-05, mean val. rec. loss:  7.73481570e-05\n",
      "Epoch: 6470 mean train loss:  7.29170078e-05, mean val. rec. loss:  7.86213341e-05\n",
      "Epoch: 6471 mean train loss:  7.26492915e-05, mean val. rec. loss:  7.70036131e-05\n",
      "Epoch: 6472 mean train loss:  7.25276217e-05, mean val. rec. loss:  7.61593851e-05\n",
      "Epoch: 6473 mean train loss:  7.20618359e-05, mean val. rec. loss:  7.53691602e-05\n",
      "Epoch: 6474 mean train loss:  7.21050827e-05, mean val. rec. loss:  7.54487860e-05\n",
      "Epoch: 6475 mean train loss:  7.19223854e-05, mean val. rec. loss:  7.65391603e-05\n",
      "Epoch: 6476 mean train loss:  7.20777462e-05, mean val. rec. loss:  7.53072431e-05\n",
      "Epoch: 6477 mean train loss:  7.20700094e-05, mean val. rec. loss:  7.60516424e-05\n",
      "Epoch: 6478 mean train loss:  7.18782010e-05, mean val. rec. loss:  7.64960469e-05\n",
      "Epoch: 6479 mean train loss:  7.18316235e-05, mean val. rec. loss:  7.53426379e-05\n",
      "Epoch: 6480 mean train loss:  7.21148513e-05, mean val. rec. loss:  7.63674971e-05\n",
      "Epoch: 6481 mean train loss:  7.14960526e-05, mean val. rec. loss:  7.50566674e-05\n",
      "Epoch: 6482 mean train loss:  7.16545306e-05, mean val. rec. loss:  7.68244085e-05\n",
      "Epoch: 6483 mean train loss:  7.20620226e-05, mean val. rec. loss:  7.65699439e-05\n",
      "Epoch: 6484 mean train loss:  7.30017010e-05, mean val. rec. loss:  7.68122241e-05\n",
      "Epoch: 6485 mean train loss:  7.22011391e-05, mean val. rec. loss:  7.61790428e-05\n",
      "Epoch: 6486 mean train loss:  7.33581306e-05, mean val. rec. loss:  7.61939803e-05\n",
      "Epoch: 6487 mean train loss:  7.19476778e-05, mean val. rec. loss:  7.61954795e-05\n",
      "Epoch: 6488 mean train loss:  7.21552848e-05, mean val. rec. loss:  7.54448290e-05\n",
      "Epoch: 6489 mean train loss:  7.28264213e-05, mean val. rec. loss:  7.57629234e-05\n",
      "Epoch: 6490 mean train loss:  7.34000284e-05, mean val. rec. loss:  7.91934841e-05\n",
      "Epoch: 6491 mean train loss:  7.25660794e-05, mean val. rec. loss:  7.49592919e-05\n",
      "Epoch: 6492 mean train loss:  7.20327875e-05, mean val. rec. loss:  7.49972808e-05\n",
      "Epoch: 6493 mean train loss:  7.19822399e-05, mean val. rec. loss:  7.76870767e-05\n",
      "Epoch: 6494 mean train loss:  7.25481926e-05, mean val. rec. loss:  7.47529654e-05\n",
      "Epoch: 6495 mean train loss:  7.17375827e-05, mean val. rec. loss:  7.68727100e-05\n",
      "Epoch: 6496 mean train loss:  7.17558884e-05, mean val. rec. loss:  7.60234529e-05\n",
      "Epoch: 6497 mean train loss:  7.15852068e-05, mean val. rec. loss:  7.55752505e-05\n",
      "Epoch: 6498 mean train loss:  7.13183650e-05, mean val. rec. loss:  7.59150469e-05\n",
      "Epoch: 6499 mean train loss:  7.16153355e-05, mean val. rec. loss:  7.51127194e-05\n",
      "Epoch: 6500 mean train loss:  7.16527058e-05, mean val. rec. loss:  7.50021646e-05\n",
      "Epoch: 6501 mean train loss:  7.09423057e-05, mean val. rec. loss:  7.55608264e-05\n",
      "Epoch: 6502 mean train loss:  7.16821468e-05, mean val. rec. loss:  7.49635715e-05\n",
      "Epoch: 6503 mean train loss:  7.10286421e-05, mean val. rec. loss:  7.51979740e-05\n",
      "Epoch: 6504 mean train loss:  7.17056164e-05, mean val. rec. loss:  7.53069796e-05\n",
      "Epoch: 6505 mean train loss:  7.15914941e-05, mean val. rec. loss:  7.50694878e-05\n",
      "Epoch: 6506 mean train loss:  7.15689433e-05, mean val. rec. loss:  7.53625273e-05\n",
      "Epoch: 6507 mean train loss:  7.16338209e-05, mean val. rec. loss:  7.51488138e-05\n",
      "Epoch: 6508 mean train loss:  7.15680706e-05, mean val. rec. loss:  7.43819128e-05\n",
      "Epoch: 6509 mean train loss:  7.16537039e-05, mean val. rec. loss:  7.54544784e-05\n",
      "Epoch: 6510 mean train loss:  7.06393617e-05, mean val. rec. loss:  7.41877162e-05\n",
      "Epoch: 6511 mean train loss:  7.08148305e-05, mean val. rec. loss:  7.54864432e-05\n",
      "Epoch: 6512 mean train loss:  7.15220270e-05, mean val. rec. loss:  7.40938752e-05\n",
      "Epoch: 6513 mean train loss:  7.08897010e-05, mean val. rec. loss:  7.48022618e-05\n",
      "Epoch: 6514 mean train loss:  7.06535317e-05, mean val. rec. loss:  7.49532042e-05\n",
      "Epoch: 6515 mean train loss:  7.04462363e-05, mean val. rec. loss:  7.54248351e-05\n",
      "Epoch: 6516 mean train loss:  7.08657965e-05, mean val. rec. loss:  7.40137042e-05\n",
      "Epoch: 6517 mean train loss:  7.07959672e-05, mean val. rec. loss:  7.46957367e-05\n",
      "Epoch: 6518 mean train loss:  7.04217723e-05, mean val. rec. loss:  7.52764322e-05\n",
      "Epoch: 6519 mean train loss:  7.08134787e-05, mean val. rec. loss:  7.56537905e-05\n",
      "Epoch: 6520 mean train loss:  7.09199281e-05, mean val. rec. loss:  7.43040224e-05\n",
      "Epoch: 6521 mean train loss:  7.05352719e-05, mean val. rec. loss:  7.50093198e-05\n",
      "Epoch: 6522 mean train loss:  7.03946090e-05, mean val. rec. loss:  7.41671498e-05\n",
      "Epoch: 6523 mean train loss:  7.07560625e-05, mean val. rec. loss:  7.42853596e-05\n",
      "Epoch: 6524 mean train loss:  7.07923272e-05, mean val. rec. loss:  7.53174604e-05\n",
      "Epoch: 6525 mean train loss:  7.12415374e-05, mean val. rec. loss:  7.58237319e-05\n",
      "Epoch: 6526 mean train loss:  7.20363996e-05, mean val. rec. loss:  7.49319792e-05\n",
      "Epoch: 6527 mean train loss:  7.01934601e-05, mean val. rec. loss:  7.46156566e-05\n",
      "Epoch: 6528 mean train loss:  7.04201024e-05, mean val. rec. loss:  7.45403558e-05\n",
      "Epoch: 6529 mean train loss:  7.08944839e-05, mean val. rec. loss:  7.52972893e-05\n",
      "Epoch: 6530 mean train loss:  7.16572035e-05, mean val. rec. loss:  7.71395499e-05\n",
      "Epoch: 6531 mean train loss:  7.12520819e-05, mean val. rec. loss:  7.34345079e-05\n",
      "Epoch: 6532 mean train loss:  7.08072289e-05, mean val. rec. loss:  7.59546168e-05\n",
      "Epoch: 6533 mean train loss:  7.03789036e-05, mean val. rec. loss:  7.33077208e-05\n",
      "Epoch: 6534 mean train loss:  7.03747115e-05, mean val. rec. loss:  7.58097030e-05\n",
      "Epoch: 6535 mean train loss:  7.12512429e-05, mean val. rec. loss:  7.63879680e-05\n",
      "Epoch: 6536 mean train loss:  7.17290652e-05, mean val. rec. loss:  7.52855955e-05\n",
      "Epoch: 6537 mean train loss:  7.03149147e-05, mean val. rec. loss:  7.62586459e-05\n",
      "Epoch: 6538 mean train loss:  7.06099566e-05, mean val. rec. loss:  7.42047480e-05\n",
      "Epoch: 6539 mean train loss:  7.07882744e-05, mean val. rec. loss:  7.51059957e-05\n",
      "Epoch: 6540 mean train loss:  7.07541208e-05, mean val. rec. loss:  7.56863232e-05\n",
      "Epoch: 6541 mean train loss:  7.08441467e-05, mean val. rec. loss:  7.38412778e-05\n",
      "Epoch: 6542 mean train loss:  7.12591905e-05, mean val. rec. loss:  7.47288418e-05\n",
      "Epoch: 6543 mean train loss:  7.02505511e-05, mean val. rec. loss:  7.75132510e-05\n",
      "Epoch: 6544 mean train loss:  7.14808154e-05, mean val. rec. loss:  7.32078421e-05\n",
      "Epoch: 6545 mean train loss:  7.10622754e-05, mean val. rec. loss:  7.53454591e-05\n",
      "Epoch: 6546 mean train loss:  7.03905911e-05, mean val. rec. loss:  7.65260672e-05\n",
      "Epoch: 6547 mean train loss:  7.14140072e-05, mean val. rec. loss:  7.48512676e-05\n",
      "Epoch: 6548 mean train loss:  7.04057993e-05, mean val. rec. loss:  7.41739916e-05\n",
      "Epoch: 6549 mean train loss:  6.99710178e-05, mean val. rec. loss:  7.59574653e-05\n",
      "Epoch: 6550 mean train loss:  7.05240256e-05, mean val. rec. loss:  7.41850857e-05\n",
      "Epoch: 6551 mean train loss:  7.06641666e-05, mean val. rec. loss:  7.53329840e-05\n",
      "Epoch: 6552 mean train loss:  7.00088623e-05, mean val. rec. loss:  7.30432297e-05\n",
      "Epoch: 6553 mean train loss:  7.06314313e-05, mean val. rec. loss:  7.56120355e-05\n",
      "Epoch: 6554 mean train loss:  7.13460315e-05, mean val. rec. loss:  7.71709695e-05\n",
      "Epoch: 6555 mean train loss:  7.07587803e-05, mean val. rec. loss:  7.33723273e-05\n",
      "Epoch: 6556 mean train loss:  7.02023980e-05, mean val. rec. loss:  7.36409753e-05\n",
      "Epoch: 6557 mean train loss:  7.02762949e-05, mean val. rec. loss:  7.48906648e-05\n",
      "Epoch: 6558 mean train loss:  6.99032615e-05, mean val. rec. loss:  7.48803930e-05\n",
      "Epoch: 6559 mean train loss:  7.04761426e-05, mean val. rec. loss:  7.66456309e-05\n",
      "Epoch: 6560 mean train loss:  7.15605182e-05, mean val. rec. loss:  7.42812164e-05\n",
      "Epoch: 6561 mean train loss:  7.07348576e-05, mean val. rec. loss:  7.38851181e-05\n",
      "Epoch: 6562 mean train loss:  7.06707922e-05, mean val. rec. loss:  7.39236612e-05\n",
      "Epoch: 6563 mean train loss:  6.94021747e-05, mean val. rec. loss:  7.45480835e-05\n",
      "Epoch: 6564 mean train loss:  6.96739080e-05, mean val. rec. loss:  7.55173540e-05\n",
      "Epoch: 6565 mean train loss:  6.98559284e-05, mean val. rec. loss:  7.25561480e-05\n",
      "Epoch: 6566 mean train loss:  7.01056081e-05, mean val. rec. loss:  7.58752772e-05\n",
      "Epoch: 6567 mean train loss:  7.03123828e-05, mean val. rec. loss:  7.32096412e-05\n",
      "Epoch: 6568 mean train loss:  6.94840316e-05, mean val. rec. loss:  7.34402094e-05\n",
      "Epoch: 6569 mean train loss:  6.94784920e-05, mean val. rec. loss:  7.41377109e-05\n",
      "Epoch: 6570 mean train loss:  7.03772615e-05, mean val. rec. loss:  7.26286685e-05\n",
      "Epoch: 6571 mean train loss:  6.97397178e-05, mean val. rec. loss:  7.58053689e-05\n",
      "Epoch: 6572 mean train loss:  6.99109424e-05, mean val. rec. loss:  7.24585500e-05\n",
      "Epoch: 6573 mean train loss:  6.98858860e-05, mean val. rec. loss:  7.31052422e-05\n",
      "Epoch: 6574 mean train loss:  6.95423604e-05, mean val. rec. loss:  7.38143194e-05\n",
      "Epoch: 6575 mean train loss:  6.93663032e-05, mean val. rec. loss:  7.30872699e-05\n",
      "Epoch: 6576 mean train loss:  6.91303158e-05, mean val. rec. loss:  7.27481413e-05\n",
      "Epoch: 6577 mean train loss:  6.91287463e-05, mean val. rec. loss:  7.38926913e-05\n",
      "Epoch: 6578 mean train loss:  6.90399625e-05, mean val. rec. loss:  7.33660261e-05\n",
      "Epoch: 6579 mean train loss:  6.89573050e-05, mean val. rec. loss:  7.20769440e-05\n",
      "Epoch: 6580 mean train loss:  6.92140644e-05, mean val. rec. loss:  7.33351199e-05\n",
      "Epoch: 6581 mean train loss:  6.90223859e-05, mean val. rec. loss:  7.22479711e-05\n",
      "Epoch: 6582 mean train loss:  6.93952876e-05, mean val. rec. loss:  7.30136228e-05\n",
      "Epoch: 6583 mean train loss:  6.93653318e-05, mean val. rec. loss:  7.28592549e-05\n",
      "Epoch: 6584 mean train loss:  6.93636522e-05, mean val. rec. loss:  7.62151282e-05\n",
      "Epoch: 6585 mean train loss:  6.92293684e-05, mean val. rec. loss:  7.30600481e-05\n",
      "Epoch: 6586 mean train loss:  6.91480923e-05, mean val. rec. loss:  7.29138350e-05\n",
      "Epoch: 6587 mean train loss:  6.90937475e-05, mean val. rec. loss:  7.23324307e-05\n",
      "Epoch: 6588 mean train loss:  6.92831514e-05, mean val. rec. loss:  7.23241488e-05\n",
      "Epoch: 6589 mean train loss:  6.96949728e-05, mean val. rec. loss:  7.33045225e-05\n",
      "Epoch: 6590 mean train loss:  6.88728219e-05, mean val. rec. loss:  7.33598022e-05\n",
      "Epoch: 6591 mean train loss:  6.97510477e-05, mean val. rec. loss:  7.48868396e-05\n",
      "Epoch: 6592 mean train loss:  6.98322800e-05, mean val. rec. loss:  7.28619171e-05\n",
      "Epoch: 6593 mean train loss:  6.87997279e-05, mean val. rec. loss:  7.18682005e-05\n",
      "Epoch: 6594 mean train loss:  6.85847811e-05, mean val. rec. loss:  7.36487257e-05\n",
      "Epoch: 6595 mean train loss:  6.90708275e-05, mean val. rec. loss:  7.31205431e-05\n",
      "Epoch: 6596 mean train loss:  6.93282472e-05, mean val. rec. loss:  7.28721844e-05\n",
      "Epoch: 6597 mean train loss:  6.92578607e-05, mean val. rec. loss:  7.34008712e-05\n",
      "Epoch: 6598 mean train loss:  6.94737996e-05, mean val. rec. loss:  7.28453759e-05\n",
      "Epoch: 6599 mean train loss:  6.93675367e-05, mean val. rec. loss:  7.22491796e-05\n",
      "Epoch: 6600 mean train loss:  6.85717298e-05, mean val. rec. loss:  7.25736160e-05\n",
      "Epoch: 6601 mean train loss:  6.87985430e-05, mean val. rec. loss:  7.25966674e-05\n",
      "Epoch: 6602 mean train loss:  6.88145995e-05, mean val. rec. loss:  7.27107703e-05\n",
      "Epoch: 6603 mean train loss:  6.87500738e-05, mean val. rec. loss:  7.16643999e-05\n",
      "Epoch: 6604 mean train loss:  6.86380496e-05, mean val. rec. loss:  7.24927772e-05\n",
      "Epoch: 6605 mean train loss:  6.79847279e-05, mean val. rec. loss:  7.18068513e-05\n",
      "Epoch: 6606 mean train loss:  6.82411328e-05, mean val. rec. loss:  7.23478907e-05\n",
      "Epoch: 6607 mean train loss:  6.81244715e-05, mean val. rec. loss:  7.22437461e-05\n",
      "Epoch: 6608 mean train loss:  6.82951546e-05, mean val. rec. loss:  7.36066481e-05\n",
      "Epoch: 6609 mean train loss:  6.85170965e-05, mean val. rec. loss:  7.22449636e-05\n",
      "Epoch: 6610 mean train loss:  6.88451859e-05, mean val. rec. loss:  7.11379573e-05\n",
      "Epoch: 6611 mean train loss:  6.82798162e-05, mean val. rec. loss:  7.29814944e-05\n",
      "Epoch: 6612 mean train loss:  6.83464075e-05, mean val. rec. loss:  7.19801954e-05\n",
      "Epoch: 6613 mean train loss:  6.85038979e-05, mean val. rec. loss:  7.21739287e-05\n",
      "Epoch: 6614 mean train loss:  6.85017263e-05, mean val. rec. loss:  7.22775508e-05\n",
      "Epoch: 6615 mean train loss:  6.90613090e-05, mean val. rec. loss:  7.25561617e-05\n",
      "Epoch: 6616 mean train loss:  6.82208205e-05, mean val. rec. loss:  7.29050124e-05\n",
      "Epoch: 6617 mean train loss:  6.85083985e-05, mean val. rec. loss:  7.20680123e-05\n",
      "Epoch: 6618 mean train loss:  6.86944384e-05, mean val. rec. loss:  7.22024681e-05\n",
      "Epoch: 6619 mean train loss:  6.88099770e-05, mean val. rec. loss:  7.33592070e-05\n",
      "Epoch: 6620 mean train loss:  6.84883796e-05, mean val. rec. loss:  7.23368965e-05\n",
      "Epoch: 6621 mean train loss:  6.87755098e-05, mean val. rec. loss:  7.21746738e-05\n",
      "Epoch: 6622 mean train loss:  6.89206110e-05, mean val. rec. loss:  7.26570761e-05\n",
      "Epoch: 6623 mean train loss:  6.84684824e-05, mean val. rec. loss:  7.19711457e-05\n",
      "Epoch: 6624 mean train loss:  6.78741564e-05, mean val. rec. loss:  7.23240034e-05\n",
      "Epoch: 6625 mean train loss:  6.75365042e-05, mean val. rec. loss:  7.07043289e-05\n",
      "Epoch: 6626 mean train loss:  6.78833707e-05, mean val. rec. loss:  7.19970546e-05\n",
      "Epoch: 6627 mean train loss:  6.80730461e-05, mean val. rec. loss:  7.20476777e-05\n",
      "Epoch: 6628 mean train loss:  6.84808670e-05, mean val. rec. loss:  7.16994358e-05\n",
      "Epoch: 6629 mean train loss:  6.86703064e-05, mean val. rec. loss:  7.25268227e-05\n",
      "Epoch: 6630 mean train loss:  6.83539706e-05, mean val. rec. loss:  7.20800332e-05\n",
      "Epoch: 6631 mean train loss:  6.78603401e-05, mean val. rec. loss:  7.15962588e-05\n",
      "Epoch: 6632 mean train loss:  6.84948403e-05, mean val. rec. loss:  7.16131817e-05\n",
      "Epoch: 6633 mean train loss:  6.84298336e-05, mean val. rec. loss:  7.29530368e-05\n",
      "Epoch: 6634 mean train loss:  6.91559991e-05, mean val. rec. loss:  7.21757459e-05\n",
      "Epoch: 6635 mean train loss:  6.81880690e-05, mean val. rec. loss:  7.17030702e-05\n",
      "Epoch: 6636 mean train loss:  6.77812618e-05, mean val. rec. loss:  7.10624020e-05\n",
      "Epoch: 6637 mean train loss:  6.80457347e-05, mean val. rec. loss:  7.08780910e-05\n",
      "Epoch: 6638 mean train loss:  6.81957521e-05, mean val. rec. loss:  7.27896828e-05\n",
      "Epoch: 6639 mean train loss:  6.81246162e-05, mean val. rec. loss:  7.05166152e-05\n",
      "Epoch: 6640 mean train loss:  6.83367923e-05, mean val. rec. loss:  7.15983986e-05\n",
      "Epoch: 6641 mean train loss:  6.78644804e-05, mean val. rec. loss:  7.31859447e-05\n",
      "Epoch: 6642 mean train loss:  6.77270276e-05, mean val. rec. loss:  7.13137773e-05\n",
      "Epoch: 6643 mean train loss:  6.76305382e-05, mean val. rec. loss:  7.24470652e-05\n",
      "Epoch: 6644 mean train loss:  6.77568021e-05, mean val. rec. loss:  7.27900099e-05\n",
      "Epoch: 6645 mean train loss:  6.79174925e-05, mean val. rec. loss:  7.05925884e-05\n",
      "Epoch: 6646 mean train loss:  6.73477994e-05, mean val. rec. loss:  7.15587652e-05\n",
      "Epoch: 6647 mean train loss:  6.70876335e-05, mean val. rec. loss:  7.09695106e-05\n",
      "Epoch: 6648 mean train loss:  6.78439243e-05, mean val. rec. loss:  7.19771380e-05\n",
      "Epoch: 6649 mean train loss:  6.74628507e-05, mean val. rec. loss:  7.15695322e-05\n",
      "Epoch: 6650 mean train loss:  6.77701118e-05, mean val. rec. loss:  7.25363449e-05\n",
      "Epoch: 6651 mean train loss:  6.80266591e-05, mean val. rec. loss:  7.18441951e-05\n",
      "Epoch: 6652 mean train loss:  6.77204293e-05, mean val. rec. loss:  7.14361668e-05\n",
      "Epoch: 6653 mean train loss:  6.76714752e-05, mean val. rec. loss:  7.14740739e-05\n",
      "Epoch: 6654 mean train loss:  6.71722769e-05, mean val. rec. loss:  7.25460398e-05\n",
      "Epoch: 6655 mean train loss:  6.73442526e-05, mean val. rec. loss:  6.99182836e-05\n",
      "Epoch: 6656 mean train loss:  6.72512881e-05, mean val. rec. loss:  7.10521438e-05\n",
      "Epoch: 6657 mean train loss:  6.73659050e-05, mean val. rec. loss:  7.07370842e-05\n",
      "Epoch: 6658 mean train loss:  6.71458036e-05, mean val. rec. loss:  7.15374492e-05\n",
      "Epoch: 6659 mean train loss:  6.70154966e-05, mean val. rec. loss:  7.10164310e-05\n",
      "Epoch: 6660 mean train loss:  6.70211826e-05, mean val. rec. loss:  7.05375359e-05\n",
      "Epoch: 6661 mean train loss:  6.72304391e-05, mean val. rec. loss:  7.19062166e-05\n",
      "Epoch: 6662 mean train loss:  6.69801902e-05, mean val. rec. loss:  6.94768912e-05\n",
      "Epoch: 6663 mean train loss:  6.71492474e-05, mean val. rec. loss:  6.99364876e-05\n",
      "Epoch: 6664 mean train loss:  6.68586538e-05, mean val. rec. loss:  7.22173147e-05\n",
      "Epoch: 6665 mean train loss:  6.76128556e-05, mean val. rec. loss:  7.03367655e-05\n",
      "Epoch: 6666 mean train loss:  6.80043795e-05, mean val. rec. loss:  7.05718858e-05\n",
      "Epoch: 6667 mean train loss:  6.76114154e-05, mean val. rec. loss:  7.37389731e-05\n",
      "Epoch: 6668 mean train loss:  6.83844586e-05, mean val. rec. loss:  7.18607545e-05\n",
      "Epoch: 6669 mean train loss:  6.76253463e-05, mean val. rec. loss:  7.14894021e-05\n",
      "Epoch: 6670 mean train loss:  6.77095795e-05, mean val. rec. loss:  6.94173547e-05\n",
      "Epoch: 6671 mean train loss:  6.70459488e-05, mean val. rec. loss:  7.36290725e-05\n",
      "Epoch: 6672 mean train loss:  6.74193037e-05, mean val. rec. loss:  6.87956174e-05\n",
      "Epoch: 6673 mean train loss:  6.71538916e-05, mean val. rec. loss:  7.44626925e-05\n",
      "Epoch: 6674 mean train loss:  6.81497790e-05, mean val. rec. loss:  7.09083204e-05\n",
      "Epoch: 6675 mean train loss:  6.73553085e-05, mean val. rec. loss:  7.18483156e-05\n",
      "Epoch: 6676 mean train loss:  6.71782600e-05, mean val. rec. loss:  6.99696472e-05\n",
      "Epoch: 6677 mean train loss:  6.68565731e-05, mean val. rec. loss:  7.19720452e-05\n",
      "Epoch: 6678 mean train loss:  6.72444897e-05, mean val. rec. loss:  7.09456187e-05\n",
      "Epoch: 6679 mean train loss:  6.74206424e-05, mean val. rec. loss:  7.05855467e-05\n",
      "Epoch: 6680 mean train loss:  6.72575180e-05, mean val. rec. loss:  7.06654133e-05\n",
      "Epoch: 6681 mean train loss:  6.70003277e-05, mean val. rec. loss:  7.03189795e-05\n",
      "Epoch: 6682 mean train loss:  6.64535057e-05, mean val. rec. loss:  7.10517668e-05\n",
      "Epoch: 6683 mean train loss:  6.64939625e-05, mean val. rec. loss:  7.00113704e-05\n",
      "Epoch: 6684 mean train loss:  6.75192043e-05, mean val. rec. loss:  6.98648121e-05\n",
      "Epoch: 6685 mean train loss:  6.66483517e-05, mean val. rec. loss:  7.02028413e-05\n",
      "Epoch: 6686 mean train loss:  6.63074962e-05, mean val. rec. loss:  7.02722362e-05\n",
      "Epoch: 6687 mean train loss:  6.65069612e-05, mean val. rec. loss:  7.10274979e-05\n",
      "Epoch: 6688 mean train loss:  6.68786044e-05, mean val. rec. loss:  7.05635130e-05\n",
      "Epoch: 6689 mean train loss:  6.58977716e-05, mean val. rec. loss:  6.89226044e-05\n",
      "Epoch: 6690 mean train loss:  6.64421991e-05, mean val. rec. loss:  7.40460415e-05\n",
      "Epoch: 6691 mean train loss:  6.69197830e-05, mean val. rec. loss:  7.01314247e-05\n",
      "Epoch: 6692 mean train loss:  6.69462123e-05, mean val. rec. loss:  6.93436894e-05\n",
      "Epoch: 6693 mean train loss:  6.67114451e-05, mean val. rec. loss:  7.26608241e-05\n",
      "Epoch: 6694 mean train loss:  6.69130730e-05, mean val. rec. loss:  7.02944471e-05\n",
      "Epoch: 6695 mean train loss:  6.70235883e-05, mean val. rec. loss:  7.10401411e-05\n",
      "Epoch: 6696 mean train loss:  6.73345895e-05, mean val. rec. loss:  6.95906807e-05\n",
      "Epoch: 6697 mean train loss:  6.63921439e-05, mean val. rec. loss:  6.99585304e-05\n",
      "Epoch: 6698 mean train loss:  6.66733317e-05, mean val. rec. loss:  7.11387568e-05\n",
      "Epoch: 6699 mean train loss:  6.64953160e-05, mean val. rec. loss:  7.05538181e-05\n",
      "Epoch: 6700 mean train loss:  6.75002986e-05, mean val. rec. loss:  6.94192764e-05\n",
      "Epoch: 6701 mean train loss:  6.67399232e-05, mean val. rec. loss:  7.02030502e-05\n",
      "Epoch: 6702 mean train loss:  6.67345837e-05, mean val. rec. loss:  6.97610491e-05\n",
      "Epoch: 6703 mean train loss:  6.60490695e-05, mean val. rec. loss:  6.86034833e-05\n",
      "Epoch: 6704 mean train loss:  6.59336344e-05, mean val. rec. loss:  7.10605394e-05\n",
      "Epoch: 6705 mean train loss:  6.60610664e-05, mean val. rec. loss:  7.00925545e-05\n",
      "Epoch: 6706 mean train loss:  6.63104965e-05, mean val. rec. loss:  6.94281853e-05\n",
      "Epoch: 6707 mean train loss:  6.56034072e-05, mean val. rec. loss:  6.83251041e-05\n",
      "Epoch: 6708 mean train loss:  6.55661876e-05, mean val. rec. loss:  7.04988519e-05\n",
      "Epoch: 6709 mean train loss:  6.60333109e-05, mean val. rec. loss:  6.89395726e-05\n",
      "Epoch: 6710 mean train loss:  6.60216631e-05, mean val. rec. loss:  7.04662375e-05\n",
      "Epoch: 6711 mean train loss:  6.59080685e-05, mean val. rec. loss:  6.99602431e-05\n",
      "Epoch: 6712 mean train loss:  6.56246434e-05, mean val. rec. loss:  6.98642805e-05\n",
      "Epoch: 6713 mean train loss:  6.58589228e-05, mean val. rec. loss:  6.97410098e-05\n",
      "Epoch: 6714 mean train loss:  6.53853594e-05, mean val. rec. loss:  7.09513747e-05\n",
      "Epoch: 6715 mean train loss:  6.65341620e-05, mean val. rec. loss:  6.94524860e-05\n",
      "Epoch: 6716 mean train loss:  6.62585480e-05, mean val. rec. loss:  6.88989987e-05\n",
      "Epoch: 6717 mean train loss:  6.53801957e-05, mean val. rec. loss:  6.98468398e-05\n",
      "Epoch: 6718 mean train loss:  6.60362340e-05, mean val. rec. loss:  6.97742830e-05\n",
      "Epoch: 6719 mean train loss:  6.60633953e-05, mean val. rec. loss:  7.10956207e-05\n",
      "Epoch: 6720 mean train loss:  6.64994925e-05, mean val. rec. loss:  6.97166091e-05\n",
      "Epoch: 6721 mean train loss:  6.56559064e-05, mean val. rec. loss:  6.83498910e-05\n",
      "Epoch: 6722 mean train loss:  6.54672823e-05, mean val. rec. loss:  6.92671665e-05\n",
      "Epoch: 6723 mean train loss:  6.55081343e-05, mean val. rec. loss:  6.88176511e-05\n",
      "Epoch: 6724 mean train loss:  6.57163508e-05, mean val. rec. loss:  6.92087248e-05\n",
      "Epoch: 6725 mean train loss:  6.56862233e-05, mean val. rec. loss:  6.84727438e-05\n",
      "Epoch: 6726 mean train loss:  6.57541490e-05, mean val. rec. loss:  7.04741333e-05\n",
      "Epoch: 6727 mean train loss:  6.57943943e-05, mean val. rec. loss:  6.92320034e-05\n",
      "Epoch: 6728 mean train loss:  6.54998570e-05, mean val. rec. loss:  6.96318133e-05\n",
      "Epoch: 6729 mean train loss:  6.68232868e-05, mean val. rec. loss:  7.07098760e-05\n",
      "Epoch: 6730 mean train loss:  6.61256495e-05, mean val. rec. loss:  7.03305233e-05\n",
      "Epoch: 6731 mean train loss:  6.64487462e-05, mean val. rec. loss:  6.99583168e-05\n",
      "Epoch: 6732 mean train loss:  6.64857267e-05, mean val. rec. loss:  6.87841780e-05\n",
      "Epoch: 6733 mean train loss:  6.62477926e-05, mean val. rec. loss:  7.01581923e-05\n",
      "Epoch: 6734 mean train loss:  6.55083921e-05, mean val. rec. loss:  6.88836206e-05\n",
      "Epoch: 6735 mean train loss:  6.54963861e-05, mean val. rec. loss:  6.91160878e-05\n",
      "Epoch: 6736 mean train loss:  6.48992719e-05, mean val. rec. loss:  6.91821026e-05\n",
      "Epoch: 6737 mean train loss:  6.54683009e-05, mean val. rec. loss:  7.02131812e-05\n",
      "Epoch: 6738 mean train loss:  6.58348807e-05, mean val. rec. loss:  6.77270451e-05\n",
      "Epoch: 6739 mean train loss:  6.52873602e-05, mean val. rec. loss:  6.82809640e-05\n",
      "Epoch: 6740 mean train loss:  6.55976160e-05, mean val. rec. loss:  7.10263667e-05\n",
      "Epoch: 6741 mean train loss:  6.56937679e-05, mean val. rec. loss:  6.82157850e-05\n",
      "Epoch: 6742 mean train loss:  6.52485838e-05, mean val. rec. loss:  6.93032473e-05\n",
      "Epoch: 6743 mean train loss:  6.52100414e-05, mean val. rec. loss:  6.79346892e-05\n",
      "Epoch: 6744 mean train loss:  6.50651872e-05, mean val. rec. loss:  6.99473227e-05\n",
      "Epoch: 6745 mean train loss:  6.51295135e-05, mean val. rec. loss:  6.80517314e-05\n",
      "Epoch: 6746 mean train loss:  6.51930686e-05, mean val. rec. loss:  6.87513410e-05\n",
      "Epoch: 6747 mean train loss:  6.48407529e-05, mean val. rec. loss:  6.91598054e-05\n",
      "Epoch: 6748 mean train loss:  6.51542089e-05, mean val. rec. loss:  6.82981821e-05\n",
      "Epoch: 6749 mean train loss:  6.51347950e-05, mean val. rec. loss:  6.86275341e-05\n",
      "Epoch: 6750 mean train loss:  6.47500064e-05, mean val. rec. loss:  6.93790842e-05\n",
      "Epoch: 6751 mean train loss:  6.51772071e-05, mean val. rec. loss:  6.82928849e-05\n",
      "Epoch: 6752 mean train loss:  6.49636019e-05, mean val. rec. loss:  6.89662584e-05\n",
      "Epoch: 6753 mean train loss:  6.53740315e-05, mean val. rec. loss:  6.82667352e-05\n",
      "Epoch: 6754 mean train loss:  6.52307087e-05, mean val. rec. loss:  6.85228444e-05\n",
      "Epoch: 6755 mean train loss:  6.59013579e-05, mean val. rec. loss:  6.84531542e-05\n",
      "Epoch: 6756 mean train loss:  6.54086482e-05, mean val. rec. loss:  6.93916593e-05\n",
      "Epoch: 6757 mean train loss:  6.52268976e-05, mean val. rec. loss:  6.95811085e-05\n",
      "Epoch: 6758 mean train loss:  6.51153535e-05, mean val. rec. loss:  6.77581331e-05\n",
      "Epoch: 6759 mean train loss:  6.44271337e-05, mean val. rec. loss:  6.81282544e-05\n",
      "Epoch: 6760 mean train loss:  6.45573048e-05, mean val. rec. loss:  6.82059948e-05\n",
      "Epoch: 6761 mean train loss:  6.47911498e-05, mean val. rec. loss:  6.87388749e-05\n",
      "Epoch: 6762 mean train loss:  6.53032132e-05, mean val. rec. loss:  6.86377151e-05\n",
      "Epoch: 6763 mean train loss:  6.53420774e-05, mean val. rec. loss:  6.88439144e-05\n",
      "Epoch: 6764 mean train loss:  6.48541906e-05, mean val. rec. loss:  6.87117984e-05\n",
      "Epoch: 6765 mean train loss:  6.49778126e-05, mean val. rec. loss:  6.85680612e-05\n",
      "Epoch: 6766 mean train loss:  6.47699281e-05, mean val. rec. loss:  6.81024363e-05\n",
      "Epoch: 6767 mean train loss:  6.47105736e-05, mean val. rec. loss:  6.89015338e-05\n",
      "Epoch: 6768 mean train loss:  6.46455948e-05, mean val. rec. loss:  7.01170142e-05\n",
      "Epoch: 6769 mean train loss:  6.49884185e-05, mean val. rec. loss:  6.73833916e-05\n",
      "Epoch: 6770 mean train loss:  6.45338696e-05, mean val. rec. loss:  6.91752517e-05\n",
      "Epoch: 6771 mean train loss:  6.47910454e-05, mean val. rec. loss:  6.84584468e-05\n",
      "Epoch: 6772 mean train loss:  6.42978973e-05, mean val. rec. loss:  6.83787302e-05\n",
      "Epoch: 6773 mean train loss:  6.41301567e-05, mean val. rec. loss:  6.86435620e-05\n",
      "Epoch: 6774 mean train loss:  6.45971518e-05, mean val. rec. loss:  6.78073524e-05\n",
      "Epoch: 6775 mean train loss:  6.52104434e-05, mean val. rec. loss:  6.78399623e-05\n",
      "Epoch: 6776 mean train loss:  6.53365469e-05, mean val. rec. loss:  6.76663910e-05\n",
      "Epoch: 6777 mean train loss:  6.44634482e-05, mean val. rec. loss:  6.79641053e-05\n",
      "Epoch: 6778 mean train loss:  6.41064039e-05, mean val. rec. loss:  6.73180037e-05\n",
      "Epoch: 6779 mean train loss:  6.42527890e-05, mean val. rec. loss:  6.71049762e-05\n",
      "Epoch: 6780 mean train loss:  6.38168002e-05, mean val. rec. loss:  6.76661866e-05\n",
      "Epoch: 6781 mean train loss:  6.42889045e-05, mean val. rec. loss:  6.88728627e-05\n",
      "Epoch: 6782 mean train loss:  6.42956384e-05, mean val. rec. loss:  6.73784443e-05\n",
      "Epoch: 6783 mean train loss:  6.41126028e-05, mean val. rec. loss:  6.82190788e-05\n",
      "Epoch: 6784 mean train loss:  6.40422604e-05, mean val. rec. loss:  6.80417549e-05\n",
      "Epoch: 6785 mean train loss:  6.41213597e-05, mean val. rec. loss:  6.68749849e-05\n",
      "Epoch: 6786 mean train loss:  6.38254808e-05, mean val. rec. loss:  6.72393910e-05\n",
      "Epoch: 6787 mean train loss:  6.33835819e-05, mean val. rec. loss:  6.84536585e-05\n",
      "Epoch: 6788 mean train loss:  6.37473262e-05, mean val. rec. loss:  6.65809369e-05\n",
      "Epoch: 6789 mean train loss:  6.40541771e-05, mean val. rec. loss:  6.92337933e-05\n",
      "Epoch: 6790 mean train loss:  6.39397959e-05, mean val. rec. loss:  6.66744644e-05\n",
      "Epoch: 6791 mean train loss:  6.39283104e-05, mean val. rec. loss:  6.82842259e-05\n",
      "Epoch: 6792 mean train loss:  6.39716263e-05, mean val. rec. loss:  6.76837091e-05\n",
      "Epoch: 6793 mean train loss:  6.43314130e-05, mean val. rec. loss:  6.85550363e-05\n",
      "Epoch: 6794 mean train loss:  6.56104359e-05, mean val. rec. loss:  6.84879266e-05\n",
      "Epoch: 6795 mean train loss:  6.43891681e-05, mean val. rec. loss:  6.81441732e-05\n",
      "Epoch: 6796 mean train loss:  6.46270565e-05, mean val. rec. loss:  6.86115154e-05\n",
      "Epoch: 6797 mean train loss:  6.42282457e-05, mean val. rec. loss:  6.82790650e-05\n",
      "Epoch: 6798 mean train loss:  6.44791695e-05, mean val. rec. loss:  6.74570116e-05\n",
      "Epoch: 6799 mean train loss:  6.39185798e-05, mean val. rec. loss:  6.66217061e-05\n",
      "Epoch: 6800 mean train loss:  6.37830144e-05, mean val. rec. loss:  6.80917647e-05\n",
      "Epoch: 6801 mean train loss:  6.39089175e-05, mean val. rec. loss:  6.69862348e-05\n",
      "Epoch: 6802 mean train loss:  6.33447788e-05, mean val. rec. loss:  6.80005859e-05\n",
      "Epoch: 6803 mean train loss:  6.33449971e-05, mean val. rec. loss:  6.73516676e-05\n",
      "Epoch: 6804 mean train loss:  6.34855049e-05, mean val. rec. loss:  6.59605443e-05\n",
      "Epoch: 6805 mean train loss:  6.37732566e-05, mean val. rec. loss:  6.82481996e-05\n",
      "Epoch: 6806 mean train loss:  6.38048604e-05, mean val. rec. loss:  6.63455667e-05\n",
      "Epoch: 6807 mean train loss:  6.34352638e-05, mean val. rec. loss:  6.62469010e-05\n",
      "Epoch: 6808 mean train loss:  6.34939647e-05, mean val. rec. loss:  6.63815248e-05\n",
      "Epoch: 6809 mean train loss:  6.34527645e-05, mean val. rec. loss:  6.74876225e-05\n",
      "Epoch: 6810 mean train loss:  6.38227372e-05, mean val. rec. loss:  6.87144878e-05\n",
      "Epoch: 6811 mean train loss:  6.48243996e-05, mean val. rec. loss:  6.64973586e-05\n",
      "Epoch: 6812 mean train loss:  6.36434286e-05, mean val. rec. loss:  6.71582115e-05\n",
      "Epoch: 6813 mean train loss:  6.36693817e-05, mean val. rec. loss:  6.64702276e-05\n",
      "Epoch: 6814 mean train loss:  6.38392105e-05, mean val. rec. loss:  6.78456093e-05\n",
      "Epoch: 6815 mean train loss:  6.35904330e-05, mean val. rec. loss:  6.77753967e-05\n",
      "Epoch: 6816 mean train loss:  6.54047990e-05, mean val. rec. loss:  6.83325138e-05\n",
      "Epoch: 6817 mean train loss:  6.34811400e-05, mean val. rec. loss:  6.70435271e-05\n",
      "Epoch: 6818 mean train loss:  6.35196608e-05, mean val. rec. loss:  6.71942150e-05\n",
      "Epoch: 6819 mean train loss:  6.35239447e-05, mean val. rec. loss:  6.61627685e-05\n",
      "Epoch: 6820 mean train loss:  6.36258091e-05, mean val. rec. loss:  6.92649631e-05\n",
      "Epoch: 6821 mean train loss:  6.41371964e-05, mean val. rec. loss:  6.72688844e-05\n",
      "Epoch: 6822 mean train loss:  6.34050134e-05, mean val. rec. loss:  6.65519432e-05\n",
      "Epoch: 6823 mean train loss:  6.34538123e-05, mean val. rec. loss:  6.77174184e-05\n",
      "Epoch: 6824 mean train loss:  6.31310605e-05, mean val. rec. loss:  6.68351061e-05\n",
      "Epoch: 6825 mean train loss:  6.35098225e-05, mean val. rec. loss:  6.76208880e-05\n",
      "Epoch: 6826 mean train loss:  6.37905358e-05, mean val. rec. loss:  6.61814858e-05\n",
      "Epoch: 6827 mean train loss:  6.32560169e-05, mean val. rec. loss:  6.64989078e-05\n",
      "Epoch: 6828 mean train loss:  6.32109714e-05, mean val. rec. loss:  6.72472141e-05\n",
      "Epoch: 6829 mean train loss:  6.28467959e-05, mean val. rec. loss:  6.53910382e-05\n",
      "Epoch: 6830 mean train loss:  6.29365428e-05, mean val. rec. loss:  6.62872068e-05\n",
      "Epoch: 6831 mean train loss:  6.31473253e-05, mean val. rec. loss:  6.77713806e-05\n",
      "Epoch: 6832 mean train loss:  6.32829164e-05, mean val. rec. loss:  6.57980035e-05\n",
      "Epoch: 6833 mean train loss:  6.30128004e-05, mean val. rec. loss:  6.72708879e-05\n",
      "Epoch: 6834 mean train loss:  6.30091158e-05, mean val. rec. loss:  6.68422569e-05\n",
      "Epoch: 6835 mean train loss:  6.30461711e-05, mean val. rec. loss:  6.76081766e-05\n",
      "Epoch: 6836 mean train loss:  6.33756927e-05, mean val. rec. loss:  6.60374488e-05\n",
      "Epoch: 6837 mean train loss:  6.26483295e-05, mean val. rec. loss:  6.60478251e-05\n",
      "Epoch: 6838 mean train loss:  6.29334890e-05, mean val. rec. loss:  6.68469498e-05\n",
      "Epoch: 6839 mean train loss:  6.29627404e-05, mean val. rec. loss:  6.66884933e-05\n",
      "Epoch: 6840 mean train loss:  6.25296217e-05, mean val. rec. loss:  6.57700911e-05\n",
      "Epoch: 6841 mean train loss:  6.31266276e-05, mean val. rec. loss:  6.67788588e-05\n",
      "Epoch: 6842 mean train loss:  6.30300884e-05, mean val. rec. loss:  6.54322618e-05\n",
      "Epoch: 6843 mean train loss:  6.24369400e-05, mean val. rec. loss:  6.60244148e-05\n",
      "Epoch: 6844 mean train loss:  6.25777238e-05, mean val. rec. loss:  6.74671107e-05\n",
      "Epoch: 6845 mean train loss:  6.30656930e-05, mean val. rec. loss:  6.71673839e-05\n",
      "Epoch: 6846 mean train loss:  6.29156933e-05, mean val. rec. loss:  6.63980569e-05\n",
      "Epoch: 6847 mean train loss:  6.24198268e-05, mean val. rec. loss:  6.67195585e-05\n",
      "Epoch: 6848 mean train loss:  6.34152836e-05, mean val. rec. loss:  6.91378080e-05\n",
      "Epoch: 6849 mean train loss:  6.40836397e-05, mean val. rec. loss:  6.59559331e-05\n",
      "Epoch: 6850 mean train loss:  6.26370299e-05, mean val. rec. loss:  6.52992644e-05\n",
      "Epoch: 6851 mean train loss:  6.23432909e-05, mean val. rec. loss:  6.81067840e-05\n",
      "Epoch: 6852 mean train loss:  6.34255759e-05, mean val. rec. loss:  6.58649133e-05\n",
      "Epoch: 6853 mean train loss:  6.31002327e-05, mean val. rec. loss:  6.70328555e-05\n",
      "Epoch: 6854 mean train loss:  6.33371179e-05, mean val. rec. loss:  6.74009005e-05\n",
      "Epoch: 6855 mean train loss:  6.35379668e-05, mean val. rec. loss:  6.65984503e-05\n",
      "Epoch: 6856 mean train loss:  6.26170014e-05, mean val. rec. loss:  6.77357541e-05\n",
      "Epoch: 6857 mean train loss:  6.31643484e-05, mean val. rec. loss:  6.57184958e-05\n",
      "Epoch: 6858 mean train loss:  6.32557619e-05, mean val. rec. loss:  6.67545854e-05\n",
      "Epoch: 6859 mean train loss:  6.25504044e-05, mean val. rec. loss:  6.81450818e-05\n",
      "Epoch: 6860 mean train loss:  6.34626895e-05, mean val. rec. loss:  6.69439528e-05\n",
      "Epoch: 6861 mean train loss:  6.24975061e-05, mean val. rec. loss:  6.56003814e-05\n",
      "Epoch: 6862 mean train loss:  6.23658138e-05, mean val. rec. loss:  6.67315612e-05\n",
      "Epoch: 6863 mean train loss:  6.23053779e-05, mean val. rec. loss:  6.50559166e-05\n",
      "Epoch: 6864 mean train loss:  6.28409407e-05, mean val. rec. loss:  6.73681134e-05\n",
      "Epoch: 6865 mean train loss:  6.23958959e-05, mean val. rec. loss:  6.58358743e-05\n",
      "Epoch: 6866 mean train loss:  6.34210415e-05, mean val. rec. loss:  6.85980680e-05\n",
      "Epoch: 6867 mean train loss:  6.30361142e-05, mean val. rec. loss:  6.65514753e-05\n",
      "Epoch: 6868 mean train loss:  6.21911533e-05, mean val. rec. loss:  6.69385556e-05\n",
      "Epoch: 6869 mean train loss:  6.23590626e-05, mean val. rec. loss:  6.65482951e-05\n",
      "Epoch: 6870 mean train loss:  6.26612426e-05, mean val. rec. loss:  6.65489402e-05\n",
      "Epoch: 6871 mean train loss:  6.21939711e-05, mean val. rec. loss:  6.50040624e-05\n",
      "Epoch: 6872 mean train loss:  6.18535375e-05, mean val. rec. loss:  6.46445900e-05\n",
      "Epoch: 6873 mean train loss:  6.20447236e-05, mean val. rec. loss:  6.55777389e-05\n",
      "Epoch: 6874 mean train loss:  6.20947537e-05, mean val. rec. loss:  6.44724907e-05\n",
      "Epoch: 6875 mean train loss:  6.19622111e-05, mean val. rec. loss:  6.77411876e-05\n",
      "Epoch: 6876 mean train loss:  6.26867465e-05, mean val. rec. loss:  6.45453610e-05\n",
      "Epoch: 6877 mean train loss:  6.22775417e-05, mean val. rec. loss:  6.55107564e-05\n",
      "Epoch: 6878 mean train loss:  6.18552651e-05, mean val. rec. loss:  6.56513815e-05\n",
      "Epoch: 6879 mean train loss:  6.22512597e-05, mean val. rec. loss:  6.56997830e-05\n",
      "Epoch: 6880 mean train loss:  6.24804651e-05, mean val. rec. loss:  6.46765276e-05\n",
      "Epoch: 6881 mean train loss:  6.21236614e-05, mean val. rec. loss:  6.64848198e-05\n",
      "Epoch: 6882 mean train loss:  6.17605485e-05, mean val. rec. loss:  6.44163251e-05\n",
      "Epoch: 6883 mean train loss:  6.15827293e-05, mean val. rec. loss:  6.42208337e-05\n",
      "Epoch: 6884 mean train loss:  6.20704805e-05, mean val. rec. loss:  6.43256279e-05\n",
      "Epoch: 6885 mean train loss:  6.20432231e-05, mean val. rec. loss:  6.58932982e-05\n",
      "Epoch: 6886 mean train loss:  6.17348524e-05, mean val. rec. loss:  6.48435523e-05\n",
      "Epoch: 6887 mean train loss:  6.21065385e-05, mean val. rec. loss:  6.53157692e-05\n",
      "Epoch: 6888 mean train loss:  6.16885137e-05, mean val. rec. loss:  6.54006422e-05\n",
      "Epoch: 6889 mean train loss:  6.16052556e-05, mean val. rec. loss:  6.39939181e-05\n",
      "Epoch: 6890 mean train loss:  6.18015054e-05, mean val. rec. loss:  6.59207064e-05\n",
      "Epoch: 6891 mean train loss:  6.17353368e-05, mean val. rec. loss:  6.66231099e-05\n",
      "Epoch: 6892 mean train loss:  6.16701397e-05, mean val. rec. loss:  6.62456744e-05\n",
      "Epoch: 6893 mean train loss:  6.20017525e-05, mean val. rec. loss:  6.58530833e-05\n",
      "Epoch: 6894 mean train loss:  6.21414054e-05, mean val. rec. loss:  6.52693121e-05\n",
      "Epoch: 6895 mean train loss:  6.13666144e-05, mean val. rec. loss:  6.45590628e-05\n",
      "Epoch: 6896 mean train loss:  6.14336205e-05, mean val. rec. loss:  6.48336167e-05\n",
      "Epoch: 6897 mean train loss:  6.14298235e-05, mean val. rec. loss:  6.48380325e-05\n",
      "Epoch: 6898 mean train loss:  6.11336388e-05, mean val. rec. loss:  6.45779891e-05\n",
      "Epoch: 6899 mean train loss:  6.14314633e-05, mean val. rec. loss:  6.47378449e-05\n",
      "Epoch: 6900 mean train loss:  6.18250645e-05, mean val. rec. loss:  6.41088206e-05\n",
      "Epoch: 6901 mean train loss:  6.18803906e-05, mean val. rec. loss:  6.40429647e-05\n",
      "Epoch: 6902 mean train loss:  6.16178014e-05, mean val. rec. loss:  6.70415145e-05\n",
      "Epoch: 6903 mean train loss:  6.24028555e-05, mean val. rec. loss:  6.51759618e-05\n",
      "Epoch: 6904 mean train loss:  6.18902502e-05, mean val. rec. loss:  6.53195808e-05\n",
      "Epoch: 6905 mean train loss:  6.14082175e-05, mean val. rec. loss:  6.52891198e-05\n",
      "Epoch: 6906 mean train loss:  6.10328643e-05, mean val. rec. loss:  6.49422589e-05\n",
      "Epoch: 6907 mean train loss:  6.22464297e-05, mean val. rec. loss:  6.52715564e-05\n",
      "Epoch: 6908 mean train loss:  6.20891394e-05, mean val. rec. loss:  6.65245396e-05\n",
      "Epoch: 6909 mean train loss:  6.18432998e-05, mean val. rec. loss:  6.42233233e-05\n",
      "Epoch: 6910 mean train loss:  6.14104588e-05, mean val. rec. loss:  6.39105670e-05\n",
      "Epoch: 6911 mean train loss:  6.09440731e-05, mean val. rec. loss:  6.47061845e-05\n",
      "Epoch: 6912 mean train loss:  6.06036816e-05, mean val. rec. loss:  6.43133390e-05\n",
      "Epoch: 6913 mean train loss:  6.11155479e-05, mean val. rec. loss:  6.35729512e-05\n",
      "Epoch: 6914 mean train loss:  6.12580789e-05, mean val. rec. loss:  6.52811513e-05\n",
      "Epoch: 6915 mean train loss:  6.10458056e-05, mean val. rec. loss:  6.40375222e-05\n",
      "Epoch: 6916 mean train loss:  6.11229826e-05, mean val. rec. loss:  6.56584414e-05\n",
      "Epoch: 6917 mean train loss:  6.16122115e-05, mean val. rec. loss:  6.57006780e-05\n",
      "Epoch: 6918 mean train loss:  6.10380179e-05, mean val. rec. loss:  6.48323083e-05\n",
      "Epoch: 6919 mean train loss:  6.13107612e-05, mean val. rec. loss:  6.57924246e-05\n",
      "Epoch: 6920 mean train loss:  6.18646844e-05, mean val. rec. loss:  6.40585837e-05\n",
      "Epoch: 6921 mean train loss:  6.04884887e-05, mean val. rec. loss:  6.55499264e-05\n",
      "Epoch: 6922 mean train loss:  6.16171570e-05, mean val. rec. loss:  6.60505873e-05\n",
      "Epoch: 6923 mean train loss:  6.09539205e-05, mean val. rec. loss:  6.36010635e-05\n",
      "Epoch: 6924 mean train loss:  6.16780054e-05, mean val. rec. loss:  6.48174707e-05\n",
      "Epoch: 6925 mean train loss:  6.13077796e-05, mean val. rec. loss:  6.43735252e-05\n",
      "Epoch: 6926 mean train loss:  6.12761584e-05, mean val. rec. loss:  6.38754175e-05\n",
      "Epoch: 6927 mean train loss:  6.09440660e-05, mean val. rec. loss:  6.48816093e-05\n",
      "Epoch: 6928 mean train loss:  6.11569827e-05, mean val. rec. loss:  6.50155199e-05\n",
      "Epoch: 6929 mean train loss:  6.06875779e-05, mean val. rec. loss:  6.41656540e-05\n",
      "Epoch: 6930 mean train loss:  6.05153736e-05, mean val. rec. loss:  6.33694004e-05\n",
      "Epoch: 6931 mean train loss:  6.08026284e-05, mean val. rec. loss:  6.42801294e-05\n",
      "Epoch: 6932 mean train loss:  6.08650722e-05, mean val. rec. loss:  6.53756964e-05\n",
      "Epoch: 6933 mean train loss:  6.09366274e-05, mean val. rec. loss:  6.37626003e-05\n",
      "Epoch: 6934 mean train loss:  6.04607062e-05, mean val. rec. loss:  6.36932099e-05\n",
      "Epoch: 6935 mean train loss:  6.05025455e-05, mean val. rec. loss:  6.37796412e-05\n",
      "Epoch: 6936 mean train loss:  6.05969337e-05, mean val. rec. loss:  6.41386320e-05\n",
      "Epoch: 6937 mean train loss:  6.02518472e-05, mean val. rec. loss:  6.46552480e-05\n",
      "Epoch: 6938 mean train loss:  6.06519028e-05, mean val. rec. loss:  6.45026474e-05\n",
      "Epoch: 6939 mean train loss:  6.09269409e-05, mean val. rec. loss:  6.42928817e-05\n",
      "Epoch: 6940 mean train loss:  6.13486432e-05, mean val. rec. loss:  6.43606593e-05\n",
      "Epoch: 6941 mean train loss:  6.09221643e-05, mean val. rec. loss:  6.42061143e-05\n",
      "Epoch: 6942 mean train loss:  6.06315240e-05, mean val. rec. loss:  6.32906469e-05\n",
      "Epoch: 6943 mean train loss:  6.02449953e-05, mean val. rec. loss:  6.30354736e-05\n",
      "Epoch: 6944 mean train loss:  6.04409517e-05, mean val. rec. loss:  6.59904057e-05\n",
      "Epoch: 6945 mean train loss:  6.12497960e-05, mean val. rec. loss:  6.48122235e-05\n",
      "Epoch: 6946 mean train loss:  6.09806912e-05, mean val. rec. loss:  6.28139915e-05\n",
      "Epoch: 6947 mean train loss:  6.11212311e-05, mean val. rec. loss:  6.45077265e-05\n",
      "Epoch: 6948 mean train loss:  6.14084048e-05, mean val. rec. loss:  6.28236863e-05\n",
      "Epoch: 6949 mean train loss:  6.07085162e-05, mean val. rec. loss:  6.64646760e-05\n",
      "Epoch: 6950 mean train loss:  6.06983689e-05, mean val. rec. loss:  6.34008701e-05\n",
      "Epoch: 6951 mean train loss:  6.08803642e-05, mean val. rec. loss:  6.44430200e-05\n",
      "Epoch: 6952 mean train loss:  6.13669382e-05, mean val. rec. loss:  6.46006498e-05\n",
      "Epoch: 6953 mean train loss:  6.00155531e-05, mean val. rec. loss:  6.31236267e-05\n",
      "Epoch: 6954 mean train loss:  6.06608177e-05, mean val. rec. loss:  6.35937629e-05\n",
      "Epoch: 6955 mean train loss:  6.09538395e-05, mean val. rec. loss:  6.44278508e-05\n",
      "Epoch: 6956 mean train loss:  6.06678339e-05, mean val. rec. loss:  6.33604643e-05\n",
      "Epoch: 6957 mean train loss:  5.98688459e-05, mean val. rec. loss:  6.35491775e-05\n",
      "Epoch: 6958 mean train loss:  6.01552492e-05, mean val. rec. loss:  6.43071968e-05\n",
      "Epoch: 6959 mean train loss:  6.01921422e-05, mean val. rec. loss:  6.38655364e-05\n",
      "Epoch: 6960 mean train loss:  5.98334653e-05, mean val. rec. loss:  6.32160775e-05\n",
      "Epoch: 6961 mean train loss:  6.01052062e-05, mean val. rec. loss:  6.34190649e-05\n",
      "Epoch: 6962 mean train loss:  6.00605147e-05, mean val. rec. loss:  6.24765438e-05\n",
      "Epoch: 6963 mean train loss:  5.97839011e-05, mean val. rec. loss:  6.34980729e-05\n",
      "Epoch: 6964 mean train loss:  5.97387421e-05, mean val. rec. loss:  6.25674318e-05\n",
      "Epoch: 6965 mean train loss:  5.98627863e-05, mean val. rec. loss:  6.33958318e-05\n",
      "Epoch: 6966 mean train loss:  5.98947660e-05, mean val. rec. loss:  6.38219596e-05\n",
      "Epoch: 6967 mean train loss:  5.99486150e-05, mean val. rec. loss:  6.33252149e-05\n",
      "Epoch: 6968 mean train loss:  5.99125265e-05, mean val. rec. loss:  6.24873971e-05\n",
      "Epoch: 6969 mean train loss:  5.96472448e-05, mean val. rec. loss:  6.20355875e-05\n",
      "Epoch: 6970 mean train loss:  5.96765343e-05, mean val. rec. loss:  6.45837133e-05\n",
      "Epoch: 6971 mean train loss:  5.97339988e-05, mean val. rec. loss:  6.27587936e-05\n",
      "Epoch: 6972 mean train loss:  6.02059212e-05, mean val. rec. loss:  6.29898433e-05\n",
      "Epoch: 6973 mean train loss:  5.99060544e-05, mean val. rec. loss:  6.62852442e-05\n",
      "Epoch: 6974 mean train loss:  6.13357119e-05, mean val. rec. loss:  6.43895258e-05\n",
      "Epoch: 6975 mean train loss:  6.14223601e-05, mean val. rec. loss:  6.34365374e-05\n",
      "Epoch: 6976 mean train loss:  6.04148531e-05, mean val. rec. loss:  6.34449284e-05\n",
      "Epoch: 6977 mean train loss:  6.00444206e-05, mean val. rec. loss:  6.42693533e-05\n",
      "Epoch: 6978 mean train loss:  5.98988594e-05, mean val. rec. loss:  6.28290562e-05\n",
      "Epoch: 6979 mean train loss:  5.96884979e-05, mean val. rec. loss:  6.20866149e-05\n",
      "Epoch: 6980 mean train loss:  5.98509887e-05, mean val. rec. loss:  6.41269064e-05\n",
      "Epoch: 6981 mean train loss:  6.00220970e-05, mean val. rec. loss:  6.24605159e-05\n",
      "Epoch: 6982 mean train loss:  6.02769036e-05, mean val. rec. loss:  6.32898564e-05\n",
      "Epoch: 6983 mean train loss:  5.96823932e-05, mean val. rec. loss:  6.28195976e-05\n",
      "Epoch: 6984 mean train loss:  5.95363842e-05, mean val. rec. loss:  6.24392272e-05\n",
      "Epoch: 6985 mean train loss:  5.94337380e-05, mean val. rec. loss:  6.43760420e-05\n",
      "Epoch: 6986 mean train loss:  5.99330940e-05, mean val. rec. loss:  6.25701576e-05\n",
      "Epoch: 6987 mean train loss:  5.98397059e-05, mean val. rec. loss:  6.31042097e-05\n",
      "Epoch: 6988 mean train loss:  5.99418450e-05, mean val. rec. loss:  6.51412621e-05\n",
      "Epoch: 6989 mean train loss:  6.05190358e-05, mean val. rec. loss:  6.26878086e-05\n",
      "Epoch: 6990 mean train loss:  5.96556930e-05, mean val. rec. loss:  6.15718116e-05\n",
      "Epoch: 6991 mean train loss:  5.99486940e-05, mean val. rec. loss:  6.24122961e-05\n",
      "Epoch: 6992 mean train loss:  5.98274120e-05, mean val. rec. loss:  6.40464083e-05\n",
      "Epoch: 6993 mean train loss:  5.99798171e-05, mean val. rec. loss:  6.43772868e-05\n",
      "Epoch: 6994 mean train loss:  6.01777303e-05, mean val. rec. loss:  6.21725510e-05\n",
      "Epoch: 6995 mean train loss:  5.99972649e-05, mean val. rec. loss:  6.30606148e-05\n",
      "Epoch: 6996 mean train loss:  5.99240867e-05, mean val. rec. loss:  6.31286967e-05\n",
      "Epoch: 6997 mean train loss:  5.92322616e-05, mean val. rec. loss:  6.19418101e-05\n",
      "Epoch: 6998 mean train loss:  5.97920661e-05, mean val. rec. loss:  6.22177633e-05\n",
      "Epoch: 6999 mean train loss:  5.91970476e-05, mean val. rec. loss:  6.32819333e-05\n",
      "Epoch: 7000 mean train loss:  5.91044140e-05, mean val. rec. loss:  6.24903273e-05\n",
      "Epoch: 7001 mean train loss:  5.89386872e-05, mean val. rec. loss:  6.23872594e-05\n",
      "Epoch: 7002 mean train loss:  5.93602198e-05, mean val. rec. loss:  6.16588153e-05\n",
      "Epoch: 7003 mean train loss:  5.96310854e-05, mean val. rec. loss:  6.24983412e-05\n",
      "Epoch: 7004 mean train loss:  5.96171534e-05, mean val. rec. loss:  6.24682300e-05\n",
      "Epoch: 7005 mean train loss:  5.97081057e-05, mean val. rec. loss:  6.24715101e-05\n",
      "Epoch: 7006 mean train loss:  5.93693738e-05, mean val. rec. loss:  6.18539024e-05\n",
      "Epoch: 7007 mean train loss:  5.94932329e-05, mean val. rec. loss:  6.40292084e-05\n",
      "Epoch: 7008 mean train loss:  5.95286084e-05, mean val. rec. loss:  6.22869992e-05\n",
      "Epoch: 7009 mean train loss:  5.91760283e-05, mean val. rec. loss:  6.33557850e-05\n",
      "Epoch: 7010 mean train loss:  5.95813987e-05, mean val. rec. loss:  6.17182609e-05\n",
      "Epoch: 7011 mean train loss:  5.95677569e-05, mean val. rec. loss:  6.29791081e-05\n",
      "Epoch: 7012 mean train loss:  5.89962870e-05, mean val. rec. loss:  6.19318972e-05\n",
      "Epoch: 7013 mean train loss:  5.85347911e-05, mean val. rec. loss:  6.18433262e-05\n",
      "Epoch: 7014 mean train loss:  5.89402237e-05, mean val. rec. loss:  6.16517917e-05\n",
      "Epoch: 7015 mean train loss:  5.89215728e-05, mean val. rec. loss:  6.18999052e-05\n",
      "Epoch: 7016 mean train loss:  5.90410263e-05, mean val. rec. loss:  6.34834897e-05\n",
      "Epoch: 7017 mean train loss:  5.91795984e-05, mean val. rec. loss:  6.22132838e-05\n",
      "Epoch: 7018 mean train loss:  5.87371326e-05, mean val. rec. loss:  6.28535704e-05\n",
      "Epoch: 7019 mean train loss:  5.90112689e-05, mean val. rec. loss:  6.28419493e-05\n",
      "Epoch: 7020 mean train loss:  5.86380627e-05, mean val. rec. loss:  6.18098440e-05\n",
      "Epoch: 7021 mean train loss:  5.88212231e-05, mean val. rec. loss:  6.29226791e-05\n",
      "Epoch: 7022 mean train loss:  5.91725239e-05, mean val. rec. loss:  6.32187897e-05\n",
      "Epoch: 7023 mean train loss:  5.93871963e-05, mean val. rec. loss:  6.32299610e-05\n",
      "Epoch: 7024 mean train loss:  5.89366824e-05, mean val. rec. loss:  6.10563995e-05\n",
      "Epoch: 7025 mean train loss:  5.89722270e-05, mean val. rec. loss:  6.32098444e-05\n",
      "Epoch: 7026 mean train loss:  5.87707290e-05, mean val. rec. loss:  6.17819588e-05\n",
      "Epoch: 7027 mean train loss:  5.86104070e-05, mean val. rec. loss:  6.20106780e-05\n",
      "Epoch: 7028 mean train loss:  5.88596325e-05, mean val. rec. loss:  6.40081196e-05\n",
      "Epoch: 7029 mean train loss:  5.91589205e-05, mean val. rec. loss:  6.15754596e-05\n",
      "Epoch: 7030 mean train loss:  5.90716548e-05, mean val. rec. loss:  6.21684804e-05\n",
      "Epoch: 7031 mean train loss:  5.82894828e-05, mean val. rec. loss:  6.13421383e-05\n",
      "Epoch: 7032 mean train loss:  5.82077115e-05, mean val. rec. loss:  6.11961388e-05\n",
      "Epoch: 7033 mean train loss:  5.89018431e-05, mean val. rec. loss:  6.25924911e-05\n",
      "Epoch: 7034 mean train loss:  5.87289329e-05, mean val. rec. loss:  6.07415216e-05\n",
      "Epoch: 7035 mean train loss:  5.84138653e-05, mean val. rec. loss:  6.32465567e-05\n",
      "Epoch: 7036 mean train loss:  5.90990228e-05, mean val. rec. loss:  6.12793717e-05\n",
      "Epoch: 7037 mean train loss:  5.83644139e-05, mean val. rec. loss:  6.22329552e-05\n",
      "Epoch: 7038 mean train loss:  5.90352311e-05, mean val. rec. loss:  6.33968358e-05\n",
      "Epoch: 7039 mean train loss:  5.88184704e-05, mean val. rec. loss:  6.15502730e-05\n",
      "Epoch: 7040 mean train loss:  5.88541455e-05, mean val. rec. loss:  6.42329046e-05\n",
      "Epoch: 7041 mean train loss:  5.92201686e-05, mean val. rec. loss:  6.20802274e-05\n",
      "Epoch: 7042 mean train loss:  5.83529342e-05, mean val. rec. loss:  6.22414143e-05\n",
      "Epoch: 7043 mean train loss:  5.82087008e-05, mean val. rec. loss:  6.02686823e-05\n",
      "Epoch: 7044 mean train loss:  5.79269641e-05, mean val. rec. loss:  6.20581437e-05\n",
      "Epoch: 7045 mean train loss:  5.81687921e-05, mean val. rec. loss:  6.04761901e-05\n",
      "Epoch: 7046 mean train loss:  5.78641487e-05, mean val. rec. loss:  6.25931908e-05\n",
      "Epoch: 7047 mean train loss:  5.81586619e-05, mean val. rec. loss:  6.07878742e-05\n",
      "Epoch: 7048 mean train loss:  5.80223095e-05, mean val. rec. loss:  6.25051331e-05\n",
      "Epoch: 7049 mean train loss:  5.89886983e-05, mean val. rec. loss:  6.11549970e-05\n",
      "Epoch: 7050 mean train loss:  5.79089292e-05, mean val. rec. loss:  6.28160358e-05\n",
      "Epoch: 7051 mean train loss:  5.84912014e-05, mean val. rec. loss:  6.17469593e-05\n",
      "Epoch: 7052 mean train loss:  5.81494626e-05, mean val. rec. loss:  6.14763306e-05\n",
      "Epoch: 7053 mean train loss:  5.86801348e-05, mean val. rec. loss:  6.14508714e-05\n",
      "Epoch: 7054 mean train loss:  5.78168051e-05, mean val. rec. loss:  6.05898841e-05\n",
      "Epoch: 7055 mean train loss:  5.79709788e-05, mean val. rec. loss:  6.27706191e-05\n",
      "Epoch: 7056 mean train loss:  5.83327930e-05, mean val. rec. loss:  6.07434070e-05\n",
      "Epoch: 7057 mean train loss:  5.86306448e-05, mean val. rec. loss:  6.08568148e-05\n",
      "Epoch: 7058 mean train loss:  5.83522573e-05, mean val. rec. loss:  6.19603503e-05\n",
      "Epoch: 7059 mean train loss:  5.80619152e-05, mean val. rec. loss:  6.11208743e-05\n",
      "Epoch: 7060 mean train loss:  5.82428914e-05, mean val. rec. loss:  6.08648469e-05\n",
      "Epoch: 7061 mean train loss:  5.84041296e-05, mean val. rec. loss:  6.22017991e-05\n",
      "Epoch: 7062 mean train loss:  5.83308346e-05, mean val. rec. loss:  6.04169761e-05\n",
      "Epoch: 7063 mean train loss:  5.79681967e-05, mean val. rec. loss:  6.09532998e-05\n",
      "Epoch: 7064 mean train loss:  5.80881713e-05, mean val. rec. loss:  6.06509062e-05\n",
      "Epoch: 7065 mean train loss:  5.82251135e-05, mean val. rec. loss:  6.20503569e-05\n",
      "Epoch: 7066 mean train loss:  5.79292628e-05, mean val. rec. loss:  6.21979920e-05\n",
      "Epoch: 7067 mean train loss:  5.85374333e-05, mean val. rec. loss:  6.18163633e-05\n",
      "Epoch: 7068 mean train loss:  5.79096883e-05, mean val. rec. loss:  6.02739795e-05\n",
      "Epoch: 7069 mean train loss:  5.85207888e-05, mean val. rec. loss:  6.25010262e-05\n",
      "Epoch: 7070 mean train loss:  5.79763904e-05, mean val. rec. loss:  6.02147837e-05\n",
      "Epoch: 7071 mean train loss:  5.78139707e-05, mean val. rec. loss:  6.14524524e-05\n",
      "Epoch: 7072 mean train loss:  5.76437468e-05, mean val. rec. loss:  6.11850946e-05\n",
      "Epoch: 7073 mean train loss:  5.80185558e-05, mean val. rec. loss:  6.17796601e-05\n",
      "Epoch: 7074 mean train loss:  5.82300269e-05, mean val. rec. loss:  6.14060998e-05\n",
      "Epoch: 7075 mean train loss:  5.77686560e-05, mean val. rec. loss:  6.07528610e-05\n",
      "Epoch: 7076 mean train loss:  5.83483364e-05, mean val. rec. loss:  6.15713346e-05\n",
      "Epoch: 7077 mean train loss:  5.74399657e-05, mean val. rec. loss:  5.98596318e-05\n",
      "Epoch: 7078 mean train loss:  5.75979940e-05, mean val. rec. loss:  6.14475095e-05\n",
      "Epoch: 7079 mean train loss:  5.81523959e-05, mean val. rec. loss:  6.16301124e-05\n",
      "Epoch: 7080 mean train loss:  5.77707870e-05, mean val. rec. loss:  6.05519770e-05\n",
      "Epoch: 7081 mean train loss:  5.79070916e-05, mean val. rec. loss:  6.10100878e-05\n",
      "Epoch: 7082 mean train loss:  5.76620031e-05, mean val. rec. loss:  6.13358962e-05\n",
      "Epoch: 7083 mean train loss:  5.72283775e-05, mean val. rec. loss:  6.04299919e-05\n",
      "Epoch: 7084 mean train loss:  5.78723609e-05, mean val. rec. loss:  6.03649356e-05\n",
      "Epoch: 7085 mean train loss:  5.75842411e-05, mean val. rec. loss:  6.16803220e-05\n",
      "Epoch: 7086 mean train loss:  5.75378174e-05, mean val. rec. loss:  6.06981583e-05\n",
      "Epoch: 7087 mean train loss:  5.72377831e-05, mean val. rec. loss:  6.02813483e-05\n",
      "Epoch: 7088 mean train loss:  5.82717348e-05, mean val. rec. loss:  6.06445368e-05\n",
      "Epoch: 7089 mean train loss:  5.78458080e-05, mean val. rec. loss:  6.03392720e-05\n",
      "Epoch: 7090 mean train loss:  5.77351023e-05, mean val. rec. loss:  6.09044440e-05\n",
      "Epoch: 7091 mean train loss:  5.73839163e-05, mean val. rec. loss:  6.06535048e-05\n",
      "Epoch: 7092 mean train loss:  5.72277592e-05, mean val. rec. loss:  6.05134203e-05\n",
      "Epoch: 7093 mean train loss:  5.72736342e-05, mean val. rec. loss:  6.04631152e-05\n",
      "Epoch: 7094 mean train loss:  5.72840442e-05, mean val. rec. loss:  6.02151108e-05\n",
      "Epoch: 7095 mean train loss:  5.73571712e-05, mean val. rec. loss:  6.02041121e-05\n",
      "Epoch: 7096 mean train loss:  5.72320326e-05, mean val. rec. loss:  6.02034988e-05\n",
      "Epoch: 7097 mean train loss:  5.75940677e-05, mean val. rec. loss:  6.15352719e-05\n",
      "Epoch: 7098 mean train loss:  5.80242407e-05, mean val. rec. loss:  5.98956626e-05\n",
      "Epoch: 7099 mean train loss:  5.69493145e-05, mean val. rec. loss:  6.05529492e-05\n",
      "Epoch: 7100 mean train loss:  5.68196059e-05, mean val. rec. loss:  6.02974216e-05\n",
      "Epoch: 7101 mean train loss:  5.72702512e-05, mean val. rec. loss:  5.98284257e-05\n",
      "Epoch: 7102 mean train loss:  5.70838309e-05, mean val. rec. loss:  6.01808336e-05\n",
      "Epoch: 7103 mean train loss:  5.72423911e-05, mean val. rec. loss:  6.10483901e-05\n",
      "Epoch: 7104 mean train loss:  5.74926440e-05, mean val. rec. loss:  6.01911872e-05\n",
      "Epoch: 7105 mean train loss:  5.70614874e-05, mean val. rec. loss:  6.02721623e-05\n",
      "Epoch: 7106 mean train loss:  5.69137497e-05, mean val. rec. loss:  5.98923235e-05\n",
      "Epoch: 7107 mean train loss:  5.74341645e-05, mean val. rec. loss:  5.95877356e-05\n",
      "Epoch: 7108 mean train loss:  5.71213715e-05, mean val. rec. loss:  6.09573704e-05\n",
      "Epoch: 7109 mean train loss:  5.69549328e-05, mean val. rec. loss:  6.15337364e-05\n",
      "Epoch: 7110 mean train loss:  5.74318055e-05, mean val. rec. loss:  5.95933917e-05\n",
      "Epoch: 7111 mean train loss:  5.67044264e-05, mean val. rec. loss:  6.17521929e-05\n",
      "Epoch: 7112 mean train loss:  5.75673371e-05, mean val. rec. loss:  5.99210128e-05\n",
      "Epoch: 7113 mean train loss:  5.65321991e-05, mean val. rec. loss:  5.98695402e-05\n",
      "Epoch: 7114 mean train loss:  5.68983379e-05, mean val. rec. loss:  6.07206600e-05\n",
      "Epoch: 7115 mean train loss:  5.67919102e-05, mean val. rec. loss:  6.03148305e-05\n",
      "Epoch: 7116 mean train loss:  5.65371559e-05, mean val. rec. loss:  5.91904016e-05\n",
      "Epoch: 7117 mean train loss:  5.65774149e-05, mean val. rec. loss:  5.96770699e-05\n",
      "Epoch: 7118 mean train loss:  5.67362139e-05, mean val. rec. loss:  5.92013321e-05\n",
      "Epoch: 7119 mean train loss:  5.73803229e-05, mean val. rec. loss:  6.08093673e-05\n",
      "Epoch: 7120 mean train loss:  5.70821678e-05, mean val. rec. loss:  6.14838629e-05\n",
      "Epoch: 7121 mean train loss:  5.69918173e-05, mean val. rec. loss:  5.96252883e-05\n",
      "Epoch: 7122 mean train loss:  5.69967255e-05, mean val. rec. loss:  5.96085427e-05\n",
      "Epoch: 7123 mean train loss:  5.65616085e-05, mean val. rec. loss:  6.03952059e-05\n",
      "Epoch: 7124 mean train loss:  5.69018821e-05, mean val. rec. loss:  6.01984969e-05\n",
      "Epoch: 7125 mean train loss:  5.73228829e-05, mean val. rec. loss:  6.03250251e-05\n",
      "Epoch: 7126 mean train loss:  5.78058811e-05, mean val. rec. loss:  6.02303936e-05\n",
      "Epoch: 7127 mean train loss:  5.70083594e-05, mean val. rec. loss:  6.47906986e-05\n",
      "Epoch: 7128 mean train loss:  5.81494842e-05, mean val. rec. loss:  6.19097181e-05\n",
      "Epoch: 7129 mean train loss:  5.83342082e-05, mean val. rec. loss:  5.95188086e-05\n",
      "Epoch: 7130 mean train loss:  5.67703334e-05, mean val. rec. loss:  6.09439820e-05\n",
      "Epoch: 7131 mean train loss:  5.68331783e-05, mean val. rec. loss:  5.87387147e-05\n",
      "Epoch: 7132 mean train loss:  5.64279623e-05, mean val. rec. loss:  5.94618480e-05\n",
      "Epoch: 7133 mean train loss:  5.66513402e-05, mean val. rec. loss:  6.05671689e-05\n",
      "Epoch: 7134 mean train loss:  5.69662602e-05, mean val. rec. loss:  5.97408768e-05\n",
      "Epoch: 7135 mean train loss:  5.66838594e-05, mean val. rec. loss:  5.89848473e-05\n",
      "Epoch: 7136 mean train loss:  5.64258509e-05, mean val. rec. loss:  6.05592368e-05\n",
      "Epoch: 7137 mean train loss:  5.71609430e-05, mean val. rec. loss:  5.97803785e-05\n",
      "Epoch: 7138 mean train loss:  5.67056173e-05, mean val. rec. loss:  5.96082837e-05\n",
      "Epoch: 7139 mean train loss:  5.67029242e-05, mean val. rec. loss:  6.05278717e-05\n",
      "Epoch: 7140 mean train loss:  5.64639374e-05, mean val. rec. loss:  5.97503218e-05\n",
      "Epoch: 7141 mean train loss:  5.62214626e-05, mean val. rec. loss:  5.93972779e-05\n",
      "Epoch: 7142 mean train loss:  5.61817119e-05, mean val. rec. loss:  5.93716415e-05\n",
      "Epoch: 7143 mean train loss:  5.65590616e-05, mean val. rec. loss:  5.87102117e-05\n",
      "Epoch: 7144 mean train loss:  5.63292646e-05, mean val. rec. loss:  6.00046228e-05\n",
      "Epoch: 7145 mean train loss:  5.62857076e-05, mean val. rec. loss:  5.86572398e-05\n",
      "Epoch: 7146 mean train loss:  5.65783888e-05, mean val. rec. loss:  6.13419339e-05\n",
      "Epoch: 7147 mean train loss:  5.65499079e-05, mean val. rec. loss:  5.90984233e-05\n",
      "Epoch: 7148 mean train loss:  5.72249627e-05, mean val. rec. loss:  6.07533380e-05\n",
      "Epoch: 7149 mean train loss:  5.60585394e-05, mean val. rec. loss:  5.84645242e-05\n",
      "Epoch: 7150 mean train loss:  5.61680156e-05, mean val. rec. loss:  6.09598963e-05\n",
      "Epoch: 7151 mean train loss:  5.64420592e-05, mean val. rec. loss:  5.79719818e-05\n",
      "Epoch: 7152 mean train loss:  5.62654155e-05, mean val. rec. loss:  6.02900618e-05\n",
      "Epoch: 7153 mean train loss:  5.63403503e-05, mean val. rec. loss:  5.82648032e-05\n",
      "Epoch: 7154 mean train loss:  5.62011133e-05, mean val. rec. loss:  5.98812976e-05\n",
      "Epoch: 7155 mean train loss:  5.61958998e-05, mean val. rec. loss:  5.93440562e-05\n",
      "Epoch: 7156 mean train loss:  5.59811333e-05, mean val. rec. loss:  5.91189578e-05\n",
      "Epoch: 7157 mean train loss:  5.64346840e-05, mean val. rec. loss:  5.84716386e-05\n",
      "Epoch: 7158 mean train loss:  5.58805553e-05, mean val. rec. loss:  6.26672695e-05\n",
      "Epoch: 7159 mean train loss:  5.63669439e-05, mean val. rec. loss:  5.85732482e-05\n",
      "Epoch: 7160 mean train loss:  5.66840030e-05, mean val. rec. loss:  5.93979366e-05\n",
      "Epoch: 7161 mean train loss:  5.61918885e-05, mean val. rec. loss:  5.89265147e-05\n",
      "Epoch: 7162 mean train loss:  5.61615412e-05, mean val. rec. loss:  5.99771647e-05\n",
      "Epoch: 7163 mean train loss:  5.58993449e-05, mean val. rec. loss:  5.99512331e-05\n",
      "Epoch: 7164 mean train loss:  5.60507019e-05, mean val. rec. loss:  5.84014260e-05\n",
      "Epoch: 7165 mean train loss:  5.61980848e-05, mean val. rec. loss:  5.95748743e-05\n",
      "Epoch: 7166 mean train loss:  5.58455070e-05, mean val. rec. loss:  6.06075247e-05\n",
      "Epoch: 7167 mean train loss:  5.61980450e-05, mean val. rec. loss:  5.86823538e-05\n",
      "Epoch: 7168 mean train loss:  5.60657074e-05, mean val. rec. loss:  5.94191798e-05\n",
      "Epoch: 7169 mean train loss:  5.62869639e-05, mean val. rec. loss:  5.88945318e-05\n",
      "Epoch: 7170 mean train loss:  5.59612210e-05, mean val. rec. loss:  6.02402247e-05\n",
      "Epoch: 7171 mean train loss:  5.64810233e-05, mean val. rec. loss:  5.91671594e-05\n",
      "Epoch: 7172 mean train loss:  5.57856957e-05, mean val. rec. loss:  5.74658375e-05\n",
      "Epoch: 7173 mean train loss:  5.63326479e-05, mean val. rec. loss:  6.11194614e-05\n",
      "Epoch: 7174 mean train loss:  5.63694587e-05, mean val. rec. loss:  5.88856274e-05\n",
      "Epoch: 7175 mean train loss:  5.60348913e-05, mean val. rec. loss:  5.89365639e-05\n",
      "Epoch: 7176 mean train loss:  5.58480511e-05, mean val. rec. loss:  5.79333978e-05\n",
      "Epoch: 7177 mean train loss:  5.52132740e-05, mean val. rec. loss:  5.86688973e-05\n",
      "Epoch: 7178 mean train loss:  5.54358917e-05, mean val. rec. loss:  5.85458310e-05\n",
      "Epoch: 7179 mean train loss:  5.54653950e-05, mean val. rec. loss:  5.93564996e-05\n",
      "Epoch: 7180 mean train loss:  5.61007116e-05, mean val. rec. loss:  5.90534245e-05\n",
      "Epoch: 7181 mean train loss:  5.56976721e-05, mean val. rec. loss:  5.81490467e-05\n",
      "Epoch: 7182 mean train loss:  5.55786644e-05, mean val. rec. loss:  5.82716950e-05\n",
      "Epoch: 7183 mean train loss:  5.53526580e-05, mean val. rec. loss:  5.85448815e-05\n",
      "Epoch: 7184 mean train loss:  5.51952039e-05, mean val. rec. loss:  5.83236810e-05\n",
      "Epoch: 7185 mean train loss:  5.51581702e-05, mean val. rec. loss:  5.87431487e-05\n",
      "Epoch: 7186 mean train loss:  5.52201137e-05, mean val. rec. loss:  5.80276204e-05\n",
      "Epoch: 7187 mean train loss:  5.56185955e-05, mean val. rec. loss:  5.85086099e-05\n",
      "Epoch: 7188 mean train loss:  5.55614431e-05, mean val. rec. loss:  5.75414700e-05\n",
      "Epoch: 7189 mean train loss:  5.52473731e-05, mean val. rec. loss:  5.91676228e-05\n",
      "Epoch: 7190 mean train loss:  5.55450836e-05, mean val. rec. loss:  5.97528114e-05\n",
      "Epoch: 7191 mean train loss:  5.59542466e-05, mean val. rec. loss:  5.88023899e-05\n",
      "Epoch: 7192 mean train loss:  5.56649473e-05, mean val. rec. loss:  5.76843258e-05\n",
      "Epoch: 7193 mean train loss:  5.60078201e-05, mean val. rec. loss:  6.12232289e-05\n",
      "Epoch: 7194 mean train loss:  5.58160533e-05, mean val. rec. loss:  5.90177571e-05\n",
      "Epoch: 7195 mean train loss:  5.53286637e-05, mean val. rec. loss:  5.91305789e-05\n",
      "Epoch: 7196 mean train loss:  5.55793862e-05, mean val. rec. loss:  5.98548344e-05\n",
      "Epoch: 7197 mean train loss:  5.57156182e-05, mean val. rec. loss:  5.86055219e-05\n",
      "Epoch: 7198 mean train loss:  5.57310701e-05, mean val. rec. loss:  5.79674569e-05\n",
      "Epoch: 7199 mean train loss:  5.59089862e-05, mean val. rec. loss:  5.95733887e-05\n",
      "Epoch: 7200 mean train loss:  5.54316905e-05, mean val. rec. loss:  5.93078618e-05\n",
      "Epoch: 7201 mean train loss:  5.54108375e-05, mean val. rec. loss:  5.83908452e-05\n",
      "Epoch: 7202 mean train loss:  5.54408499e-05, mean val. rec. loss:  5.83791742e-05\n",
      "Epoch: 7203 mean train loss:  5.52475272e-05, mean val. rec. loss:  5.92447863e-05\n",
      "Epoch: 7204 mean train loss:  5.54655863e-05, mean val. rec. loss:  5.74518858e-05\n",
      "Epoch: 7205 mean train loss:  5.54007519e-05, mean val. rec. loss:  5.80572228e-05\n",
      "Epoch: 7206 mean train loss:  5.55596476e-05, mean val. rec. loss:  5.75075335e-05\n",
      "Epoch: 7207 mean train loss:  5.54034938e-05, mean val. rec. loss:  5.86024872e-05\n",
      "Epoch: 7208 mean train loss:  5.46619530e-05, mean val. rec. loss:  5.74513270e-05\n",
      "Epoch: 7209 mean train loss:  5.51361448e-05, mean val. rec. loss:  5.84339587e-05\n",
      "Epoch: 7210 mean train loss:  5.49709133e-05, mean val. rec. loss:  5.76161212e-05\n",
      "Epoch: 7211 mean train loss:  5.53050176e-05, mean val. rec. loss:  5.81947314e-05\n",
      "Epoch: 7212 mean train loss:  5.49632691e-05, mean val. rec. loss:  5.75024635e-05\n",
      "Epoch: 7213 mean train loss:  5.52593469e-05, mean val. rec. loss:  5.97505081e-05\n",
      "Epoch: 7214 mean train loss:  5.52197893e-05, mean val. rec. loss:  5.87846857e-05\n",
      "Epoch: 7215 mean train loss:  5.57520211e-05, mean val. rec. loss:  5.76990997e-05\n",
      "Epoch: 7216 mean train loss:  5.47523476e-05, mean val. rec. loss:  5.79992355e-05\n",
      "Epoch: 7217 mean train loss:  5.49798023e-05, mean val. rec. loss:  5.70066728e-05\n",
      "Epoch: 7218 mean train loss:  5.49225448e-05, mean val. rec. loss:  5.70870118e-05\n",
      "Epoch: 7219 mean train loss:  5.48062912e-05, mean val. rec. loss:  6.10393540e-05\n",
      "Epoch: 7220 mean train loss:  5.49288261e-05, mean val. rec. loss:  5.75470579e-05\n",
      "Epoch: 7221 mean train loss:  5.53082670e-05, mean val. rec. loss:  5.78932464e-05\n",
      "Epoch: 7222 mean train loss:  5.54547601e-05, mean val. rec. loss:  5.79040861e-05\n",
      "Epoch: 7223 mean train loss:  5.49478983e-05, mean val. rec. loss:  5.72124542e-05\n",
      "Epoch: 7224 mean train loss:  5.51843075e-05, mean val. rec. loss:  5.75267505e-05\n",
      "Epoch: 7225 mean train loss:  5.45733713e-05, mean val. rec. loss:  5.86733086e-05\n",
      "Epoch: 7226 mean train loss:  5.44476888e-05, mean val. rec. loss:  5.65266736e-05\n",
      "Epoch: 7227 mean train loss:  5.44751947e-05, mean val. rec. loss:  5.77014121e-05\n",
      "Epoch: 7228 mean train loss:  5.43101305e-05, mean val. rec. loss:  5.73748859e-05\n",
      "Epoch: 7229 mean train loss:  5.43313056e-05, mean val. rec. loss:  5.79812450e-05\n",
      "Epoch: 7230 mean train loss:  5.45720278e-05, mean val. rec. loss:  5.75723354e-05\n",
      "Epoch: 7231 mean train loss:  5.46032544e-05, mean val. rec. loss:  5.76094111e-05\n",
      "Epoch: 7232 mean train loss:  5.46320429e-05, mean val. rec. loss:  5.64514137e-05\n",
      "Epoch: 7233 mean train loss:  5.40938323e-05, mean val. rec. loss:  5.71710943e-05\n",
      "Epoch: 7234 mean train loss:  5.49986258e-05, mean val. rec. loss:  5.81570606e-05\n",
      "Epoch: 7235 mean train loss:  5.47882686e-05, mean val. rec. loss:  5.72737034e-05\n",
      "Epoch: 7236 mean train loss:  5.47655601e-05, mean val. rec. loss:  5.76742720e-05\n",
      "Epoch: 7237 mean train loss:  5.47207525e-05, mean val. rec. loss:  5.83029102e-05\n",
      "Epoch: 7238 mean train loss:  5.45750688e-05, mean val. rec. loss:  5.84267807e-05\n",
      "Epoch: 7239 mean train loss:  5.52397900e-05, mean val. rec. loss:  5.67461432e-05\n",
      "Epoch: 7240 mean train loss:  5.51399634e-05, mean val. rec. loss:  5.87611164e-05\n",
      "Epoch: 7241 mean train loss:  5.51015657e-05, mean val. rec. loss:  5.87185981e-05\n",
      "Epoch: 7242 mean train loss:  5.48871241e-05, mean val. rec. loss:  5.80395004e-05\n",
      "Epoch: 7243 mean train loss:  5.45335177e-05, mean val. rec. loss:  5.72637905e-05\n",
      "Epoch: 7244 mean train loss:  5.43919999e-05, mean val. rec. loss:  5.78578289e-05\n",
      "Epoch: 7245 mean train loss:  5.44141828e-05, mean val. rec. loss:  5.71917697e-05\n",
      "Epoch: 7246 mean train loss:  5.41140732e-05, mean val. rec. loss:  5.62925028e-05\n",
      "Epoch: 7247 mean train loss:  5.43176823e-05, mean val. rec. loss:  5.95978984e-05\n",
      "Epoch: 7248 mean train loss:  5.46551221e-05, mean val. rec. loss:  5.64717938e-05\n",
      "Epoch: 7249 mean train loss:  5.44798587e-05, mean val. rec. loss:  5.76367102e-05\n",
      "Epoch: 7250 mean train loss:  5.41303094e-05, mean val. rec. loss:  5.66616882e-05\n",
      "Epoch: 7251 mean train loss:  5.42065371e-05, mean val. rec. loss:  5.64020218e-05\n",
      "Epoch: 7252 mean train loss:  5.40472494e-05, mean val. rec. loss:  5.79807271e-05\n",
      "Epoch: 7253 mean train loss:  5.44933373e-05, mean val. rec. loss:  5.75173464e-05\n",
      "Epoch: 7254 mean train loss:  5.45349437e-05, mean val. rec. loss:  5.72926842e-05\n",
      "Epoch: 7255 mean train loss:  5.41023057e-05, mean val. rec. loss:  5.71814843e-05\n",
      "Epoch: 7256 mean train loss:  5.39619512e-05, mean val. rec. loss:  5.79956101e-05\n",
      "Epoch: 7257 mean train loss:  5.42545434e-05, mean val. rec. loss:  5.67027708e-05\n",
      "Epoch: 7258 mean train loss:  5.44174384e-05, mean val. rec. loss:  6.00774568e-05\n",
      "Epoch: 7259 mean train loss:  5.50694774e-05, mean val. rec. loss:  5.67159456e-05\n",
      "Epoch: 7260 mean train loss:  5.44248225e-05, mean val. rec. loss:  5.71235197e-05\n",
      "Epoch: 7261 mean train loss:  5.42152019e-05, mean val. rec. loss:  5.57596182e-05\n",
      "Epoch: 7262 mean train loss:  5.45570422e-05, mean val. rec. loss:  5.78102996e-05\n",
      "Epoch: 7263 mean train loss:  5.43307129e-05, mean val. rec. loss:  5.83341981e-05\n",
      "Epoch: 7264 mean train loss:  5.38317329e-05, mean val. rec. loss:  5.63344941e-05\n",
      "Epoch: 7265 mean train loss:  5.38124529e-05, mean val. rec. loss:  5.78142884e-05\n",
      "Epoch: 7266 mean train loss:  5.40381755e-05, mean val. rec. loss:  5.67992241e-05\n",
      "Epoch: 7267 mean train loss:  5.37378541e-05, mean val. rec. loss:  5.69928074e-05\n",
      "Epoch: 7268 mean train loss:  5.40641175e-05, mean val. rec. loss:  5.63988053e-05\n",
      "Epoch: 7269 mean train loss:  5.39347839e-05, mean val. rec. loss:  5.73960746e-05\n",
      "Epoch: 7270 mean train loss:  5.40307104e-05, mean val. rec. loss:  5.74993106e-05\n",
      "Epoch: 7271 mean train loss:  5.36255490e-05, mean val. rec. loss:  5.70035426e-05\n",
      "Epoch: 7272 mean train loss:  5.37970130e-05, mean val. rec. loss:  5.84615531e-05\n",
      "Epoch: 7273 mean train loss:  5.44523136e-05, mean val. rec. loss:  5.66806599e-05\n",
      "Epoch: 7274 mean train loss:  5.43568593e-05, mean val. rec. loss:  5.65870779e-05\n",
      "Epoch: 7275 mean train loss:  5.37462568e-05, mean val. rec. loss:  5.76058766e-05\n",
      "Epoch: 7276 mean train loss:  5.37651812e-05, mean val. rec. loss:  5.83047093e-05\n",
      "Epoch: 7277 mean train loss:  5.43936511e-05, mean val. rec. loss:  5.56921904e-05\n",
      "Epoch: 7278 mean train loss:  5.39647781e-05, mean val. rec. loss:  5.70164948e-05\n",
      "Epoch: 7279 mean train loss:  5.41055932e-05, mean val. rec. loss:  5.69217906e-05\n",
      "Epoch: 7280 mean train loss:  5.45418687e-05, mean val. rec. loss:  5.82158566e-05\n",
      "Epoch: 7281 mean train loss:  5.45082492e-05, mean val. rec. loss:  5.62075889e-05\n",
      "Epoch: 7282 mean train loss:  5.47575150e-05, mean val. rec. loss:  5.74617397e-05\n",
      "Epoch: 7283 mean train loss:  5.42643763e-05, mean val. rec. loss:  5.69379093e-05\n",
      "Epoch: 7284 mean train loss:  5.40197194e-05, mean val. rec. loss:  5.75059707e-05\n",
      "Epoch: 7285 mean train loss:  5.39842438e-05, mean val. rec. loss:  5.67674092e-05\n",
      "Epoch: 7286 mean train loss:  5.40953302e-05, mean val. rec. loss:  5.72243524e-05\n",
      "Epoch: 7287 mean train loss:  5.38135764e-05, mean val. rec. loss:  5.70858443e-05\n",
      "Epoch: 7288 mean train loss:  5.41736662e-05, mean val. rec. loss:  5.73667811e-05\n",
      "Epoch: 7289 mean train loss:  5.37653989e-05, mean val. rec. loss:  5.58238295e-05\n",
      "Epoch: 7290 mean train loss:  5.35742541e-05, mean val. rec. loss:  5.66766393e-05\n",
      "Epoch: 7291 mean train loss:  5.29443573e-05, mean val. rec. loss:  5.60911236e-05\n",
      "Epoch: 7292 mean train loss:  5.33889553e-05, mean val. rec. loss:  5.62143580e-05\n",
      "Epoch: 7293 mean train loss:  5.31886558e-05, mean val. rec. loss:  5.64145651e-05\n",
      "Epoch: 7294 mean train loss:  5.32625703e-05, mean val. rec. loss:  5.67030071e-05\n",
      "Epoch: 7295 mean train loss:  5.32039010e-05, mean val. rec. loss:  5.56744181e-05\n",
      "Epoch: 7296 mean train loss:  5.32643752e-05, mean val. rec. loss:  5.66973601e-05\n",
      "Epoch: 7297 mean train loss:  5.31414259e-05, mean val. rec. loss:  5.70732464e-05\n",
      "Epoch: 7298 mean train loss:  5.33307885e-05, mean val. rec. loss:  5.61928149e-05\n",
      "Epoch: 7299 mean train loss:  5.32090824e-05, mean val. rec. loss:  5.57853636e-05\n",
      "Epoch: 7300 mean train loss:  5.29407065e-05, mean val. rec. loss:  5.57988700e-05\n",
      "Epoch: 7301 mean train loss:  5.33628296e-05, mean val. rec. loss:  5.54560706e-05\n",
      "Epoch: 7302 mean train loss:  5.29009010e-05, mean val. rec. loss:  5.70864939e-05\n",
      "Epoch: 7303 mean train loss:  5.34866367e-05, mean val. rec. loss:  5.50485511e-05\n",
      "Epoch: 7304 mean train loss:  5.29776933e-05, mean val. rec. loss:  5.59872607e-05\n",
      "Epoch: 7305 mean train loss:  5.34006554e-05, mean val. rec. loss:  5.85975125e-05\n",
      "Epoch: 7306 mean train loss:  5.36364190e-05, mean val. rec. loss:  5.51724398e-05\n",
      "Epoch: 7307 mean train loss:  5.33125857e-05, mean val. rec. loss:  5.78664698e-05\n",
      "Epoch: 7308 mean train loss:  5.36928956e-05, mean val. rec. loss:  5.62688018e-05\n",
      "Epoch: 7309 mean train loss:  5.35603650e-05, mean val. rec. loss:  5.72429970e-05\n",
      "Epoch: 7310 mean train loss:  5.41221336e-05, mean val. rec. loss:  5.55208589e-05\n",
      "Epoch: 7311 mean train loss:  5.36827983e-05, mean val. rec. loss:  5.68402250e-05\n",
      "Epoch: 7312 mean train loss:  5.28966725e-05, mean val. rec. loss:  5.55999032e-05\n",
      "Epoch: 7313 mean train loss:  5.27437767e-05, mean val. rec. loss:  5.49931806e-05\n",
      "Epoch: 7314 mean train loss:  5.32132330e-05, mean val. rec. loss:  5.64879306e-05\n",
      "Epoch: 7315 mean train loss:  5.28135779e-05, mean val. rec. loss:  5.55132993e-05\n",
      "Epoch: 7316 mean train loss:  5.32695333e-05, mean val. rec. loss:  5.66075760e-05\n",
      "Epoch: 7317 mean train loss:  5.37336225e-05, mean val. rec. loss:  5.82317981e-05\n",
      "Epoch: 7318 mean train loss:  5.45883373e-05, mean val. rec. loss:  5.66749856e-05\n",
      "Epoch: 7319 mean train loss:  5.36516758e-05, mean val. rec. loss:  5.60515038e-05\n",
      "Epoch: 7320 mean train loss:  5.27845195e-05, mean val. rec. loss:  5.49160762e-05\n",
      "Epoch: 7321 mean train loss:  5.29762679e-05, mean val. rec. loss:  5.68911751e-05\n",
      "Epoch: 7322 mean train loss:  5.29494006e-05, mean val. rec. loss:  5.57167138e-05\n",
      "Epoch: 7323 mean train loss:  5.30036743e-05, mean val. rec. loss:  5.50834326e-05\n",
      "Epoch: 7324 mean train loss:  5.35821743e-05, mean val. rec. loss:  5.59530425e-05\n",
      "Epoch: 7325 mean train loss:  5.37362468e-05, mean val. rec. loss:  5.67989288e-05\n",
      "Epoch: 7326 mean train loss:  5.35967170e-05, mean val. rec. loss:  5.84930727e-05\n",
      "Epoch: 7327 mean train loss:  5.41590396e-05, mean val. rec. loss:  5.77823509e-05\n",
      "Epoch: 7328 mean train loss:  5.35121679e-05, mean val. rec. loss:  5.69304179e-05\n",
      "Epoch: 7329 mean train loss:  5.31592216e-05, mean val. rec. loss:  5.52806004e-05\n",
      "Epoch: 7330 mean train loss:  5.31630089e-05, mean val. rec. loss:  5.55385449e-05\n",
      "Epoch: 7331 mean train loss:  5.27031903e-05, mean val. rec. loss:  5.52321625e-05\n",
      "Epoch: 7332 mean train loss:  5.23618330e-05, mean val. rec. loss:  5.53242544e-05\n",
      "Epoch: 7333 mean train loss:  5.32908523e-05, mean val. rec. loss:  5.49330581e-05\n",
      "Epoch: 7334 mean train loss:  5.29679952e-05, mean val. rec. loss:  5.69422343e-05\n",
      "Epoch: 7335 mean train loss:  5.26914470e-05, mean val. rec. loss:  5.42708650e-05\n",
      "Epoch: 7336 mean train loss:  5.29585862e-05, mean val. rec. loss:  5.58372996e-05\n",
      "Epoch: 7337 mean train loss:  5.26144016e-05, mean val. rec. loss:  5.50241278e-05\n",
      "Epoch: 7338 mean train loss:  5.22644043e-05, mean val. rec. loss:  5.49961245e-05\n",
      "Epoch: 7339 mean train loss:  5.26059978e-05, mean val. rec. loss:  5.53589814e-05\n",
      "Epoch: 7340 mean train loss:  5.24248539e-05, mean val. rec. loss:  5.61809576e-05\n",
      "Epoch: 7341 mean train loss:  5.24318633e-05, mean val. rec. loss:  5.57356037e-05\n",
      "Epoch: 7342 mean train loss:  5.30944623e-05, mean val. rec. loss:  5.48096237e-05\n",
      "Epoch: 7343 mean train loss:  5.28016148e-05, mean val. rec. loss:  5.54839195e-05\n",
      "Epoch: 7344 mean train loss:  5.23677549e-05, mean val. rec. loss:  5.46357708e-05\n",
      "Epoch: 7345 mean train loss:  5.25028124e-05, mean val. rec. loss:  5.57846049e-05\n",
      "Epoch: 7346 mean train loss:  5.25562159e-05, mean val. rec. loss:  5.60030614e-05\n",
      "Epoch: 7347 mean train loss:  5.24433851e-05, mean val. rec. loss:  5.55201547e-05\n",
      "Epoch: 7348 mean train loss:  5.32270058e-05, mean val. rec. loss:  5.58275639e-05\n",
      "Epoch: 7349 mean train loss:  5.33728834e-05, mean val. rec. loss:  5.58590198e-05\n",
      "Epoch: 7350 mean train loss:  5.24377031e-05, mean val. rec. loss:  5.70871935e-05\n",
      "Epoch: 7351 mean train loss:  5.29595036e-05, mean val. rec. loss:  5.52446104e-05\n",
      "Epoch: 7352 mean train loss:  5.26391144e-05, mean val. rec. loss:  5.70064729e-05\n",
      "Epoch: 7353 mean train loss:  5.23029294e-05, mean val. rec. loss:  5.46439982e-05\n",
      "Epoch: 7354 mean train loss:  5.32191683e-05, mean val. rec. loss:  5.48198955e-05\n",
      "Epoch: 7355 mean train loss:  5.29044881e-05, mean val. rec. loss:  5.51564801e-05\n",
      "Epoch: 7356 mean train loss:  5.24247479e-05, mean val. rec. loss:  5.84629251e-05\n",
      "Epoch: 7357 mean train loss:  5.38269020e-05, mean val. rec. loss:  5.56042872e-05\n",
      "Epoch: 7358 mean train loss:  5.28845867e-05, mean val. rec. loss:  5.48799227e-05\n",
      "Epoch: 7359 mean train loss:  5.25584984e-05, mean val. rec. loss:  5.49784793e-05\n",
      "Epoch: 7360 mean train loss:  5.23400228e-05, mean val. rec. loss:  5.48292724e-05\n",
      "Epoch: 7361 mean train loss:  5.23453796e-05, mean val. rec. loss:  5.55187827e-05\n",
      "Epoch: 7362 mean train loss:  5.24988198e-05, mean val. rec. loss:  5.54266136e-05\n",
      "Epoch: 7363 mean train loss:  5.31055523e-05, mean val. rec. loss:  5.51065612e-05\n",
      "Epoch: 7364 mean train loss:  5.23070916e-05, mean val. rec. loss:  5.69732996e-05\n",
      "Epoch: 7365 mean train loss:  5.20577906e-05, mean val. rec. loss:  5.49299324e-05\n",
      "Epoch: 7366 mean train loss:  5.30537690e-05, mean val. rec. loss:  5.65444233e-05\n",
      "Epoch: 7367 mean train loss:  5.23713358e-05, mean val. rec. loss:  5.59582307e-05\n",
      "Epoch: 7368 mean train loss:  5.29102475e-05, mean val. rec. loss:  5.68790498e-05\n",
      "Epoch: 7369 mean train loss:  5.29214569e-05, mean val. rec. loss:  5.48147755e-05\n",
      "Epoch: 7370 mean train loss:  5.30012269e-05, mean val. rec. loss:  5.47127026e-05\n",
      "Epoch: 7371 mean train loss:  5.29137880e-05, mean val. rec. loss:  5.57081819e-05\n",
      "Epoch: 7372 mean train loss:  5.26758919e-05, mean val. rec. loss:  5.42007841e-05\n",
      "Epoch: 7373 mean train loss:  5.25083133e-05, mean val. rec. loss:  5.42076077e-05\n",
      "Epoch: 7374 mean train loss:  5.23104613e-05, mean val. rec. loss:  5.57256272e-05\n",
      "Epoch: 7375 mean train loss:  5.23096047e-05, mean val. rec. loss:  5.48298493e-05\n",
      "Epoch: 7376 mean train loss:  5.22625809e-05, mean val. rec. loss:  5.68941644e-05\n",
      "Epoch: 7377 mean train loss:  5.20749794e-05, mean val. rec. loss:  5.46710702e-05\n",
      "Epoch: 7378 mean train loss:  5.27350417e-05, mean val. rec. loss:  5.44234429e-05\n",
      "Epoch: 7379 mean train loss:  5.16546632e-05, mean val. rec. loss:  5.53278025e-05\n",
      "Epoch: 7380 mean train loss:  5.17693575e-05, mean val. rec. loss:  5.47771319e-05\n",
      "Epoch: 7381 mean train loss:  5.18989480e-05, mean val. rec. loss:  5.45349245e-05\n",
      "Epoch: 7382 mean train loss:  5.22050896e-05, mean val. rec. loss:  5.46883065e-05\n",
      "Epoch: 7383 mean train loss:  5.20740148e-05, mean val. rec. loss:  5.69317081e-05\n",
      "Epoch: 7384 mean train loss:  5.24279569e-05, mean val. rec. loss:  5.52334709e-05\n",
      "Epoch: 7385 mean train loss:  5.17966328e-05, mean val. rec. loss:  5.45798687e-05\n",
      "Epoch: 7386 mean train loss:  5.21417145e-05, mean val. rec. loss:  5.45884233e-05\n",
      "Epoch: 7387 mean train loss:  5.21925628e-05, mean val. rec. loss:  5.55490484e-05\n",
      "Epoch: 7388 mean train loss:  5.17659833e-05, mean val. rec. loss:  5.47065013e-05\n",
      "Epoch: 7389 mean train loss:  5.15460044e-05, mean val. rec. loss:  5.44412516e-05\n",
      "Epoch: 7390 mean train loss:  5.14048638e-05, mean val. rec. loss:  5.38412663e-05\n",
      "Epoch: 7391 mean train loss:  5.18370327e-05, mean val. rec. loss:  5.39991459e-05\n",
      "Epoch: 7392 mean train loss:  5.16152395e-05, mean val. rec. loss:  5.37487792e-05\n",
      "Epoch: 7393 mean train loss:  5.17321645e-05, mean val. rec. loss:  5.49478002e-05\n",
      "Epoch: 7394 mean train loss:  5.13341677e-05, mean val. rec. loss:  5.48788233e-05\n",
      "Epoch: 7395 mean train loss:  5.20461638e-05, mean val. rec. loss:  5.53881522e-05\n",
      "Epoch: 7396 mean train loss:  5.22775673e-05, mean val. rec. loss:  5.42334576e-05\n",
      "Epoch: 7397 mean train loss:  5.18329253e-05, mean val. rec. loss:  5.36694169e-05\n",
      "Epoch: 7398 mean train loss:  5.13577430e-05, mean val. rec. loss:  5.54946456e-05\n",
      "Epoch: 7399 mean train loss:  5.20774672e-05, mean val. rec. loss:  5.55480444e-05\n",
      "Epoch: 7400 mean train loss:  5.25515462e-05, mean val. rec. loss:  5.35320263e-05\n",
      "Epoch: 7401 mean train loss:  5.15493078e-05, mean val. rec. loss:  5.65832526e-05\n",
      "Epoch: 7402 mean train loss:  5.21167732e-05, mean val. rec. loss:  5.46777575e-05\n",
      "Epoch: 7403 mean train loss:  5.13844662e-05, mean val. rec. loss:  5.44842923e-05\n",
      "Epoch: 7404 mean train loss:  5.13586038e-05, mean val. rec. loss:  5.38032593e-05\n",
      "Epoch: 7405 mean train loss:  5.11565381e-05, mean val. rec. loss:  5.58809218e-05\n",
      "Epoch: 7406 mean train loss:  5.15803973e-05, mean val. rec. loss:  5.40007678e-05\n",
      "Epoch: 7407 mean train loss:  5.30738388e-05, mean val. rec. loss:  5.63315002e-05\n",
      "Epoch: 7408 mean train loss:  5.29014449e-05, mean val. rec. loss:  5.41817942e-05\n",
      "Epoch: 7409 mean train loss:  5.15133788e-05, mean val. rec. loss:  5.58412929e-05\n",
      "Epoch: 7410 mean train loss:  5.22052588e-05, mean val. rec. loss:  5.38982406e-05\n",
      "Epoch: 7411 mean train loss:  5.16664011e-05, mean val. rec. loss:  5.35152580e-05\n",
      "Epoch: 7412 mean train loss:  5.13928495e-05, mean val. rec. loss:  5.48493662e-05\n",
      "Epoch: 7413 mean train loss:  5.14174153e-05, mean val. rec. loss:  5.48133945e-05\n",
      "Epoch: 7414 mean train loss:  5.17339834e-05, mean val. rec. loss:  5.31202455e-05\n",
      "Epoch: 7415 mean train loss:  5.11496680e-05, mean val. rec. loss:  5.46351575e-05\n",
      "Epoch: 7416 mean train loss:  5.12006824e-05, mean val. rec. loss:  5.34069066e-05\n",
      "Epoch: 7417 mean train loss:  5.11539423e-05, mean val. rec. loss:  5.43624980e-05\n",
      "Epoch: 7418 mean train loss:  5.16090813e-05, mean val. rec. loss:  5.48105369e-05\n",
      "Epoch: 7419 mean train loss:  5.16916620e-05, mean val. rec. loss:  5.44659430e-05\n",
      "Epoch: 7420 mean train loss:  5.17507211e-05, mean val. rec. loss:  5.29325544e-05\n",
      "Epoch: 7421 mean train loss:  5.14629120e-05, mean val. rec. loss:  5.44953319e-05\n",
      "Epoch: 7422 mean train loss:  5.18580961e-05, mean val. rec. loss:  5.47925328e-05\n",
      "Epoch: 7423 mean train loss:  5.07625571e-05, mean val. rec. loss:  5.36915551e-05\n",
      "Epoch: 7424 mean train loss:  5.19364756e-05, mean val. rec. loss:  5.55523421e-05\n",
      "Epoch: 7425 mean train loss:  5.23574712e-05, mean val. rec. loss:  5.49199923e-05\n",
      "Epoch: 7426 mean train loss:  5.12804947e-05, mean val. rec. loss:  5.47353360e-05\n",
      "Epoch: 7427 mean train loss:  5.10087916e-05, mean val. rec. loss:  5.38374729e-05\n",
      "Epoch: 7428 mean train loss:  5.10720709e-05, mean val. rec. loss:  5.34332607e-05\n",
      "Epoch: 7429 mean train loss:  5.09444553e-05, mean val. rec. loss:  5.44254055e-05\n",
      "Epoch: 7430 mean train loss:  5.07127802e-05, mean val. rec. loss:  5.33484649e-05\n",
      "Epoch: 7431 mean train loss:  5.11232283e-05, mean val. rec. loss:  5.38578938e-05\n",
      "Epoch: 7432 mean train loss:  5.16144165e-05, mean val. rec. loss:  5.51769283e-05\n",
      "Epoch: 7433 mean train loss:  5.10777202e-05, mean val. rec. loss:  5.35142404e-05\n",
      "Epoch: 7434 mean train loss:  5.08424874e-05, mean val. rec. loss:  5.59192060e-05\n",
      "Epoch: 7435 mean train loss:  5.16264910e-05, mean val. rec. loss:  5.32966743e-05\n",
      "Epoch: 7436 mean train loss:  5.10772327e-05, mean val. rec. loss:  5.36849223e-05\n",
      "Epoch: 7437 mean train loss:  5.19118538e-05, mean val. rec. loss:  5.56370471e-05\n",
      "Epoch: 7438 mean train loss:  5.16034496e-05, mean val. rec. loss:  5.43648513e-05\n",
      "Epoch: 7439 mean train loss:  5.10547911e-05, mean val. rec. loss:  5.40296706e-05\n",
      "Epoch: 7440 mean train loss:  5.08798278e-05, mean val. rec. loss:  5.34289039e-05\n",
      "Epoch: 7441 mean train loss:  5.11694137e-05, mean val. rec. loss:  5.37796082e-05\n",
      "Epoch: 7442 mean train loss:  5.09491107e-05, mean val. rec. loss:  5.38292954e-05\n",
      "Epoch: 7443 mean train loss:  5.13753745e-05, mean val. rec. loss:  5.32391140e-05\n",
      "Epoch: 7444 mean train loss:  5.09795518e-05, mean val. rec. loss:  5.33980976e-05\n",
      "Epoch: 7445 mean train loss:  5.08883306e-05, mean val. rec. loss:  5.41112863e-05\n",
      "Epoch: 7446 mean train loss:  5.06327522e-05, mean val. rec. loss:  5.35218091e-05\n",
      "Epoch: 7447 mean train loss:  5.12588170e-05, mean val. rec. loss:  5.37049207e-05\n",
      "Epoch: 7448 mean train loss:  5.13289948e-05, mean val. rec. loss:  5.45541052e-05\n",
      "Epoch: 7449 mean train loss:  5.08642046e-05, mean val. rec. loss:  5.34603690e-05\n",
      "Epoch: 7450 mean train loss:  5.07024024e-05, mean val. rec. loss:  5.25098657e-05\n",
      "Epoch: 7451 mean train loss:  5.07281798e-05, mean val. rec. loss:  5.44870000e-05\n",
      "Epoch: 7452 mean train loss:  5.04643164e-05, mean val. rec. loss:  5.29992371e-05\n",
      "Epoch: 7453 mean train loss:  5.08935469e-05, mean val. rec. loss:  5.33300020e-05\n",
      "Epoch: 7454 mean train loss:  5.09201004e-05, mean val. rec. loss:  5.27960998e-05\n",
      "Epoch: 7455 mean train loss:  5.12431019e-05, mean val. rec. loss:  5.27403522e-05\n",
      "Epoch: 7456 mean train loss:  5.10555305e-05, mean val. rec. loss:  5.39114290e-05\n",
      "Epoch: 7457 mean train loss:  5.09235957e-05, mean val. rec. loss:  5.60485826e-05\n",
      "Epoch: 7458 mean train loss:  5.08570514e-05, mean val. rec. loss:  5.38731675e-05\n",
      "Epoch: 7459 mean train loss:  5.09839063e-05, mean val. rec. loss:  5.37305752e-05\n",
      "Epoch: 7460 mean train loss:  5.01299204e-05, mean val. rec. loss:  5.26109211e-05\n",
      "Epoch: 7461 mean train loss:  5.03332677e-05, mean val. rec. loss:  5.31477081e-05\n",
      "Epoch: 7462 mean train loss:  5.06628644e-05, mean val. rec. loss:  5.39461650e-05\n",
      "Epoch: 7463 mean train loss:  5.05300093e-05, mean val. rec. loss:  5.29082992e-05\n",
      "Epoch: 7464 mean train loss:  5.07292788e-05, mean val. rec. loss:  5.36465518e-05\n",
      "Epoch: 7465 mean train loss:  5.06192275e-05, mean val. rec. loss:  5.30517501e-05\n",
      "Epoch: 7466 mean train loss:  5.01520993e-05, mean val. rec. loss:  5.27601235e-05\n",
      "Epoch: 7467 mean train loss:  5.02594118e-05, mean val. rec. loss:  5.44195904e-05\n",
      "Epoch: 7468 mean train loss:  5.03152940e-05, mean val. rec. loss:  5.26484556e-05\n",
      "Epoch: 7469 mean train loss:  5.03199637e-05, mean val. rec. loss:  5.34223620e-05\n",
      "Epoch: 7470 mean train loss:  5.07229622e-05, mean val. rec. loss:  5.27004371e-05\n",
      "Epoch: 7471 mean train loss:  5.05645496e-05, mean val. rec. loss:  5.39087259e-05\n",
      "Epoch: 7472 mean train loss:  5.07015211e-05, mean val. rec. loss:  5.21611695e-05\n",
      "Epoch: 7473 mean train loss:  5.04173125e-05, mean val. rec. loss:  5.52087568e-05\n",
      "Epoch: 7474 mean train loss:  5.00668242e-05, mean val. rec. loss:  5.24520601e-05\n",
      "Epoch: 7475 mean train loss:  5.05280685e-05, mean val. rec. loss:  5.38270830e-05\n",
      "Epoch: 7476 mean train loss:  5.03380898e-05, mean val. rec. loss:  5.28189785e-05\n",
      "Epoch: 7477 mean train loss:  5.00791659e-05, mean val. rec. loss:  5.28811136e-05\n",
      "Epoch: 7478 mean train loss:  5.03483502e-05, mean val. rec. loss:  5.52557545e-05\n",
      "Epoch: 7479 mean train loss:  5.08668399e-05, mean val. rec. loss:  5.31955645e-05\n",
      "Epoch: 7480 mean train loss:  4.98406157e-05, mean val. rec. loss:  5.29624249e-05\n",
      "Epoch: 7481 mean train loss:  5.06682004e-05, mean val. rec. loss:  5.23608360e-05\n",
      "Epoch: 7482 mean train loss:  5.03884642e-05, mean val. rec. loss:  5.58291812e-05\n",
      "Epoch: 7483 mean train loss:  5.12630395e-05, mean val. rec. loss:  5.39927494e-05\n",
      "Epoch: 7484 mean train loss:  5.03860319e-05, mean val. rec. loss:  5.25557822e-05\n",
      "Epoch: 7485 mean train loss:  5.02604619e-05, mean val. rec. loss:  5.40119346e-05\n",
      "Epoch: 7486 mean train loss:  5.06979302e-05, mean val. rec. loss:  5.27140843e-05\n",
      "Epoch: 7487 mean train loss:  5.07203900e-05, mean val. rec. loss:  5.46114020e-05\n",
      "Epoch: 7488 mean train loss:  5.04531394e-05, mean val. rec. loss:  5.23132613e-05\n",
      "Epoch: 7489 mean train loss:  5.04557588e-05, mean val. rec. loss:  5.43370252e-05\n",
      "Epoch: 7490 mean train loss:  5.01927128e-05, mean val. rec. loss:  5.21852476e-05\n",
      "Epoch: 7491 mean train loss:  5.03039364e-05, mean val. rec. loss:  5.27536905e-05\n",
      "Epoch: 7492 mean train loss:  4.98057562e-05, mean val. rec. loss:  5.20629081e-05\n",
      "Epoch: 7493 mean train loss:  4.96420706e-05, mean val. rec. loss:  5.24118088e-05\n",
      "Epoch: 7494 mean train loss:  4.97010654e-05, mean val. rec. loss:  5.26403872e-05\n",
      "Epoch: 7495 mean train loss:  5.01727655e-05, mean val. rec. loss:  5.37363222e-05\n",
      "Epoch: 7496 mean train loss:  5.02872474e-05, mean val. rec. loss:  5.29414497e-05\n",
      "Epoch: 7497 mean train loss:  4.97365262e-05, mean val. rec. loss:  5.14197050e-05\n",
      "Epoch: 7498 mean train loss:  5.01622441e-05, mean val. rec. loss:  5.39705975e-05\n",
      "Epoch: 7499 mean train loss:  5.00824804e-05, mean val. rec. loss:  5.31237027e-05\n",
      "Epoch: 7500 mean train loss:  4.98337306e-05, mean val. rec. loss:  5.31742667e-05\n",
      "Epoch: 7501 mean train loss:  4.97486226e-05, mean val. rec. loss:  5.29160769e-05\n",
      "Epoch: 7502 mean train loss:  5.13306045e-05, mean val. rec. loss:  5.70087035e-05\n",
      "Epoch: 7503 mean train loss:  5.18278244e-05, mean val. rec. loss:  5.59303319e-05\n",
      "Epoch: 7504 mean train loss:  5.16618244e-05, mean val. rec. loss:  5.22966474e-05\n",
      "Epoch: 7505 mean train loss:  5.02485313e-05, mean val. rec. loss:  5.16480880e-05\n",
      "Epoch: 7506 mean train loss:  5.04575870e-05, mean val. rec. loss:  5.50661236e-05\n",
      "Epoch: 7507 mean train loss:  5.03600628e-05, mean val. rec. loss:  5.22046918e-05\n",
      "Epoch: 7508 mean train loss:  5.06770878e-05, mean val. rec. loss:  5.36168403e-05\n",
      "Epoch: 7509 mean train loss:  5.07146187e-05, mean val. rec. loss:  5.48066253e-05\n",
      "Epoch: 7510 mean train loss:  5.12951704e-05, mean val. rec. loss:  5.19946127e-05\n",
      "Epoch: 7511 mean train loss:  5.06902719e-05, mean val. rec. loss:  5.20257506e-05\n",
      "Epoch: 7512 mean train loss:  4.99920392e-05, mean val. rec. loss:  5.30548939e-05\n",
      "Epoch: 7513 mean train loss:  5.04256151e-05, mean val. rec. loss:  5.26022075e-05\n",
      "Epoch: 7514 mean train loss:  5.03483087e-05, mean val. rec. loss:  5.27327789e-05\n",
      "Epoch: 7515 mean train loss:  4.93391158e-05, mean val. rec. loss:  5.23661786e-05\n",
      "Epoch: 7516 mean train loss:  4.93900824e-05, mean val. rec. loss:  5.16204572e-05\n",
      "Epoch: 7517 mean train loss:  5.00259722e-05, mean val. rec. loss:  5.14887728e-05\n",
      "Epoch: 7518 mean train loss:  4.96200947e-05, mean val. rec. loss:  5.29561101e-05\n",
      "Epoch: 7519 mean train loss:  4.98437161e-05, mean val. rec. loss:  5.30700903e-05\n",
      "Epoch: 7520 mean train loss:  5.04450907e-05, mean val. rec. loss:  5.23947952e-05\n",
      "Epoch: 7521 mean train loss:  5.00369087e-05, mean val. rec. loss:  5.43429448e-05\n",
      "Epoch: 7522 mean train loss:  5.07294863e-05, mean val. rec. loss:  5.30122711e-05\n",
      "Epoch: 7523 mean train loss:  5.02405664e-05, mean val. rec. loss:  5.36904829e-05\n",
      "Epoch: 7524 mean train loss:  5.00071348e-05, mean val. rec. loss:  5.15391187e-05\n",
      "Epoch: 7525 mean train loss:  4.95550738e-05, mean val. rec. loss:  5.34146343e-05\n",
      "Epoch: 7526 mean train loss:  4.97200270e-05, mean val. rec. loss:  5.17474851e-05\n",
      "Epoch: 7527 mean train loss:  5.02717083e-05, mean val. rec. loss:  5.22022840e-05\n",
      "Epoch: 7528 mean train loss:  5.00777550e-05, mean val. rec. loss:  5.56683577e-05\n",
      "Epoch: 7529 mean train loss:  5.12482387e-05, mean val. rec. loss:  5.34691007e-05\n",
      "Epoch: 7530 mean train loss:  4.98187700e-05, mean val. rec. loss:  5.09977977e-05\n",
      "Epoch: 7531 mean train loss:  4.94659099e-05, mean val. rec. loss:  5.18128866e-05\n",
      "Epoch: 7532 mean train loss:  4.93298705e-05, mean val. rec. loss:  5.28798234e-05\n",
      "Epoch: 7533 mean train loss:  4.99825554e-05, mean val. rec. loss:  5.25383506e-05\n",
      "Epoch: 7534 mean train loss:  4.97056436e-05, mean val. rec. loss:  5.17731033e-05\n",
      "Epoch: 7535 mean train loss:  4.92375653e-05, mean val. rec. loss:  5.27829659e-05\n",
      "Epoch: 7536 mean train loss:  4.97849186e-05, mean val. rec. loss:  5.22161402e-05\n",
      "Epoch: 7537 mean train loss:  4.92721638e-05, mean val. rec. loss:  5.12370295e-05\n",
      "Epoch: 7538 mean train loss:  4.93736507e-05, mean val. rec. loss:  5.34927609e-05\n",
      "Epoch: 7539 mean train loss:  4.93749982e-05, mean val. rec. loss:  5.11196783e-05\n",
      "Epoch: 7540 mean train loss:  4.96661167e-05, mean val. rec. loss:  5.20415332e-05\n",
      "Epoch: 7541 mean train loss:  4.97122416e-05, mean val. rec. loss:  5.42692068e-05\n",
      "Epoch: 7542 mean train loss:  5.02863468e-05, mean val. rec. loss:  5.13734887e-05\n",
      "Epoch: 7543 mean train loss:  4.94295357e-05, mean val. rec. loss:  5.29372111e-05\n",
      "Epoch: 7544 mean train loss:  4.99194250e-05, mean val. rec. loss:  5.09885662e-05\n",
      "Epoch: 7545 mean train loss:  4.92472859e-05, mean val. rec. loss:  5.22880929e-05\n",
      "Epoch: 7546 mean train loss:  4.92589524e-05, mean val. rec. loss:  5.12019072e-05\n",
      "Epoch: 7547 mean train loss:  4.90792032e-05, mean val. rec. loss:  5.16851410e-05\n",
      "Epoch: 7548 mean train loss:  4.88450603e-05, mean val. rec. loss:  5.16190398e-05\n",
      "Epoch: 7549 mean train loss:  4.88703334e-05, mean val. rec. loss:  5.09715571e-05\n",
      "Epoch: 7550 mean train loss:  4.88602946e-05, mean val. rec. loss:  5.13533903e-05\n",
      "Epoch: 7551 mean train loss:  4.90342359e-05, mean val. rec. loss:  5.43287796e-05\n",
      "Epoch: 7552 mean train loss:  5.03069087e-05, mean val. rec. loss:  5.15629833e-05\n",
      "Epoch: 7553 mean train loss:  4.92431873e-05, mean val. rec. loss:  5.20092276e-05\n",
      "Epoch: 7554 mean train loss:  5.02230896e-05, mean val. rec. loss:  5.31619460e-05\n",
      "Epoch: 7555 mean train loss:  4.93362482e-05, mean val. rec. loss:  5.17674154e-05\n",
      "Epoch: 7556 mean train loss:  4.91283157e-05, mean val. rec. loss:  5.13744200e-05\n",
      "Epoch: 7557 mean train loss:  4.87928488e-05, mean val. rec. loss:  5.11844302e-05\n",
      "Epoch: 7558 mean train loss:  4.85770753e-05, mean val. rec. loss:  5.15864208e-05\n",
      "Epoch: 7559 mean train loss:  4.88474253e-05, mean val. rec. loss:  5.13544715e-05\n",
      "Epoch: 7560 mean train loss:  4.86993566e-05, mean val. rec. loss:  5.13855186e-05\n",
      "Epoch: 7561 mean train loss:  4.89403037e-05, mean val. rec. loss:  5.19022073e-05\n",
      "Epoch: 7562 mean train loss:  4.85157889e-05, mean val. rec. loss:  5.14208180e-05\n",
      "Epoch: 7563 mean train loss:  4.86841007e-05, mean val. rec. loss:  5.15685939e-05\n",
      "Epoch: 7564 mean train loss:  4.87137518e-05, mean val. rec. loss:  5.15019203e-05\n",
      "Epoch: 7565 mean train loss:  4.92023845e-05, mean val. rec. loss:  5.04999626e-05\n",
      "Epoch: 7566 mean train loss:  4.87903812e-05, mean val. rec. loss:  5.15196018e-05\n",
      "Epoch: 7567 mean train loss:  4.88915229e-05, mean val. rec. loss:  5.19155093e-05\n",
      "Epoch: 7568 mean train loss:  4.90324196e-05, mean val. rec. loss:  5.09374480e-05\n",
      "Epoch: 7569 mean train loss:  4.86919717e-05, mean val. rec. loss:  5.21523923e-05\n",
      "Epoch: 7570 mean train loss:  4.92227613e-05, mean val. rec. loss:  5.12354803e-05\n",
      "Epoch: 7571 mean train loss:  4.88457727e-05, mean val. rec. loss:  5.13224613e-05\n",
      "Epoch: 7572 mean train loss:  4.84213168e-05, mean val. rec. loss:  5.23697948e-05\n",
      "Epoch: 7573 mean train loss:  4.94681856e-05, mean val. rec. loss:  5.19675634e-05\n",
      "Epoch: 7574 mean train loss:  4.90592136e-05, mean val. rec. loss:  5.13171732e-05\n",
      "Epoch: 7575 mean train loss:  4.88909021e-05, mean val. rec. loss:  5.14739943e-05\n",
      "Epoch: 7576 mean train loss:  4.91904633e-05, mean val. rec. loss:  5.14729539e-05\n",
      "Epoch: 7577 mean train loss:  4.84725716e-05, mean val. rec. loss:  5.06056246e-05\n",
      "Epoch: 7578 mean train loss:  4.83732374e-05, mean val. rec. loss:  5.16276988e-05\n",
      "Epoch: 7579 mean train loss:  4.82079791e-05, mean val. rec. loss:  5.22451475e-05\n",
      "Epoch: 7580 mean train loss:  4.92473297e-05, mean val. rec. loss:  5.14400351e-05\n",
      "Epoch: 7581 mean train loss:  4.91198863e-05, mean val. rec. loss:  5.12776851e-05\n",
      "Epoch: 7582 mean train loss:  4.93102781e-05, mean val. rec. loss:  5.26397603e-05\n",
      "Epoch: 7583 mean train loss:  4.91711699e-05, mean val. rec. loss:  5.13284263e-05\n",
      "Epoch: 7584 mean train loss:  4.87184996e-05, mean val. rec. loss:  5.23054745e-05\n",
      "Epoch: 7585 mean train loss:  4.91324569e-05, mean val. rec. loss:  5.14378362e-05\n",
      "Epoch: 7586 mean train loss:  4.93265180e-05, mean val. rec. loss:  5.49068765e-05\n",
      "Epoch: 7587 mean train loss:  4.98662242e-05, mean val. rec. loss:  5.33845367e-05\n",
      "Epoch: 7588 mean train loss:  5.00152088e-05, mean val. rec. loss:  5.13650568e-05\n",
      "Epoch: 7589 mean train loss:  4.98298594e-05, mean val. rec. loss:  5.40306701e-05\n",
      "Epoch: 7590 mean train loss:  4.95980036e-05, mean val. rec. loss:  5.30786858e-05\n",
      "Epoch: 7591 mean train loss:  4.98658649e-05, mean val. rec. loss:  5.33595908e-05\n",
      "Epoch: 7592 mean train loss:  4.89114867e-05, mean val. rec. loss:  5.15753676e-05\n",
      "Epoch: 7593 mean train loss:  4.91449834e-05, mean val. rec. loss:  5.07966956e-05\n",
      "Epoch: 7594 mean train loss:  4.84886133e-05, mean val. rec. loss:  5.18014745e-05\n",
      "Epoch: 7595 mean train loss:  4.89661817e-05, mean val. rec. loss:  5.19752911e-05\n",
      "Epoch: 7596 mean train loss:  4.88962546e-05, mean val. rec. loss:  5.11759438e-05\n",
      "Epoch: 7597 mean train loss:  4.87941272e-05, mean val. rec. loss:  5.15747316e-05\n",
      "Epoch: 7598 mean train loss:  4.87124600e-05, mean val. rec. loss:  5.10230206e-05\n",
      "Epoch: 7599 mean train loss:  4.84435301e-05, mean val. rec. loss:  5.10583836e-05\n",
      "Epoch: 7600 mean train loss:  4.83474344e-05, mean val. rec. loss:  5.10384397e-05\n",
      "Epoch: 7601 mean train loss:  4.80154122e-05, mean val. rec. loss:  5.00208539e-05\n",
      "Epoch: 7602 mean train loss:  4.81275547e-05, mean val. rec. loss:  5.06057109e-05\n",
      "Epoch: 7603 mean train loss:  4.80058397e-05, mean val. rec. loss:  5.09533531e-05\n",
      "Epoch: 7604 mean train loss:  4.79657502e-05, mean val. rec. loss:  4.97579575e-05\n",
      "Epoch: 7605 mean train loss:  4.80709888e-05, mean val. rec. loss:  5.06206347e-05\n",
      "Epoch: 7606 mean train loss:  4.82163391e-05, mean val. rec. loss:  5.01201602e-05\n",
      "Epoch: 7607 mean train loss:  4.76004334e-05, mean val. rec. loss:  5.22741094e-05\n",
      "Epoch: 7608 mean train loss:  4.81818693e-05, mean val. rec. loss:  4.98885198e-05\n",
      "Epoch: 7609 mean train loss:  4.81585971e-05, mean val. rec. loss:  5.05223416e-05\n",
      "Epoch: 7610 mean train loss:  4.79928455e-05, mean val. rec. loss:  5.02037112e-05\n",
      "Epoch: 7611 mean train loss:  4.79686317e-05, mean val. rec. loss:  5.02519855e-05\n",
      "Epoch: 7612 mean train loss:  4.79393869e-05, mean val. rec. loss:  4.98146818e-05\n",
      "Epoch: 7613 mean train loss:  4.76950961e-05, mean val. rec. loss:  5.09882982e-05\n",
      "Epoch: 7614 mean train loss:  4.79349500e-05, mean val. rec. loss:  5.00508062e-05\n",
      "Epoch: 7615 mean train loss:  4.75188717e-05, mean val. rec. loss:  5.05975107e-05\n",
      "Epoch: 7616 mean train loss:  4.79281269e-05, mean val. rec. loss:  4.96879583e-05\n",
      "Epoch: 7617 mean train loss:  4.76315407e-05, mean val. rec. loss:  5.09351128e-05\n",
      "Epoch: 7618 mean train loss:  4.81462903e-05, mean val. rec. loss:  5.10111951e-05\n",
      "Epoch: 7619 mean train loss:  4.81827649e-05, mean val. rec. loss:  5.01582172e-05\n",
      "Epoch: 7620 mean train loss:  4.79044617e-05, mean val. rec. loss:  5.11927394e-05\n",
      "Epoch: 7621 mean train loss:  4.85694622e-05, mean val. rec. loss:  5.21168204e-05\n",
      "Epoch: 7622 mean train loss:  4.79916777e-05, mean val. rec. loss:  4.92543436e-05\n",
      "Epoch: 7623 mean train loss:  4.84023233e-05, mean val. rec. loss:  5.11428023e-05\n",
      "Epoch: 7624 mean train loss:  4.84892641e-05, mean val. rec. loss:  5.11602022e-05\n",
      "Epoch: 7625 mean train loss:  4.84198712e-05, mean val. rec. loss:  5.11485584e-05\n",
      "Epoch: 7626 mean train loss:  4.94081667e-05, mean val. rec. loss:  5.09600814e-05\n",
      "Epoch: 7627 mean train loss:  4.77679317e-05, mean val. rec. loss:  5.04564766e-05\n",
      "Epoch: 7628 mean train loss:  4.77948770e-05, mean val. rec. loss:  5.18935574e-05\n",
      "Epoch: 7629 mean train loss:  4.83138863e-05, mean val. rec. loss:  5.03526092e-05\n",
      "Epoch: 7630 mean train loss:  4.77469826e-05, mean val. rec. loss:  4.94933528e-05\n",
      "Epoch: 7631 mean train loss:  4.85060003e-05, mean val. rec. loss:  5.02411276e-05\n",
      "Epoch: 7632 mean train loss:  4.83962344e-05, mean val. rec. loss:  4.98450975e-05\n",
      "Epoch: 7633 mean train loss:  4.83991870e-05, mean val. rec. loss:  5.06618174e-05\n",
      "Epoch: 7634 mean train loss:  4.80722132e-05, mean val. rec. loss:  5.13592462e-05\n",
      "Epoch: 7635 mean train loss:  4.85136004e-05, mean val. rec. loss:  4.97256837e-05\n",
      "Epoch: 7636 mean train loss:  4.78368041e-05, mean val. rec. loss:  5.11887233e-05\n",
      "Epoch: 7637 mean train loss:  4.82230693e-05, mean val. rec. loss:  4.94304726e-05\n",
      "Epoch: 7638 mean train loss:  4.78950177e-05, mean val. rec. loss:  5.04028779e-05\n",
      "Epoch: 7639 mean train loss:  4.79187059e-05, mean val. rec. loss:  5.02325594e-05\n",
      "Epoch: 7640 mean train loss:  4.80770117e-05, mean val. rec. loss:  5.02662779e-05\n",
      "Epoch: 7641 mean train loss:  4.76044852e-05, mean val. rec. loss:  5.00037221e-05\n",
      "Epoch: 7642 mean train loss:  4.75313437e-05, mean val. rec. loss:  4.98029335e-05\n",
      "Epoch: 7643 mean train loss:  4.73017030e-05, mean val. rec. loss:  5.03319066e-05\n",
      "Epoch: 7644 mean train loss:  4.74356753e-05, mean val. rec. loss:  5.01775887e-05\n",
      "Epoch: 7645 mean train loss:  4.77971871e-05, mean val. rec. loss:  4.95954621e-05\n",
      "Epoch: 7646 mean train loss:  4.75317948e-05, mean val. rec. loss:  5.01370375e-05\n",
      "Epoch: 7647 mean train loss:  4.74966434e-05, mean val. rec. loss:  4.95698121e-05\n",
      "Epoch: 7648 mean train loss:  4.73921485e-05, mean val. rec. loss:  4.96878675e-05\n",
      "Epoch: 7649 mean train loss:  4.74487957e-05, mean val. rec. loss:  5.11028509e-05\n",
      "Epoch: 7650 mean train loss:  4.74161897e-05, mean val. rec. loss:  4.96484430e-05\n",
      "Epoch: 7651 mean train loss:  4.74115740e-05, mean val. rec. loss:  4.96438000e-05\n",
      "Epoch: 7652 mean train loss:  4.75643036e-05, mean val. rec. loss:  4.91246308e-05\n",
      "Epoch: 7653 mean train loss:  4.75440624e-05, mean val. rec. loss:  5.09598088e-05\n",
      "Epoch: 7654 mean train loss:  4.76799200e-05, mean val. rec. loss:  4.99859225e-05\n",
      "Epoch: 7655 mean train loss:  4.72501425e-05, mean val. rec. loss:  4.87814317e-05\n",
      "Epoch: 7656 mean train loss:  4.74869554e-05, mean val. rec. loss:  5.01321356e-05\n",
      "Epoch: 7657 mean train loss:  4.72724013e-05, mean val. rec. loss:  5.00945056e-05\n",
      "Epoch: 7658 mean train loss:  4.74989563e-05, mean val. rec. loss:  5.10402024e-05\n",
      "Epoch: 7659 mean train loss:  4.75541397e-05, mean val. rec. loss:  5.08911817e-05\n",
      "Epoch: 7660 mean train loss:  4.78656543e-05, mean val. rec. loss:  5.09105259e-05\n",
      "Epoch: 7661 mean train loss:  4.70737547e-05, mean val. rec. loss:  4.95347353e-05\n",
      "Epoch: 7662 mean train loss:  4.83323200e-05, mean val. rec. loss:  4.97812269e-05\n",
      "Epoch: 7663 mean train loss:  4.78955752e-05, mean val. rec. loss:  5.20535631e-05\n",
      "Epoch: 7664 mean train loss:  4.83432184e-05, mean val. rec. loss:  4.90726767e-05\n",
      "Epoch: 7665 mean train loss:  4.77074219e-05, mean val. rec. loss:  4.96255779e-05\n",
      "Epoch: 7666 mean train loss:  4.73766915e-05, mean val. rec. loss:  4.92771906e-05\n",
      "Epoch: 7667 mean train loss:  4.70434435e-05, mean val. rec. loss:  4.95177126e-05\n",
      "Epoch: 7668 mean train loss:  4.74371956e-05, mean val. rec. loss:  5.26009082e-05\n",
      "Epoch: 7669 mean train loss:  4.76768887e-05, mean val. rec. loss:  4.94024739e-05\n",
      "Epoch: 7670 mean train loss:  4.73500536e-05, mean val. rec. loss:  5.02301017e-05\n",
      "Epoch: 7671 mean train loss:  4.70833235e-05, mean val. rec. loss:  5.06716167e-05\n",
      "Epoch: 7672 mean train loss:  4.73624220e-05, mean val. rec. loss:  4.96754014e-05\n",
      "Epoch: 7673 mean train loss:  4.69320612e-05, mean val. rec. loss:  4.88266258e-05\n",
      "Epoch: 7674 mean train loss:  4.71649396e-05, mean val. rec. loss:  4.93602055e-05\n",
      "Epoch: 7675 mean train loss:  4.67624871e-05, mean val. rec. loss:  4.90607785e-05\n",
      "Epoch: 7676 mean train loss:  4.70976660e-05, mean val. rec. loss:  4.99913514e-05\n",
      "Epoch: 7677 mean train loss:  4.75145398e-05, mean val. rec. loss:  5.00985489e-05\n",
      "Epoch: 7678 mean train loss:  4.73301154e-05, mean val. rec. loss:  4.95567645e-05\n",
      "Epoch: 7679 mean train loss:  4.69537198e-05, mean val. rec. loss:  4.97043042e-05\n",
      "Epoch: 7680 mean train loss:  4.72295164e-05, mean val. rec. loss:  5.01574494e-05\n",
      "Epoch: 7681 mean train loss:  4.73195670e-05, mean val. rec. loss:  5.02531939e-05\n",
      "Epoch: 7682 mean train loss:  4.76402751e-05, mean val. rec. loss:  5.01509665e-05\n",
      "Epoch: 7683 mean train loss:  4.73315229e-05, mean val. rec. loss:  5.00175829e-05\n",
      "Epoch: 7684 mean train loss:  4.80906486e-05, mean val. rec. loss:  5.01695930e-05\n",
      "Epoch: 7685 mean train loss:  4.72849611e-05, mean val. rec. loss:  4.94420483e-05\n",
      "Epoch: 7686 mean train loss:  4.69266504e-05, mean val. rec. loss:  4.90862831e-05\n",
      "Epoch: 7687 mean train loss:  4.71436209e-05, mean val. rec. loss:  4.91005755e-05\n",
      "Epoch: 7688 mean train loss:  4.69097484e-05, mean val. rec. loss:  5.00111954e-05\n",
      "Epoch: 7689 mean train loss:  4.71826779e-05, mean val. rec. loss:  4.91885423e-05\n",
      "Epoch: 7690 mean train loss:  4.67541771e-05, mean val. rec. loss:  4.87158120e-05\n",
      "Epoch: 7691 mean train loss:  4.67546198e-05, mean val. rec. loss:  5.09190123e-05\n",
      "Epoch: 7692 mean train loss:  4.71747082e-05, mean val. rec. loss:  4.99842916e-05\n",
      "Epoch: 7693 mean train loss:  4.70556439e-05, mean val. rec. loss:  4.93373858e-05\n",
      "Epoch: 7694 mean train loss:  4.68612596e-05, mean val. rec. loss:  4.98645644e-05\n",
      "Epoch: 7695 mean train loss:  4.67077927e-05, mean val. rec. loss:  4.91762625e-05\n",
      "Epoch: 7696 mean train loss:  4.68808174e-05, mean val. rec. loss:  4.98109656e-05\n",
      "Epoch: 7697 mean train loss:  4.67677359e-05, mean val. rec. loss:  4.89480067e-05\n",
      "Epoch: 7698 mean train loss:  4.66364015e-05, mean val. rec. loss:  4.90404847e-05\n",
      "Epoch: 7699 mean train loss:  4.70428834e-05, mean val. rec. loss:  4.99536851e-05\n",
      "Epoch: 7700 mean train loss:  4.73180244e-05, mean val. rec. loss:  4.75665146e-05\n",
      "Epoch: 7701 mean train loss:  4.69711540e-05, mean val. rec. loss:  5.05287564e-05\n",
      "Epoch: 7702 mean train loss:  4.70563490e-05, mean val. rec. loss:  4.89740110e-05\n",
      "Epoch: 7703 mean train loss:  4.70572689e-05, mean val. rec. loss:  5.00916753e-05\n",
      "Epoch: 7704 mean train loss:  4.68292962e-05, mean val. rec. loss:  4.98358978e-05\n",
      "Epoch: 7705 mean train loss:  4.69951674e-05, mean val. rec. loss:  4.96207487e-05\n",
      "Epoch: 7706 mean train loss:  4.66329446e-05, mean val. rec. loss:  4.94168299e-05\n",
      "Epoch: 7707 mean train loss:  4.67896194e-05, mean val. rec. loss:  4.90715591e-05\n",
      "Epoch: 7708 mean train loss:  4.64225359e-05, mean val. rec. loss:  4.85383701e-05\n",
      "Epoch: 7709 mean train loss:  4.64924934e-05, mean val. rec. loss:  4.88513672e-05\n",
      "Epoch: 7710 mean train loss:  4.65344692e-05, mean val. rec. loss:  4.80163070e-05\n",
      "Epoch: 7711 mean train loss:  4.63927228e-05, mean val. rec. loss:  5.05629791e-05\n",
      "Epoch: 7712 mean train loss:  4.71723879e-05, mean val. rec. loss:  5.08003800e-05\n",
      "Epoch: 7713 mean train loss:  4.74747411e-05, mean val. rec. loss:  5.05936536e-05\n",
      "Epoch: 7714 mean train loss:  4.83057745e-05, mean val. rec. loss:  4.91180980e-05\n",
      "Epoch: 7715 mean train loss:  4.67875356e-05, mean val. rec. loss:  5.02771993e-05\n",
      "Epoch: 7716 mean train loss:  4.71685961e-05, mean val. rec. loss:  4.89921922e-05\n",
      "Epoch: 7717 mean train loss:  4.70247949e-05, mean val. rec. loss:  4.86324155e-05\n",
      "Epoch: 7718 mean train loss:  4.73473193e-05, mean val. rec. loss:  4.95572779e-05\n",
      "Epoch: 7719 mean train loss:  4.67977153e-05, mean val. rec. loss:  4.88057369e-05\n",
      "Epoch: 7720 mean train loss:  4.64377677e-05, mean val. rec. loss:  4.95244181e-05\n",
      "Epoch: 7721 mean train loss:  4.65859819e-05, mean val. rec. loss:  4.89523680e-05\n",
      "Epoch: 7722 mean train loss:  4.62454962e-05, mean val. rec. loss:  4.77451059e-05\n",
      "Epoch: 7723 mean train loss:  4.62161249e-05, mean val. rec. loss:  4.98305779e-05\n",
      "Epoch: 7724 mean train loss:  4.63936336e-05, mean val. rec. loss:  4.85969753e-05\n",
      "Epoch: 7725 mean train loss:  4.66186768e-05, mean val. rec. loss:  4.86451042e-05\n",
      "Epoch: 7726 mean train loss:  4.65634286e-05, mean val. rec. loss:  4.81531660e-05\n",
      "Epoch: 7727 mean train loss:  4.64041315e-05, mean val. rec. loss:  5.11518748e-05\n",
      "Epoch: 7728 mean train loss:  4.71489166e-05, mean val. rec. loss:  4.95737555e-05\n",
      "Epoch: 7729 mean train loss:  4.67119387e-05, mean val. rec. loss:  4.86946960e-05\n",
      "Epoch: 7730 mean train loss:  4.65461673e-05, mean val. rec. loss:  4.90005242e-05\n",
      "Epoch: 7731 mean train loss:  4.62329916e-05, mean val. rec. loss:  4.90296995e-05\n",
      "Epoch: 7732 mean train loss:  4.65493564e-05, mean val. rec. loss:  4.91598666e-05\n",
      "Epoch: 7733 mean train loss:  4.65090616e-05, mean val. rec. loss:  4.92994651e-05\n",
      "Epoch: 7734 mean train loss:  4.66948227e-05, mean val. rec. loss:  4.97204229e-05\n",
      "Epoch: 7735 mean train loss:  4.67008402e-05, mean val. rec. loss:  4.87480085e-05\n",
      "Epoch: 7736 mean train loss:  4.64228915e-05, mean val. rec. loss:  4.84466871e-05\n",
      "Epoch: 7737 mean train loss:  4.65130416e-05, mean val. rec. loss:  4.84254029e-05\n",
      "Epoch: 7738 mean train loss:  4.62528086e-05, mean val. rec. loss:  4.77832538e-05\n",
      "Epoch: 7739 mean train loss:  4.63511808e-05, mean val. rec. loss:  4.91240811e-05\n",
      "Epoch: 7740 mean train loss:  4.64788286e-05, mean val. rec. loss:  4.92211159e-05\n",
      "Epoch: 7741 mean train loss:  4.68837689e-05, mean val. rec. loss:  4.91991548e-05\n",
      "Epoch: 7742 mean train loss:  4.62638730e-05, mean val. rec. loss:  4.74102886e-05\n",
      "Epoch: 7743 mean train loss:  4.63325277e-05, mean val. rec. loss:  4.90826668e-05\n",
      "Epoch: 7744 mean train loss:  4.62643143e-05, mean val. rec. loss:  5.01474865e-05\n",
      "Epoch: 7745 mean train loss:  4.67927833e-05, mean val. rec. loss:  5.13792220e-05\n",
      "Epoch: 7746 mean train loss:  4.68793763e-05, mean val. rec. loss:  4.90057668e-05\n",
      "Epoch: 7747 mean train loss:  4.62937862e-05, mean val. rec. loss:  4.99193806e-05\n",
      "Epoch: 7748 mean train loss:  4.61220340e-05, mean val. rec. loss:  4.78977201e-05\n",
      "Epoch: 7749 mean train loss:  4.57847113e-05, mean val. rec. loss:  4.96280629e-05\n",
      "Epoch: 7750 mean train loss:  4.61190231e-05, mean val. rec. loss:  4.82626986e-05\n",
      "Epoch: 7751 mean train loss:  4.65739398e-05, mean val. rec. loss:  4.87487491e-05\n",
      "Epoch: 7752 mean train loss:  4.65652273e-05, mean val. rec. loss:  4.82770546e-05\n",
      "Epoch: 7753 mean train loss:  4.60467990e-05, mean val. rec. loss:  4.81935263e-05\n",
      "Epoch: 7754 mean train loss:  4.61849901e-05, mean val. rec. loss:  4.92399604e-05\n",
      "Epoch: 7755 mean train loss:  4.60096589e-05, mean val. rec. loss:  4.82826198e-05\n",
      "Epoch: 7756 mean train loss:  4.58952217e-05, mean val. rec. loss:  4.89910519e-05\n",
      "Epoch: 7757 mean train loss:  4.62865682e-05, mean val. rec. loss:  4.89175592e-05\n",
      "Epoch: 7758 mean train loss:  4.58988199e-05, mean val. rec. loss:  4.84012703e-05\n",
      "Epoch: 7759 mean train loss:  4.57029235e-05, mean val. rec. loss:  4.76324431e-05\n",
      "Epoch: 7760 mean train loss:  4.57904343e-05, mean val. rec. loss:  4.83257651e-05\n",
      "Epoch: 7761 mean train loss:  4.57323238e-05, mean val. rec. loss:  4.76194409e-05\n",
      "Epoch: 7762 mean train loss:  4.56620988e-05, mean val. rec. loss:  4.74881472e-05\n",
      "Epoch: 7763 mean train loss:  4.54456007e-05, mean val. rec. loss:  4.86456221e-05\n",
      "Epoch: 7764 mean train loss:  4.57197117e-05, mean val. rec. loss:  4.72388344e-05\n",
      "Epoch: 7765 mean train loss:  4.56830364e-05, mean val. rec. loss:  5.02464430e-05\n",
      "Epoch: 7766 mean train loss:  4.61453671e-05, mean val. rec. loss:  4.78208065e-05\n",
      "Epoch: 7767 mean train loss:  4.62581910e-05, mean val. rec. loss:  4.72154105e-05\n",
      "Epoch: 7768 mean train loss:  4.63417664e-05, mean val. rec. loss:  4.97364780e-05\n",
      "Epoch: 7769 mean train loss:  4.66809765e-05, mean val. rec. loss:  4.81149954e-05\n",
      "Epoch: 7770 mean train loss:  4.63413607e-05, mean val. rec. loss:  4.76104321e-05\n",
      "Epoch: 7771 mean train loss:  4.61779770e-05, mean val. rec. loss:  4.99839508e-05\n",
      "Epoch: 7772 mean train loss:  4.62963204e-05, mean val. rec. loss:  4.76608825e-05\n",
      "Epoch: 7773 mean train loss:  4.63050116e-05, mean val. rec. loss:  4.87426705e-05\n",
      "Epoch: 7774 mean train loss:  4.59166404e-05, mean val. rec. loss:  4.79841741e-05\n",
      "Epoch: 7775 mean train loss:  4.62719087e-05, mean val. rec. loss:  4.80447464e-05\n",
      "Epoch: 7776 mean train loss:  4.58628955e-05, mean val. rec. loss:  4.76681786e-05\n",
      "Epoch: 7777 mean train loss:  4.56060961e-05, mean val. rec. loss:  4.81509581e-05\n",
      "Epoch: 7778 mean train loss:  4.57541030e-05, mean val. rec. loss:  4.79272135e-05\n",
      "Epoch: 7779 mean train loss:  4.56578987e-05, mean val. rec. loss:  4.87962283e-05\n",
      "Epoch: 7780 mean train loss:  4.55966828e-05, mean val. rec. loss:  4.81895466e-05\n",
      "Epoch: 7781 mean train loss:  4.57617856e-05, mean val. rec. loss:  4.77152354e-05\n",
      "Epoch: 7782 mean train loss:  4.59557207e-05, mean val. rec. loss:  4.77872017e-05\n",
      "Epoch: 7783 mean train loss:  4.57858524e-05, mean val. rec. loss:  4.90408754e-05\n",
      "Epoch: 7784 mean train loss:  4.61642394e-05, mean val. rec. loss:  4.93023090e-05\n",
      "Epoch: 7785 mean train loss:  4.62267390e-05, mean val. rec. loss:  4.94036233e-05\n",
      "Epoch: 7786 mean train loss:  4.66420816e-05, mean val. rec. loss:  4.94605703e-05\n",
      "Epoch: 7787 mean train loss:  4.59934174e-05, mean val. rec. loss:  4.79881447e-05\n",
      "Epoch: 7788 mean train loss:  4.54300738e-05, mean val. rec. loss:  4.78700485e-05\n",
      "Epoch: 7789 mean train loss:  4.56348306e-05, mean val. rec. loss:  4.75147421e-05\n",
      "Epoch: 7790 mean train loss:  4.57432933e-05, mean val. rec. loss:  4.89801532e-05\n",
      "Epoch: 7791 mean train loss:  4.62286622e-05, mean val. rec. loss:  4.78237504e-05\n",
      "Epoch: 7792 mean train loss:  4.54297691e-05, mean val. rec. loss:  4.80286641e-05\n",
      "Epoch: 7793 mean train loss:  4.56079200e-05, mean val. rec. loss:  4.87146581e-05\n",
      "Epoch: 7794 mean train loss:  4.57301092e-05, mean val. rec. loss:  4.79637123e-05\n",
      "Epoch: 7795 mean train loss:  4.61977477e-05, mean val. rec. loss:  4.76956186e-05\n",
      "Epoch: 7796 mean train loss:  4.63895339e-05, mean val. rec. loss:  4.97066393e-05\n",
      "Epoch: 7797 mean train loss:  4.68465751e-05, mean val. rec. loss:  4.83755477e-05\n",
      "Epoch: 7798 mean train loss:  4.56663554e-05, mean val. rec. loss:  4.73883276e-05\n",
      "Epoch: 7799 mean train loss:  4.53213581e-05, mean val. rec. loss:  4.79985301e-05\n",
      "Epoch: 7800 mean train loss:  4.60293623e-05, mean val. rec. loss:  4.86949141e-05\n",
      "Epoch: 7801 mean train loss:  4.57675583e-05, mean val. rec. loss:  4.77205326e-05\n",
      "Epoch: 7802 mean train loss:  4.52339611e-05, mean val. rec. loss:  4.78430946e-05\n",
      "Epoch: 7803 mean train loss:  4.56339490e-05, mean val. rec. loss:  4.75650335e-05\n",
      "Epoch: 7804 mean train loss:  4.60014072e-05, mean val. rec. loss:  4.82931279e-05\n",
      "Epoch: 7805 mean train loss:  4.59864384e-05, mean val. rec. loss:  4.86849148e-05\n",
      "Epoch: 7806 mean train loss:  4.60155289e-05, mean val. rec. loss:  4.84983369e-05\n",
      "Epoch: 7807 mean train loss:  4.61683062e-05, mean val. rec. loss:  4.92051244e-05\n",
      "Epoch: 7808 mean train loss:  4.57939986e-05, mean val. rec. loss:  4.77309952e-05\n",
      "Epoch: 7809 mean train loss:  4.60970159e-05, mean val. rec. loss:  4.95152593e-05\n",
      "Epoch: 7810 mean train loss:  4.61345608e-05, mean val. rec. loss:  5.00446912e-05\n",
      "Epoch: 7811 mean train loss:  4.59350966e-05, mean val. rec. loss:  4.73902084e-05\n",
      "Epoch: 7812 mean train loss:  4.58862067e-05, mean val. rec. loss:  4.69740753e-05\n",
      "Epoch: 7813 mean train loss:  4.59558338e-05, mean val. rec. loss:  4.95308647e-05\n",
      "Epoch: 7814 mean train loss:  4.57091368e-05, mean val. rec. loss:  4.71553970e-05\n",
      "Epoch: 7815 mean train loss:  4.55213613e-05, mean val. rec. loss:  4.65029351e-05\n",
      "Epoch: 7816 mean train loss:  4.46557007e-05, mean val. rec. loss:  4.88619933e-05\n",
      "Epoch: 7817 mean train loss:  4.53389691e-05, mean val. rec. loss:  4.68105895e-05\n",
      "Epoch: 7818 mean train loss:  4.58544252e-05, mean val. rec. loss:  4.87919034e-05\n",
      "Epoch: 7819 mean train loss:  4.54724609e-05, mean val. rec. loss:  4.68651151e-05\n",
      "Epoch: 7820 mean train loss:  4.48014413e-05, mean val. rec. loss:  4.73328298e-05\n",
      "Epoch: 7821 mean train loss:  4.49793489e-05, mean val. rec. loss:  4.90878777e-05\n",
      "Epoch: 7822 mean train loss:  4.57758114e-05, mean val. rec. loss:  4.76237977e-05\n",
      "Epoch: 7823 mean train loss:  4.52997347e-05, mean val. rec. loss:  4.64997640e-05\n",
      "Epoch: 7824 mean train loss:  4.53669085e-05, mean val. rec. loss:  4.74530795e-05\n",
      "Epoch: 7825 mean train loss:  4.52455093e-05, mean val. rec. loss:  4.68113891e-05\n",
      "Epoch: 7826 mean train loss:  4.57796939e-05, mean val. rec. loss:  5.05185981e-05\n",
      "Epoch: 7827 mean train loss:  4.60096334e-05, mean val. rec. loss:  4.77452649e-05\n",
      "Epoch: 7828 mean train loss:  4.58313832e-05, mean val. rec. loss:  5.07326342e-05\n",
      "Epoch: 7829 mean train loss:  4.69709735e-05, mean val. rec. loss:  4.65265271e-05\n",
      "Epoch: 7830 mean train loss:  4.55229764e-05, mean val. rec. loss:  4.81770624e-05\n",
      "Epoch: 7831 mean train loss:  4.51617125e-05, mean val. rec. loss:  4.68207478e-05\n",
      "Epoch: 7832 mean train loss:  4.50629835e-05, mean val. rec. loss:  4.67774981e-05\n",
      "Epoch: 7833 mean train loss:  4.49823044e-05, mean val. rec. loss:  4.80835667e-05\n",
      "Epoch: 7834 mean train loss:  4.48757726e-05, mean val. rec. loss:  4.67931307e-05\n",
      "Epoch: 7835 mean train loss:  4.44942703e-05, mean val. rec. loss:  4.66734716e-05\n",
      "Epoch: 7836 mean train loss:  4.45223169e-05, mean val. rec. loss:  4.68066008e-05\n",
      "Epoch: 7837 mean train loss:  4.43470574e-05, mean val. rec. loss:  4.70171569e-05\n",
      "Epoch: 7838 mean train loss:  4.45272845e-05, mean val. rec. loss:  4.81441799e-05\n",
      "Epoch: 7839 mean train loss:  4.51560361e-05, mean val. rec. loss:  4.69135257e-05\n",
      "Epoch: 7840 mean train loss:  4.45486489e-05, mean val. rec. loss:  4.67455969e-05\n",
      "Epoch: 7841 mean train loss:  4.45354984e-05, mean val. rec. loss:  4.73417160e-05\n",
      "Epoch: 7842 mean train loss:  4.47345982e-05, mean val. rec. loss:  4.60405266e-05\n",
      "Epoch: 7843 mean train loss:  4.44533228e-05, mean val. rec. loss:  4.67284787e-05\n",
      "Epoch: 7844 mean train loss:  4.52368915e-05, mean val. rec. loss:  4.75710530e-05\n",
      "Epoch: 7845 mean train loss:  4.49941671e-05, mean val. rec. loss:  4.69681966e-05\n",
      "Epoch: 7846 mean train loss:  4.51238529e-05, mean val. rec. loss:  4.82307974e-05\n",
      "Epoch: 7847 mean train loss:  4.58613103e-05, mean val. rec. loss:  4.84747767e-05\n",
      "Epoch: 7848 mean train loss:  4.55804409e-05, mean val. rec. loss:  4.80641225e-05\n",
      "Epoch: 7849 mean train loss:  4.47085217e-05, mean val. rec. loss:  4.77413397e-05\n",
      "Epoch: 7850 mean train loss:  4.51739692e-05, mean val. rec. loss:  4.67279835e-05\n",
      "Epoch: 7851 mean train loss:  4.44137314e-05, mean val. rec. loss:  4.68261086e-05\n",
      "Epoch: 7852 mean train loss:  4.45096371e-05, mean val. rec. loss:  4.77154944e-05\n",
      "Epoch: 7853 mean train loss:  4.48878351e-05, mean val. rec. loss:  4.65080869e-05\n",
      "Epoch: 7854 mean train loss:  4.43459347e-05, mean val. rec. loss:  4.67694842e-05\n",
      "Epoch: 7855 mean train loss:  4.52020729e-05, mean val. rec. loss:  4.73612102e-05\n",
      "Epoch: 7856 mean train loss:  4.50592974e-05, mean val. rec. loss:  4.82632165e-05\n",
      "Epoch: 7857 mean train loss:  4.48283593e-05, mean val. rec. loss:  4.70632642e-05\n",
      "Epoch: 7858 mean train loss:  4.46946315e-05, mean val. rec. loss:  4.61758909e-05\n",
      "Epoch: 7859 mean train loss:  4.46915743e-05, mean val. rec. loss:  4.75104035e-05\n",
      "Epoch: 7860 mean train loss:  4.52801161e-05, mean val. rec. loss:  4.74595260e-05\n",
      "Epoch: 7861 mean train loss:  4.55462611e-05, mean val. rec. loss:  4.75437903e-05\n",
      "Epoch: 7862 mean train loss:  4.46753529e-05, mean val. rec. loss:  4.74250398e-05\n",
      "Epoch: 7863 mean train loss:  4.46753364e-05, mean val. rec. loss:  4.81912412e-05\n",
      "Epoch: 7864 mean train loss:  4.50920481e-05, mean val. rec. loss:  4.67859118e-05\n",
      "Epoch: 7865 mean train loss:  4.46489597e-05, mean val. rec. loss:  4.76745934e-05\n",
      "Epoch: 7866 mean train loss:  4.52079812e-05, mean val. rec. loss:  4.70256978e-05\n",
      "Epoch: 7867 mean train loss:  4.43685361e-05, mean val. rec. loss:  4.65241011e-05\n",
      "Epoch: 7868 mean train loss:  4.42833195e-05, mean val. rec. loss:  4.70420437e-05\n",
      "Epoch: 7869 mean train loss:  4.48208606e-05, mean val. rec. loss:  4.61544660e-05\n",
      "Epoch: 7870 mean train loss:  4.42535507e-05, mean val. rec. loss:  4.67321858e-05\n",
      "Epoch: 7871 mean train loss:  4.41594158e-05, mean val. rec. loss:  4.71715838e-05\n",
      "Epoch: 7872 mean train loss:  4.44334506e-05, mean val. rec. loss:  4.67223320e-05\n",
      "Epoch: 7873 mean train loss:  4.43231824e-05, mean val. rec. loss:  4.62562073e-05\n",
      "Epoch: 7874 mean train loss:  4.43427485e-05, mean val. rec. loss:  4.68898201e-05\n",
      "Epoch: 7875 mean train loss:  4.43060894e-05, mean val. rec. loss:  4.80674025e-05\n",
      "Epoch: 7876 mean train loss:  4.51389690e-05, mean val. rec. loss:  4.61399555e-05\n",
      "Epoch: 7877 mean train loss:  4.49564650e-05, mean val. rec. loss:  4.72904251e-05\n",
      "Epoch: 7878 mean train loss:  4.48580268e-05, mean val. rec. loss:  4.69048121e-05\n",
      "Epoch: 7879 mean train loss:  4.40778978e-05, mean val. rec. loss:  4.68756913e-05\n",
      "Epoch: 7880 mean train loss:  4.45406039e-05, mean val. rec. loss:  4.67249170e-05\n",
      "Epoch: 7881 mean train loss:  4.44021817e-05, mean val. rec. loss:  4.69532591e-05\n",
      "Epoch: 7882 mean train loss:  4.45665163e-05, mean val. rec. loss:  4.59319707e-05\n",
      "Epoch: 7883 mean train loss:  4.46940896e-05, mean val. rec. loss:  4.81563370e-05\n",
      "Epoch: 7884 mean train loss:  4.46244173e-05, mean val. rec. loss:  4.63656354e-05\n",
      "Epoch: 7885 mean train loss:  4.44971453e-05, mean val. rec. loss:  4.68671594e-05\n",
      "Epoch: 7886 mean train loss:  4.40826852e-05, mean val. rec. loss:  4.59883226e-05\n",
      "Epoch: 7887 mean train loss:  4.40930082e-05, mean val. rec. loss:  4.71414907e-05\n",
      "Epoch: 7888 mean train loss:  4.44947030e-05, mean val. rec. loss:  4.65846825e-05\n",
      "Epoch: 7889 mean train loss:  4.40280021e-05, mean val. rec. loss:  4.72358269e-05\n",
      "Epoch: 7890 mean train loss:  4.39367069e-05, mean val. rec. loss:  4.58117665e-05\n",
      "Epoch: 7891 mean train loss:  4.40738498e-05, mean val. rec. loss:  4.64682490e-05\n",
      "Epoch: 7892 mean train loss:  4.41276644e-05, mean val. rec. loss:  4.69672698e-05\n",
      "Epoch: 7893 mean train loss:  4.41566113e-05, mean val. rec. loss:  4.56457821e-05\n",
      "Epoch: 7894 mean train loss:  4.44920068e-05, mean val. rec. loss:  4.65869540e-05\n",
      "Epoch: 7895 mean train loss:  4.45690564e-05, mean val. rec. loss:  4.71208971e-05\n",
      "Epoch: 7896 mean train loss:  4.42160309e-05, mean val. rec. loss:  4.72405244e-05\n",
      "Epoch: 7897 mean train loss:  4.39528490e-05, mean val. rec. loss:  4.65492286e-05\n",
      "Epoch: 7898 mean train loss:  4.45865034e-05, mean val. rec. loss:  4.82710896e-05\n",
      "Epoch: 7899 mean train loss:  4.48281082e-05, mean val. rec. loss:  4.63380955e-05\n",
      "Epoch: 7900 mean train loss:  4.44402150e-05, mean val. rec. loss:  4.63617193e-05\n",
      "Epoch: 7901 mean train loss:  4.43974767e-05, mean val. rec. loss:  4.61670184e-05\n",
      "Epoch: 7902 mean train loss:  4.38220749e-05, mean val. rec. loss:  4.64283657e-05\n",
      "Epoch: 7903 mean train loss:  4.38446663e-05, mean val. rec. loss:  4.60996588e-05\n",
      "Epoch: 7904 mean train loss:  4.38120151e-05, mean val. rec. loss:  4.56794233e-05\n",
      "Epoch: 7905 mean train loss:  4.38706216e-05, mean val. rec. loss:  4.51873988e-05\n",
      "Epoch: 7906 mean train loss:  4.37726289e-05, mean val. rec. loss:  4.59745481e-05\n",
      "Epoch: 7907 mean train loss:  4.34160626e-05, mean val. rec. loss:  4.72662562e-05\n",
      "Epoch: 7908 mean train loss:  4.39749906e-05, mean val. rec. loss:  4.51282530e-05\n",
      "Epoch: 7909 mean train loss:  4.37852763e-05, mean val. rec. loss:  4.59851470e-05\n",
      "Epoch: 7910 mean train loss:  4.40017730e-05, mean val. rec. loss:  4.52999298e-05\n",
      "Epoch: 7911 mean train loss:  4.39185060e-05, mean val. rec. loss:  4.76098142e-05\n",
      "Epoch: 7912 mean train loss:  4.40648101e-05, mean val. rec. loss:  4.63699877e-05\n",
      "Epoch: 7913 mean train loss:  4.39261670e-05, mean val. rec. loss:  4.68621258e-05\n",
      "Epoch: 7914 mean train loss:  4.38894352e-05, mean val. rec. loss:  4.49507838e-05\n",
      "Epoch: 7915 mean train loss:  4.36378550e-05, mean val. rec. loss:  4.73095240e-05\n",
      "Epoch: 7916 mean train loss:  4.41663467e-05, mean val. rec. loss:  4.57322043e-05\n",
      "Epoch: 7917 mean train loss:  4.33397283e-05, mean val. rec. loss:  4.55188179e-05\n",
      "Epoch: 7918 mean train loss:  4.36464745e-05, mean val. rec. loss:  4.62836700e-05\n",
      "Epoch: 7919 mean train loss:  4.35958727e-05, mean val. rec. loss:  4.54582365e-05\n",
      "Epoch: 7920 mean train loss:  4.36502558e-05, mean val. rec. loss:  4.53676483e-05\n",
      "Epoch: 7921 mean train loss:  4.40028734e-05, mean val. rec. loss:  4.77521158e-05\n",
      "Epoch: 7922 mean train loss:  4.45494725e-05, mean val. rec. loss:  4.73872872e-05\n",
      "Epoch: 7923 mean train loss:  4.52357577e-05, mean val. rec. loss:  4.54912507e-05\n",
      "Epoch: 7924 mean train loss:  4.41071148e-05, mean val. rec. loss:  4.72239151e-05\n",
      "Epoch: 7925 mean train loss:  4.41642313e-05, mean val. rec. loss:  4.71217331e-05\n",
      "Epoch: 7926 mean train loss:  4.41654512e-05, mean val. rec. loss:  4.76176601e-05\n",
      "Epoch: 7927 mean train loss:  4.41915410e-05, mean val. rec. loss:  4.49099283e-05\n",
      "Epoch: 7928 mean train loss:  4.36026674e-05, mean val. rec. loss:  4.67709697e-05\n",
      "Epoch: 7929 mean train loss:  4.36468407e-05, mean val. rec. loss:  4.49662074e-05\n",
      "Epoch: 7930 mean train loss:  4.39828662e-05, mean val. rec. loss:  4.63196463e-05\n",
      "Epoch: 7931 mean train loss:  4.40724978e-05, mean val. rec. loss:  4.53628918e-05\n",
      "Epoch: 7932 mean train loss:  4.39304759e-05, mean val. rec. loss:  4.63476087e-05\n",
      "Epoch: 7933 mean train loss:  4.37775585e-05, mean val. rec. loss:  4.63694379e-05\n",
      "Epoch: 7934 mean train loss:  4.32779448e-05, mean val. rec. loss:  4.59331565e-05\n",
      "Epoch: 7935 mean train loss:  4.35466536e-05, mean val. rec. loss:  4.51817473e-05\n",
      "Epoch: 7936 mean train loss:  4.38912674e-05, mean val. rec. loss:  4.82977618e-05\n",
      "Epoch: 7937 mean train loss:  4.39211496e-05, mean val. rec. loss:  4.49753207e-05\n",
      "Epoch: 7938 mean train loss:  4.33488897e-05, mean val. rec. loss:  4.53421437e-05\n",
      "Epoch: 7939 mean train loss:  4.37411644e-05, mean val. rec. loss:  4.67993501e-05\n",
      "Epoch: 7940 mean train loss:  4.40343670e-05, mean val. rec. loss:  4.66560945e-05\n",
      "Epoch: 7941 mean train loss:  4.37577420e-05, mean val. rec. loss:  4.58267858e-05\n",
      "Epoch: 7942 mean train loss:  4.40651879e-05, mean val. rec. loss:  4.59981083e-05\n",
      "Epoch: 7943 mean train loss:  4.34426508e-05, mean val. rec. loss:  4.48761372e-05\n",
      "Epoch: 7944 mean train loss:  4.33690530e-05, mean val. rec. loss:  4.57650687e-05\n",
      "Epoch: 7945 mean train loss:  4.34347206e-05, mean val. rec. loss:  4.68128611e-05\n",
      "Epoch: 7946 mean train loss:  4.35151797e-05, mean val. rec. loss:  4.50215870e-05\n",
      "Epoch: 7947 mean train loss:  4.30979579e-05, mean val. rec. loss:  4.62127804e-05\n",
      "Epoch: 7948 mean train loss:  4.34341716e-05, mean val. rec. loss:  4.59449320e-05\n",
      "Epoch: 7949 mean train loss:  4.31230410e-05, mean val. rec. loss:  4.59848971e-05\n",
      "Epoch: 7950 mean train loss:  4.38554282e-05, mean val. rec. loss:  4.67427711e-05\n",
      "Epoch: 7951 mean train loss:  4.45411290e-05, mean val. rec. loss:  4.57782843e-05\n",
      "Epoch: 7952 mean train loss:  4.38449665e-05, mean val. rec. loss:  4.59444505e-05\n",
      "Epoch: 7953 mean train loss:  4.30087471e-05, mean val. rec. loss:  4.51402830e-05\n",
      "Epoch: 7954 mean train loss:  4.38160909e-05, mean val. rec. loss:  4.51597453e-05\n",
      "Epoch: 7955 mean train loss:  4.31460911e-05, mean val. rec. loss:  4.49631091e-05\n",
      "Epoch: 7956 mean train loss:  4.31455401e-05, mean val. rec. loss:  4.61076091e-05\n",
      "Epoch: 7957 mean train loss:  4.34911580e-05, mean val. rec. loss:  4.48810164e-05\n",
      "Epoch: 7958 mean train loss:  4.28972156e-05, mean val. rec. loss:  4.52939603e-05\n",
      "Epoch: 7959 mean train loss:  4.32178205e-05, mean val. rec. loss:  4.55571566e-05\n",
      "Epoch: 7960 mean train loss:  4.30507721e-05, mean val. rec. loss:  4.57847900e-05\n",
      "Epoch: 7961 mean train loss:  4.30644454e-05, mean val. rec. loss:  4.56455550e-05\n",
      "Epoch: 7962 mean train loss:  4.35395967e-05, mean val. rec. loss:  4.48543397e-05\n",
      "Epoch: 7963 mean train loss:  4.32753197e-05, mean val. rec. loss:  4.49499979e-05\n",
      "Epoch: 7964 mean train loss:  4.34261519e-05, mean val. rec. loss:  4.60169573e-05\n",
      "Epoch: 7965 mean train loss:  4.38251767e-05, mean val. rec. loss:  4.50910501e-05\n",
      "Epoch: 7966 mean train loss:  4.29101998e-05, mean val. rec. loss:  4.47858579e-05\n",
      "Epoch: 7967 mean train loss:  4.31135178e-05, mean val. rec. loss:  4.57830455e-05\n",
      "Epoch: 7968 mean train loss:  4.32554447e-05, mean val. rec. loss:  4.64894741e-05\n",
      "Epoch: 7969 mean train loss:  4.34145041e-05, mean val. rec. loss:  4.49532416e-05\n",
      "Epoch: 7970 mean train loss:  4.33784316e-05, mean val. rec. loss:  4.58515999e-05\n",
      "Epoch: 7971 mean train loss:  4.39103359e-05, mean val. rec. loss:  4.53540556e-05\n",
      "Epoch: 7972 mean train loss:  4.43394814e-05, mean val. rec. loss:  4.65283261e-05\n",
      "Epoch: 7973 mean train loss:  4.35941860e-05, mean val. rec. loss:  4.64139461e-05\n",
      "Epoch: 7974 mean train loss:  4.40028680e-05, mean val. rec. loss:  4.56229579e-05\n",
      "Epoch: 7975 mean train loss:  4.42851719e-05, mean val. rec. loss:  4.52812716e-05\n",
      "Epoch: 7976 mean train loss:  4.33718029e-05, mean val. rec. loss:  4.62279133e-05\n",
      "Epoch: 7977 mean train loss:  4.34679210e-05, mean val. rec. loss:  4.43042052e-05\n",
      "Epoch: 7978 mean train loss:  4.28993682e-05, mean val. rec. loss:  4.47290972e-05\n",
      "Epoch: 7979 mean train loss:  4.28649741e-05, mean val. rec. loss:  4.40053369e-05\n",
      "Epoch: 7980 mean train loss:  4.35663319e-05, mean val. rec. loss:  4.61580186e-05\n",
      "Epoch: 7981 mean train loss:  4.31018754e-05, mean val. rec. loss:  4.41055927e-05\n",
      "Epoch: 7982 mean train loss:  4.27299292e-05, mean val. rec. loss:  4.51349721e-05\n",
      "Epoch: 7983 mean train loss:  4.25155135e-05, mean val. rec. loss:  4.42313530e-05\n",
      "Epoch: 7984 mean train loss:  4.29744943e-05, mean val. rec. loss:  4.65980027e-05\n",
      "Epoch: 7985 mean train loss:  4.29826968e-05, mean val. rec. loss:  4.49910078e-05\n",
      "Epoch: 7986 mean train loss:  4.30547202e-05, mean val. rec. loss:  4.42203816e-05\n",
      "Epoch: 7987 mean train loss:  4.28545132e-05, mean val. rec. loss:  4.49286365e-05\n",
      "Epoch: 7988 mean train loss:  4.26620882e-05, mean val. rec. loss:  4.50889784e-05\n",
      "Epoch: 7989 mean train loss:  4.26212959e-05, mean val. rec. loss:  4.47594992e-05\n",
      "Epoch: 7990 mean train loss:  4.23075670e-05, mean val. rec. loss:  4.48449356e-05\n",
      "Epoch: 7991 mean train loss:  4.27454256e-05, mean val. rec. loss:  4.59418655e-05\n",
      "Epoch: 7992 mean train loss:  4.27710032e-05, mean val. rec. loss:  4.47661003e-05\n",
      "Epoch: 7993 mean train loss:  4.26055825e-05, mean val. rec. loss:  4.41214297e-05\n",
      "Epoch: 7994 mean train loss:  4.29176227e-05, mean val. rec. loss:  4.50910591e-05\n",
      "Epoch: 7995 mean train loss:  4.24405426e-05, mean val. rec. loss:  4.51786852e-05\n",
      "Epoch: 7996 mean train loss:  4.24004519e-05, mean val. rec. loss:  4.53691339e-05\n",
      "Epoch: 7997 mean train loss:  4.29192145e-05, mean val. rec. loss:  4.52560168e-05\n",
      "Epoch: 7998 mean train loss:  4.29805277e-05, mean val. rec. loss:  4.56139536e-05\n",
      "Epoch: 7999 mean train loss:  4.26810748e-05, mean val. rec. loss:  4.44595134e-05\n",
      "Epoch: 8000 mean train loss:  4.25511275e-05, mean val. rec. loss:  4.56387404e-05\n",
      "Epoch: 8001 mean train loss:  4.26531201e-05, mean val. rec. loss:  4.47691759e-05\n",
      "Epoch: 8002 mean train loss:  4.22193466e-05, mean val. rec. loss:  4.42798545e-05\n",
      "Epoch: 8003 mean train loss:  4.22515137e-05, mean val. rec. loss:  4.40751725e-05\n",
      "Epoch: 8004 mean train loss:  4.21583086e-05, mean val. rec. loss:  4.38213258e-05\n",
      "Epoch: 8005 mean train loss:  4.22886821e-05, mean val. rec. loss:  4.46136859e-05\n",
      "Epoch: 8006 mean train loss:  4.23411188e-05, mean val. rec. loss:  4.53825722e-05\n",
      "Epoch: 8007 mean train loss:  4.24970556e-05, mean val. rec. loss:  4.36248621e-05\n",
      "Epoch: 8008 mean train loss:  4.25491048e-05, mean val. rec. loss:  4.63285733e-05\n",
      "Epoch: 8009 mean train loss:  4.26380870e-05, mean val. rec. loss:  4.43905229e-05\n",
      "Epoch: 8010 mean train loss:  4.24573834e-05, mean val. rec. loss:  4.48847780e-05\n",
      "Epoch: 8011 mean train loss:  4.26608069e-05, mean val. rec. loss:  4.43064040e-05\n",
      "Epoch: 8012 mean train loss:  4.22916549e-05, mean val. rec. loss:  4.45121127e-05\n",
      "Epoch: 8013 mean train loss:  4.23561962e-05, mean val. rec. loss:  4.52830343e-05\n",
      "Epoch: 8014 mean train loss:  4.28739115e-05, mean val. rec. loss:  4.47107888e-05\n",
      "Epoch: 8015 mean train loss:  4.23854046e-05, mean val. rec. loss:  4.50346619e-05\n",
      "Epoch: 8016 mean train loss:  4.24837916e-05, mean val. rec. loss:  4.42605647e-05\n",
      "Epoch: 8017 mean train loss:  4.21402678e-05, mean val. rec. loss:  4.49289045e-05\n",
      "Epoch: 8018 mean train loss:  4.24137688e-05, mean val. rec. loss:  4.38615907e-05\n",
      "Epoch: 8019 mean train loss:  4.21440833e-05, mean val. rec. loss:  4.53469457e-05\n",
      "Epoch: 8020 mean train loss:  4.25097501e-05, mean val. rec. loss:  4.49740214e-05\n",
      "Epoch: 8021 mean train loss:  4.26477880e-05, mean val. rec. loss:  4.45017182e-05\n",
      "Epoch: 8022 mean train loss:  4.22225716e-05, mean val. rec. loss:  4.46395176e-05\n",
      "Epoch: 8023 mean train loss:  4.27300133e-05, mean val. rec. loss:  4.46720231e-05\n",
      "Epoch: 8024 mean train loss:  4.22498044e-05, mean val. rec. loss:  4.42782735e-05\n",
      "Epoch: 8025 mean train loss:  4.21645248e-05, mean val. rec. loss:  4.42282183e-05\n",
      "Epoch: 8026 mean train loss:  4.23807034e-05, mean val. rec. loss:  4.44383247e-05\n",
      "Epoch: 8027 mean train loss:  4.20833249e-05, mean val. rec. loss:  4.37725790e-05\n",
      "Epoch: 8028 mean train loss:  4.19140019e-05, mean val. rec. loss:  4.47248904e-05\n",
      "Epoch: 8029 mean train loss:  4.22524504e-05, mean val. rec. loss:  4.35287905e-05\n",
      "Epoch: 8030 mean train loss:  4.22781676e-05, mean val. rec. loss:  4.52351688e-05\n",
      "Epoch: 8031 mean train loss:  4.21929785e-05, mean val. rec. loss:  4.47195432e-05\n",
      "Epoch: 8032 mean train loss:  4.25565971e-05, mean val. rec. loss:  4.41837329e-05\n",
      "Epoch: 8033 mean train loss:  4.22411221e-05, mean val. rec. loss:  4.42342333e-05\n",
      "Epoch: 8034 mean train loss:  4.22771745e-05, mean val. rec. loss:  4.41765367e-05\n",
      "Epoch: 8035 mean train loss:  4.22992471e-05, mean val. rec. loss:  4.49452458e-05\n",
      "Epoch: 8036 mean train loss:  4.24412226e-05, mean val. rec. loss:  4.45550171e-05\n",
      "Epoch: 8037 mean train loss:  4.22176830e-05, mean val. rec. loss:  4.42491299e-05\n",
      "Epoch: 8038 mean train loss:  4.21441563e-05, mean val. rec. loss:  4.48160419e-05\n",
      "Epoch: 8039 mean train loss:  4.26858915e-05, mean val. rec. loss:  4.47772443e-05\n",
      "Epoch: 8040 mean train loss:  4.22891708e-05, mean val. rec. loss:  4.57811010e-05\n",
      "Epoch: 8041 mean train loss:  4.26864672e-05, mean val. rec. loss:  4.44662280e-05\n",
      "Epoch: 8042 mean train loss:  4.21656830e-05, mean val. rec. loss:  4.50479276e-05\n",
      "Epoch: 8043 mean train loss:  4.23782847e-05, mean val. rec. loss:  4.46172159e-05\n",
      "Epoch: 8044 mean train loss:  4.20631085e-05, mean val. rec. loss:  4.36483178e-05\n",
      "Epoch: 8045 mean train loss:  4.25012626e-05, mean val. rec. loss:  4.46644589e-05\n",
      "Epoch: 8046 mean train loss:  4.24572706e-05, mean val. rec. loss:  4.44319463e-05\n",
      "Epoch: 8047 mean train loss:  4.21903207e-05, mean val. rec. loss:  4.42985309e-05\n",
      "Epoch: 8048 mean train loss:  4.18863272e-05, mean val. rec. loss:  4.40325134e-05\n",
      "Epoch: 8049 mean train loss:  4.23284643e-05, mean val. rec. loss:  4.54943582e-05\n",
      "Epoch: 8050 mean train loss:  4.25567828e-05, mean val. rec. loss:  4.54921457e-05\n",
      "Epoch: 8051 mean train loss:  4.22023673e-05, mean val. rec. loss:  4.48614722e-05\n",
      "Epoch: 8052 mean train loss:  4.22200906e-05, mean val. rec. loss:  4.36493536e-05\n",
      "Epoch: 8053 mean train loss:  4.19883138e-05, mean val. rec. loss:  4.34761912e-05\n",
      "Epoch: 8054 mean train loss:  4.18987603e-05, mean val. rec. loss:  4.43397044e-05\n",
      "Epoch: 8055 mean train loss:  4.17414991e-05, mean val. rec. loss:  4.35199679e-05\n",
      "Epoch: 8056 mean train loss:  4.23669334e-05, mean val. rec. loss:  4.46811364e-05\n",
      "Epoch: 8057 mean train loss:  4.21999816e-05, mean val. rec. loss:  4.40372018e-05\n",
      "Epoch: 8058 mean train loss:  4.18870012e-05, mean val. rec. loss:  4.41521406e-05\n",
      "Epoch: 8059 mean train loss:  4.17979687e-05, mean val. rec. loss:  4.47149593e-05\n",
      "Epoch: 8060 mean train loss:  4.20943761e-05, mean val. rec. loss:  4.43109834e-05\n",
      "Epoch: 8061 mean train loss:  4.21583459e-05, mean val. rec. loss:  4.48625171e-05\n",
      "Epoch: 8062 mean train loss:  4.19695998e-05, mean val. rec. loss:  4.36677438e-05\n",
      "Epoch: 8063 mean train loss:  4.20542979e-05, mean val. rec. loss:  4.51842823e-05\n",
      "Epoch: 8064 mean train loss:  4.21414733e-05, mean val. rec. loss:  4.38491110e-05\n",
      "Epoch: 8065 mean train loss:  4.18334672e-05, mean val. rec. loss:  4.45733846e-05\n",
      "Epoch: 8066 mean train loss:  4.19380099e-05, mean val. rec. loss:  4.36415442e-05\n",
      "Epoch: 8067 mean train loss:  4.18571162e-05, mean val. rec. loss:  4.32051082e-05\n",
      "Epoch: 8068 mean train loss:  4.24703460e-05, mean val. rec. loss:  4.45300668e-05\n",
      "Epoch: 8069 mean train loss:  4.22569208e-05, mean val. rec. loss:  4.52556443e-05\n",
      "Epoch: 8070 mean train loss:  4.22347044e-05, mean val. rec. loss:  4.39612195e-05\n",
      "Epoch: 8071 mean train loss:  4.17950072e-05, mean val. rec. loss:  4.45745704e-05\n",
      "Epoch: 8072 mean train loss:  4.18727141e-05, mean val. rec. loss:  4.41064922e-05\n",
      "Epoch: 8073 mean train loss:  4.19263405e-05, mean val. rec. loss:  4.47943534e-05\n",
      "Epoch: 8074 mean train loss:  4.15981283e-05, mean val. rec. loss:  4.35349327e-05\n",
      "Epoch: 8075 mean train loss:  4.20699882e-05, mean val. rec. loss:  4.38936872e-05\n",
      "Epoch: 8076 mean train loss:  4.16381604e-05, mean val. rec. loss:  4.28990893e-05\n",
      "Epoch: 8077 mean train loss:  4.18036669e-05, mean val. rec. loss:  4.46992495e-05\n",
      "Epoch: 8078 mean train loss:  4.21630133e-05, mean val. rec. loss:  4.45215622e-05\n",
      "Epoch: 8079 mean train loss:  4.18689942e-05, mean val. rec. loss:  4.31548713e-05\n",
      "Epoch: 8080 mean train loss:  4.14690206e-05, mean val. rec. loss:  4.33289741e-05\n",
      "Epoch: 8081 mean train loss:  4.15905620e-05, mean val. rec. loss:  4.32357328e-05\n",
      "Epoch: 8082 mean train loss:  4.15473741e-05, mean val. rec. loss:  4.34704125e-05\n",
      "Epoch: 8083 mean train loss:  4.15533117e-05, mean val. rec. loss:  4.46792056e-05\n",
      "Epoch: 8084 mean train loss:  4.17803392e-05, mean val. rec. loss:  4.32568625e-05\n",
      "Epoch: 8085 mean train loss:  4.18916291e-05, mean val. rec. loss:  4.42306125e-05\n",
      "Epoch: 8086 mean train loss:  4.27298769e-05, mean val. rec. loss:  4.52363000e-05\n",
      "Epoch: 8087 mean train loss:  4.19242422e-05, mean val. rec. loss:  4.85219425e-05\n",
      "Epoch: 8088 mean train loss:  4.34569942e-05, mean val. rec. loss:  4.45212442e-05\n",
      "Epoch: 8089 mean train loss:  4.21143434e-05, mean val. rec. loss:  4.39055082e-05\n",
      "Epoch: 8090 mean train loss:  4.15745263e-05, mean val. rec. loss:  4.36809732e-05\n",
      "Epoch: 8091 mean train loss:  4.14703461e-05, mean val. rec. loss:  4.31324015e-05\n",
      "Epoch: 8092 mean train loss:  4.10588510e-05, mean val. rec. loss:  4.39277691e-05\n",
      "Epoch: 8093 mean train loss:  4.14220833e-05, mean val. rec. loss:  4.31066879e-05\n",
      "Epoch: 8094 mean train loss:  4.13300734e-05, mean val. rec. loss:  4.38470893e-05\n",
      "Epoch: 8095 mean train loss:  4.21238218e-05, mean val. rec. loss:  4.33182753e-05\n",
      "Epoch: 8096 mean train loss:  4.24393639e-05, mean val. rec. loss:  4.37869077e-05\n",
      "Epoch: 8097 mean train loss:  4.22310956e-05, mean val. rec. loss:  4.56892045e-05\n",
      "Epoch: 8098 mean train loss:  4.25744205e-05, mean val. rec. loss:  4.36688115e-05\n",
      "Epoch: 8099 mean train loss:  4.21970523e-05, mean val. rec. loss:  4.51407645e-05\n",
      "Epoch: 8100 mean train loss:  4.18068606e-05, mean val. rec. loss:  4.29586894e-05\n",
      "Epoch: 8101 mean train loss:  4.19489248e-05, mean val. rec. loss:  4.54340130e-05\n",
      "Epoch: 8102 mean train loss:  4.14602030e-05, mean val. rec. loss:  4.33060545e-05\n",
      "Epoch: 8103 mean train loss:  4.15274689e-05, mean val. rec. loss:  4.41187856e-05\n",
      "Epoch: 8104 mean train loss:  4.19027805e-05, mean val. rec. loss:  4.37116205e-05\n",
      "Epoch: 8105 mean train loss:  4.23648484e-05, mean val. rec. loss:  4.46465866e-05\n",
      "Epoch: 8106 mean train loss:  4.26327063e-05, mean val. rec. loss:  4.47084219e-05\n",
      "Epoch: 8107 mean train loss:  4.21835254e-05, mean val. rec. loss:  4.35326794e-05\n",
      "Epoch: 8108 mean train loss:  4.21832556e-05, mean val. rec. loss:  4.32484670e-05\n",
      "Epoch: 8109 mean train loss:  4.12748150e-05, mean val. rec. loss:  4.43093842e-05\n",
      "Epoch: 8110 mean train loss:  4.13208154e-05, mean val. rec. loss:  4.35821439e-05\n",
      "Epoch: 8111 mean train loss:  4.11497229e-05, mean val. rec. loss:  4.20894292e-05\n",
      "Epoch: 8112 mean train loss:  4.12506628e-05, mean val. rec. loss:  4.25908215e-05\n",
      "Epoch: 8113 mean train loss:  4.10410638e-05, mean val. rec. loss:  4.49648672e-05\n",
      "Epoch: 8114 mean train loss:  4.19135377e-05, mean val. rec. loss:  4.42713953e-05\n",
      "Epoch: 8115 mean train loss:  4.18493028e-05, mean val. rec. loss:  4.33913500e-05\n",
      "Epoch: 8116 mean train loss:  4.19155783e-05, mean val. rec. loss:  4.33803650e-05\n",
      "Epoch: 8117 mean train loss:  4.11069569e-05, mean val. rec. loss:  4.40056322e-05\n",
      "Epoch: 8118 mean train loss:  4.09942637e-05, mean val. rec. loss:  4.24898661e-05\n",
      "Epoch: 8119 mean train loss:  4.09161934e-05, mean val. rec. loss:  4.26616156e-05\n",
      "Epoch: 8120 mean train loss:  4.08994456e-05, mean val. rec. loss:  4.28194044e-05\n",
      "Epoch: 8121 mean train loss:  4.07253472e-05, mean val. rec. loss:  4.21763284e-05\n",
      "Epoch: 8122 mean train loss:  4.11816924e-05, mean val. rec. loss:  4.34729566e-05\n",
      "Epoch: 8123 mean train loss:  4.11963772e-05, mean val. rec. loss:  4.22199688e-05\n",
      "Epoch: 8124 mean train loss:  4.06912982e-05, mean val. rec. loss:  4.30745731e-05\n",
      "Epoch: 8125 mean train loss:  4.15127156e-05, mean val. rec. loss:  4.41468298e-05\n",
      "Epoch: 8126 mean train loss:  4.17776976e-05, mean val. rec. loss:  4.36407219e-05\n",
      "Epoch: 8127 mean train loss:  4.12162019e-05, mean val. rec. loss:  4.32205909e-05\n",
      "Epoch: 8128 mean train loss:  4.15344932e-05, mean val. rec. loss:  4.34998332e-05\n",
      "Epoch: 8129 mean train loss:  4.17491200e-05, mean val. rec. loss:  4.35000513e-05\n",
      "Epoch: 8130 mean train loss:  4.14289870e-05, mean val. rec. loss:  4.46742946e-05\n",
      "Epoch: 8131 mean train loss:  4.15167724e-05, mean val. rec. loss:  4.28459857e-05\n",
      "Epoch: 8132 mean train loss:  4.08247528e-05, mean val. rec. loss:  4.28983760e-05\n",
      "Epoch: 8133 mean train loss:  4.15024790e-05, mean val. rec. loss:  4.24512912e-05\n",
      "Epoch: 8134 mean train loss:  4.14284843e-05, mean val. rec. loss:  4.26647776e-05\n",
      "Epoch: 8135 mean train loss:  4.10384930e-05, mean val. rec. loss:  4.42390126e-05\n",
      "Epoch: 8136 mean train loss:  4.11215320e-05, mean val. rec. loss:  4.24887349e-05\n",
      "Epoch: 8137 mean train loss:  4.10035894e-05, mean val. rec. loss:  4.32758842e-05\n",
      "Epoch: 8138 mean train loss:  4.15646590e-05, mean val. rec. loss:  4.28207764e-05\n",
      "Epoch: 8139 mean train loss:  4.09327154e-05, mean val. rec. loss:  4.32826488e-05\n",
      "Epoch: 8140 mean train loss:  4.13551504e-05, mean val. rec. loss:  4.33826410e-05\n",
      "Epoch: 8141 mean train loss:  4.14564297e-05, mean val. rec. loss:  4.31268817e-05\n",
      "Epoch: 8142 mean train loss:  4.16340110e-05, mean val. rec. loss:  4.40986373e-05\n",
      "Epoch: 8143 mean train loss:  4.13973297e-05, mean val. rec. loss:  4.38026130e-05\n",
      "Epoch: 8144 mean train loss:  4.24455895e-05, mean val. rec. loss:  4.27467612e-05\n",
      "Epoch: 8145 mean train loss:  4.10080749e-05, mean val. rec. loss:  4.36413806e-05\n",
      "Epoch: 8146 mean train loss:  4.10182470e-05, mean val. rec. loss:  4.26514574e-05\n",
      "Epoch: 8147 mean train loss:  4.13347497e-05, mean val. rec. loss:  4.33654638e-05\n",
      "Epoch: 8148 mean train loss:  4.12594021e-05, mean val. rec. loss:  4.23980741e-05\n",
      "Epoch: 8149 mean train loss:  4.07328333e-05, mean val. rec. loss:  4.22941748e-05\n",
      "Epoch: 8150 mean train loss:  4.12118776e-05, mean val. rec. loss:  4.35884588e-05\n",
      "Epoch: 8151 mean train loss:  4.12760909e-05, mean val. rec. loss:  4.36920264e-05\n",
      "Epoch: 8152 mean train loss:  4.12637233e-05, mean val. rec. loss:  4.36639504e-05\n",
      "Epoch: 8153 mean train loss:  4.15202321e-05, mean val. rec. loss:  4.35276911e-05\n",
      "Epoch: 8154 mean train loss:  4.12414869e-05, mean val. rec. loss:  4.51424909e-05\n",
      "Epoch: 8155 mean train loss:  4.18807390e-05, mean val. rec. loss:  4.23706841e-05\n",
      "Epoch: 8156 mean train loss:  4.07049345e-05, mean val. rec. loss:  4.19778976e-05\n",
      "Epoch: 8157 mean train loss:  4.08678509e-05, mean val. rec. loss:  4.29234490e-05\n",
      "Epoch: 8158 mean train loss:  4.09801435e-05, mean val. rec. loss:  4.25913939e-05\n",
      "Epoch: 8159 mean train loss:  4.08513398e-05, mean val. rec. loss:  4.32273055e-05\n",
      "Epoch: 8160 mean train loss:  4.08926736e-05, mean val. rec. loss:  4.26142227e-05\n",
      "Epoch: 8161 mean train loss:  4.02958733e-05, mean val. rec. loss:  4.23100391e-05\n",
      "Epoch: 8162 mean train loss:  4.05540793e-05, mean val. rec. loss:  4.21725577e-05\n",
      "Epoch: 8163 mean train loss:  4.02037306e-05, mean val. rec. loss:  4.33313456e-05\n",
      "Epoch: 8164 mean train loss:  4.07519262e-05, mean val. rec. loss:  4.18260148e-05\n",
      "Epoch: 8165 mean train loss:  4.09622855e-05, mean val. rec. loss:  4.26709743e-05\n",
      "Epoch: 8166 mean train loss:  4.05526585e-05, mean val. rec. loss:  4.29873241e-05\n",
      "Epoch: 8167 mean train loss:  4.07164372e-05, mean val. rec. loss:  4.21249739e-05\n",
      "Epoch: 8168 mean train loss:  4.04702125e-05, mean val. rec. loss:  4.26446338e-05\n",
      "Epoch: 8169 mean train loss:  4.06454435e-05, mean val. rec. loss:  4.17366079e-05\n",
      "Epoch: 8170 mean train loss:  4.04120524e-05, mean val. rec. loss:  4.45937147e-05\n",
      "Epoch: 8171 mean train loss:  4.07205097e-05, mean val. rec. loss:  4.20672138e-05\n",
      "Epoch: 8172 mean train loss:  4.01454684e-05, mean val. rec. loss:  4.24062969e-05\n",
      "Epoch: 8173 mean train loss:  4.02091335e-05, mean val. rec. loss:  4.17914469e-05\n",
      "Epoch: 8174 mean train loss:  4.05867524e-05, mean val. rec. loss:  4.23137462e-05\n",
      "Epoch: 8175 mean train loss:  4.02537876e-05, mean val. rec. loss:  4.28474167e-05\n",
      "Epoch: 8176 mean train loss:  4.08259812e-05, mean val. rec. loss:  4.21236065e-05\n",
      "Epoch: 8177 mean train loss:  4.05329847e-05, mean val. rec. loss:  4.22476087e-05\n",
      "Epoch: 8178 mean train loss:  4.03959188e-05, mean val. rec. loss:  4.18135033e-05\n",
      "Epoch: 8179 mean train loss:  4.07672322e-05, mean val. rec. loss:  4.26986550e-05\n",
      "Epoch: 8180 mean train loss:  4.06523231e-05, mean val. rec. loss:  4.28099458e-05\n",
      "Epoch: 8181 mean train loss:  4.08330502e-05, mean val. rec. loss:  4.34462572e-05\n",
      "Epoch: 8182 mean train loss:  4.05726559e-05, mean val. rec. loss:  4.21413561e-05\n",
      "Epoch: 8183 mean train loss:  4.05607743e-05, mean val. rec. loss:  4.44631933e-05\n",
      "Epoch: 8184 mean train loss:  4.04497530e-05, mean val. rec. loss:  4.23350212e-05\n",
      "Epoch: 8185 mean train loss:  4.06448369e-05, mean val. rec. loss:  4.24382708e-05\n",
      "Epoch: 8186 mean train loss:  4.08198520e-05, mean val. rec. loss:  4.29569130e-05\n",
      "Epoch: 8187 mean train loss:  4.01823315e-05, mean val. rec. loss:  4.19593393e-05\n",
      "Epoch: 8188 mean train loss:  4.02747130e-05, mean val. rec. loss:  4.29303953e-05\n",
      "Epoch: 8189 mean train loss:  4.03004105e-05, mean val. rec. loss:  4.11298489e-05\n",
      "Epoch: 8190 mean train loss:  4.00934897e-05, mean val. rec. loss:  4.25985674e-05\n",
      "Epoch: 8191 mean train loss:  4.01373858e-05, mean val. rec. loss:  4.20520309e-05\n",
      "Epoch: 8192 mean train loss:  4.04012173e-05, mean val. rec. loss:  4.29333165e-05\n",
      "Epoch: 8193 mean train loss:  4.01800012e-05, mean val. rec. loss:  4.21447089e-05\n",
      "Epoch: 8194 mean train loss:  4.02479488e-05, mean val. rec. loss:  4.12786607e-05\n",
      "Epoch: 8195 mean train loss:  4.02195489e-05, mean val. rec. loss:  4.25055668e-05\n",
      "Epoch: 8196 mean train loss:  4.03902871e-05, mean val. rec. loss:  4.25389445e-05\n",
      "Epoch: 8197 mean train loss:  4.02707199e-05, mean val. rec. loss:  4.16497995e-05\n",
      "Epoch: 8198 mean train loss:  4.07841331e-05, mean val. rec. loss:  4.18991259e-05\n",
      "Epoch: 8199 mean train loss:  4.06152437e-05, mean val. rec. loss:  4.33500038e-05\n",
      "Epoch: 8200 mean train loss:  4.07360250e-05, mean val. rec. loss:  4.41219022e-05\n",
      "Epoch: 8201 mean train loss:  4.10297157e-05, mean val. rec. loss:  4.23093122e-05\n",
      "Epoch: 8202 mean train loss:  3.99474819e-05, mean val. rec. loss:  4.23056187e-05\n",
      "Epoch: 8203 mean train loss:  4.05861665e-05, mean val. rec. loss:  4.14980076e-05\n",
      "Epoch: 8204 mean train loss:  3.98332792e-05, mean val. rec. loss:  4.24516274e-05\n",
      "Epoch: 8205 mean train loss:  4.00470816e-05, mean val. rec. loss:  4.35075018e-05\n",
      "Epoch: 8206 mean train loss:  4.04361415e-05, mean val. rec. loss:  4.20410186e-05\n",
      "Epoch: 8207 mean train loss:  3.99026060e-05, mean val. rec. loss:  4.17517316e-05\n",
      "Epoch: 8208 mean train loss:  3.96152183e-05, mean val. rec. loss:  4.31508825e-05\n",
      "Epoch: 8209 mean train loss:  4.02986988e-05, mean val. rec. loss:  4.13870530e-05\n",
      "Epoch: 8210 mean train loss:  3.98554350e-05, mean val. rec. loss:  4.20704847e-05\n",
      "Epoch: 8211 mean train loss:  4.01643834e-05, mean val. rec. loss:  4.16598124e-05\n",
      "Epoch: 8212 mean train loss:  4.02964964e-05, mean val. rec. loss:  4.32425201e-05\n",
      "Epoch: 8213 mean train loss:  3.99152951e-05, mean val. rec. loss:  4.18103050e-05\n",
      "Epoch: 8214 mean train loss:  4.02464435e-05, mean val. rec. loss:  4.14758694e-05\n",
      "Epoch: 8215 mean train loss:  3.98620191e-05, mean val. rec. loss:  4.16530478e-05\n",
      "Epoch: 8216 mean train loss:  3.97387184e-05, mean val. rec. loss:  4.16228866e-05\n",
      "Epoch: 8217 mean train loss:  3.98292045e-05, mean val. rec. loss:  4.19225680e-05\n",
      "Epoch: 8218 mean train loss:  3.98903238e-05, mean val. rec. loss:  4.25493890e-05\n",
      "Epoch: 8219 mean train loss:  4.05400708e-05, mean val. rec. loss:  4.24453807e-05\n",
      "Epoch: 8220 mean train loss:  4.04466398e-05, mean val. rec. loss:  4.10015263e-05\n",
      "Epoch: 8221 mean train loss:  3.97653756e-05, mean val. rec. loss:  4.10911604e-05\n",
      "Epoch: 8222 mean train loss:  3.94487464e-05, mean val. rec. loss:  4.08328615e-05\n",
      "Epoch: 8223 mean train loss:  3.96622201e-05, mean val. rec. loss:  4.20963255e-05\n",
      "Epoch: 8224 mean train loss:  3.95707693e-05, mean val. rec. loss:  4.15104509e-05\n",
      "Epoch: 8225 mean train loss:  3.94882183e-05, mean val. rec. loss:  4.19120054e-05\n",
      "Epoch: 8226 mean train loss:  4.01737525e-05, mean val. rec. loss:  4.25714363e-05\n",
      "Epoch: 8227 mean train loss:  4.01935159e-05, mean val. rec. loss:  4.21575748e-05\n",
      "Epoch: 8228 mean train loss:  4.01291569e-05, mean val. rec. loss:  4.23114838e-05\n",
      "Epoch: 8229 mean train loss:  4.03103796e-05, mean val. rec. loss:  4.12670850e-05\n",
      "Epoch: 8230 mean train loss:  4.08602258e-05, mean val. rec. loss:  4.21055842e-05\n",
      "Epoch: 8231 mean train loss:  4.00407932e-05, mean val. rec. loss:  4.11004555e-05\n",
      "Epoch: 8232 mean train loss:  3.97636247e-05, mean val. rec. loss:  4.19495037e-05\n",
      "Epoch: 8233 mean train loss:  3.93182774e-05, mean val. rec. loss:  4.10913422e-05\n",
      "Epoch: 8234 mean train loss:  3.95971784e-05, mean val. rec. loss:  4.30651645e-05\n",
      "Epoch: 8235 mean train loss:  4.00070914e-05, mean val. rec. loss:  4.19843942e-05\n",
      "Epoch: 8236 mean train loss:  3.95151390e-05, mean val. rec. loss:  4.20604810e-05\n",
      "Epoch: 8237 mean train loss:  4.00941041e-05, mean val. rec. loss:  4.24400971e-05\n",
      "Epoch: 8238 mean train loss:  4.00230870e-05, mean val. rec. loss:  4.25620595e-05\n",
      "Epoch: 8239 mean train loss:  4.03998403e-05, mean val. rec. loss:  4.21585470e-05\n",
      "Epoch: 8240 mean train loss:  3.97414282e-05, mean val. rec. loss:  4.15538506e-05\n",
      "Epoch: 8241 mean train loss:  3.97789054e-05, mean val. rec. loss:  4.14212393e-05\n",
      "Epoch: 8242 mean train loss:  3.95200548e-05, mean val. rec. loss:  4.10423955e-05\n",
      "Epoch: 8243 mean train loss:  3.94357656e-05, mean val. rec. loss:  4.17445309e-05\n",
      "Epoch: 8244 mean train loss:  3.94079942e-05, mean val. rec. loss:  4.14972489e-05\n",
      "Epoch: 8245 mean train loss:  3.96234533e-05, mean val. rec. loss:  4.15989811e-05\n",
      "Epoch: 8246 mean train loss:  3.94780137e-05, mean val. rec. loss:  4.19546373e-05\n",
      "Epoch: 8247 mean train loss:  3.98120563e-05, mean val. rec. loss:  4.14529543e-05\n",
      "Epoch: 8248 mean train loss:  3.93933387e-05, mean val. rec. loss:  4.09619201e-05\n",
      "Epoch: 8249 mean train loss:  3.97799422e-05, mean val. rec. loss:  4.12910722e-05\n",
      "Epoch: 8250 mean train loss:  3.93660890e-05, mean val. rec. loss:  4.13872665e-05\n",
      "Epoch: 8251 mean train loss:  3.90371546e-05, mean val. rec. loss:  4.08906535e-05\n",
      "Epoch: 8252 mean train loss:  3.90747812e-05, mean val. rec. loss:  4.17725887e-05\n",
      "Epoch: 8253 mean train loss:  3.94710328e-05, mean val. rec. loss:  4.24140746e-05\n",
      "Epoch: 8254 mean train loss:  4.00182661e-05, mean val. rec. loss:  4.31523545e-05\n",
      "Epoch: 8255 mean train loss:  4.12509758e-05, mean val. rec. loss:  4.20220878e-05\n",
      "Epoch: 8256 mean train loss:  4.02732810e-05, mean val. rec. loss:  4.35540771e-05\n",
      "Epoch: 8257 mean train loss:  4.02156226e-05, mean val. rec. loss:  4.18083697e-05\n",
      "Epoch: 8258 mean train loss:  3.98681119e-05, mean val. rec. loss:  4.13460021e-05\n",
      "Epoch: 8259 mean train loss:  3.96147490e-05, mean val. rec. loss:  4.06604487e-05\n",
      "Epoch: 8260 mean train loss:  3.93250568e-05, mean val. rec. loss:  4.22699013e-05\n",
      "Epoch: 8261 mean train loss:  3.94413268e-05, mean val. rec. loss:  4.23460745e-05\n",
      "Epoch: 8262 mean train loss:  4.03140540e-05, mean val. rec. loss:  4.15415344e-05\n",
      "Epoch: 8263 mean train loss:  3.94970700e-05, mean val. rec. loss:  4.18402255e-05\n",
      "Epoch: 8264 mean train loss:  3.98111750e-05, mean val. rec. loss:  4.15259745e-05\n",
      "Epoch: 8265 mean train loss:  3.94751373e-05, mean val. rec. loss:  4.09765532e-05\n",
      "Epoch: 8266 mean train loss:  3.93570165e-05, mean val. rec. loss:  4.12026965e-05\n",
      "Epoch: 8267 mean train loss:  3.91525293e-05, mean val. rec. loss:  4.04146295e-05\n",
      "Epoch: 8268 mean train loss:  3.90671961e-05, mean val. rec. loss:  4.19450151e-05\n",
      "Epoch: 8269 mean train loss:  3.93833727e-05, mean val. rec. loss:  4.35169695e-05\n",
      "Epoch: 8270 mean train loss:  4.03821777e-05, mean val. rec. loss:  4.09205603e-05\n",
      "Epoch: 8271 mean train loss:  3.95231069e-05, mean val. rec. loss:  4.04518052e-05\n",
      "Epoch: 8272 mean train loss:  3.91650208e-05, mean val. rec. loss:  4.18837977e-05\n",
      "Epoch: 8273 mean train loss:  3.93368129e-05, mean val. rec. loss:  4.05485356e-05\n",
      "Epoch: 8274 mean train loss:  3.96093154e-05, mean val. rec. loss:  4.16510897e-05\n",
      "Epoch: 8275 mean train loss:  3.97906962e-05, mean val. rec. loss:  4.19676031e-05\n",
      "Epoch: 8276 mean train loss:  3.95847897e-05, mean val. rec. loss:  4.18880091e-05\n",
      "Epoch: 8277 mean train loss:  3.93789595e-05, mean val. rec. loss:  4.13106845e-05\n",
      "Epoch: 8278 mean train loss:  3.99166183e-05, mean val. rec. loss:  4.11927791e-05\n",
      "Epoch: 8279 mean train loss:  3.97601462e-05, mean val. rec. loss:  4.12128593e-05\n",
      "Epoch: 8280 mean train loss:  3.96330388e-05, mean val. rec. loss:  4.23549334e-05\n",
      "Epoch: 8281 mean train loss:  3.96847623e-05, mean val. rec. loss:  4.05684250e-05\n",
      "Epoch: 8282 mean train loss:  3.93303189e-05, mean val. rec. loss:  4.31581696e-05\n",
      "Epoch: 8283 mean train loss:  4.02592469e-05, mean val. rec. loss:  4.20015487e-05\n",
      "Epoch: 8284 mean train loss:  4.01437391e-05, mean val. rec. loss:  4.42111592e-05\n",
      "Epoch: 8285 mean train loss:  3.99309236e-05, mean val. rec. loss:  4.10008630e-05\n",
      "Epoch: 8286 mean train loss:  4.01136868e-05, mean val. rec. loss:  4.15791689e-05\n",
      "Epoch: 8287 mean train loss:  3.97529469e-05, mean val. rec. loss:  4.17276490e-05\n",
      "Epoch: 8288 mean train loss:  3.94032189e-05, mean val. rec. loss:  4.08639813e-05\n",
      "Epoch: 8289 mean train loss:  3.90173806e-05, mean val. rec. loss:  4.03454209e-05\n",
      "Epoch: 8290 mean train loss:  3.90108430e-05, mean val. rec. loss:  4.09085758e-05\n",
      "Epoch: 8291 mean train loss:  3.91605409e-05, mean val. rec. loss:  4.06458202e-05\n",
      "Epoch: 8292 mean train loss:  3.94046996e-05, mean val. rec. loss:  4.18551675e-05\n",
      "Epoch: 8293 mean train loss:  3.92780187e-05, mean val. rec. loss:  4.14510916e-05\n",
      "Epoch: 8294 mean train loss:  3.98863046e-05, mean val. rec. loss:  4.21385667e-05\n",
      "Epoch: 8295 mean train loss:  3.93087751e-05, mean val. rec. loss:  4.09419489e-05\n",
      "Epoch: 8296 mean train loss:  3.90013827e-05, mean val. rec. loss:  4.02330353e-05\n",
      "Epoch: 8297 mean train loss:  3.89262227e-05, mean val. rec. loss:  4.07359994e-05\n",
      "Epoch: 8298 mean train loss:  3.87996477e-05, mean val. rec. loss:  4.10990108e-05\n",
      "Epoch: 8299 mean train loss:  3.91684467e-05, mean val. rec. loss:  4.04811442e-05\n",
      "Epoch: 8300 mean train loss:  3.90371866e-05, mean val. rec. loss:  4.08728584e-05\n",
      "Epoch: 8301 mean train loss:  3.93433537e-05, mean val. rec. loss:  4.11281589e-05\n",
      "Epoch: 8302 mean train loss:  3.91565641e-05, mean val. rec. loss:  4.13453070e-05\n",
      "Epoch: 8303 mean train loss:  3.88067087e-05, mean val. rec. loss:  4.08496208e-05\n",
      "Epoch: 8304 mean train loss:  3.96019646e-05, mean val. rec. loss:  4.03388880e-05\n",
      "Epoch: 8305 mean train loss:  3.94515196e-05, mean val. rec. loss:  4.40669087e-05\n",
      "Epoch: 8306 mean train loss:  3.97019698e-05, mean val. rec. loss:  4.03954988e-05\n",
      "Epoch: 8307 mean train loss:  3.88917078e-05, mean val. rec. loss:  4.08852700e-05\n",
      "Epoch: 8308 mean train loss:  3.95149803e-05, mean val. rec. loss:  3.99222370e-05\n",
      "Epoch: 8309 mean train loss:  3.94991029e-05, mean val. rec. loss:  4.05073847e-05\n",
      "Epoch: 8310 mean train loss:  3.85763974e-05, mean val. rec. loss:  4.26941256e-05\n",
      "Epoch: 8311 mean train loss:  3.92047595e-05, mean val. rec. loss:  4.06194387e-05\n",
      "Epoch: 8312 mean train loss:  3.85679219e-05, mean val. rec. loss:  4.02544920e-05\n",
      "Epoch: 8313 mean train loss:  3.87808176e-05, mean val. rec. loss:  4.06046830e-05\n",
      "Epoch: 8314 mean train loss:  3.92567504e-05, mean val. rec. loss:  4.05503755e-05\n",
      "Epoch: 8315 mean train loss:  3.91990689e-05, mean val. rec. loss:  4.17294571e-05\n",
      "Epoch: 8316 mean train loss:  3.95656024e-05, mean val. rec. loss:  4.17100220e-05\n",
      "Epoch: 8317 mean train loss:  3.92751800e-05, mean val. rec. loss:  4.07512050e-05\n",
      "Epoch: 8318 mean train loss:  3.84009528e-05, mean val. rec. loss:  3.99577908e-05\n",
      "Epoch: 8319 mean train loss:  3.84273690e-05, mean val. rec. loss:  4.21802036e-05\n",
      "Epoch: 8320 mean train loss:  3.90538975e-05, mean val. rec. loss:  4.03562606e-05\n",
      "Epoch: 8321 mean train loss:  3.93371711e-05, mean val. rec. loss:  4.00934823e-05\n",
      "Epoch: 8322 mean train loss:  3.92258698e-05, mean val. rec. loss:  4.05947701e-05\n",
      "Epoch: 8323 mean train loss:  3.90937906e-05, mean val. rec. loss:  4.14293623e-05\n",
      "Epoch: 8324 mean train loss:  3.88718273e-05, mean val. rec. loss:  3.99568368e-05\n",
      "Epoch: 8325 mean train loss:  3.87743537e-05, mean val. rec. loss:  4.06022570e-05\n",
      "Epoch: 8326 mean train loss:  3.86244958e-05, mean val. rec. loss:  4.10625757e-05\n",
      "Epoch: 8327 mean train loss:  3.88507006e-05, mean val. rec. loss:  3.98165296e-05\n",
      "Epoch: 8328 mean train loss:  3.85430804e-05, mean val. rec. loss:  3.97656476e-05\n",
      "Epoch: 8329 mean train loss:  3.84157256e-05, mean val. rec. loss:  4.27815473e-05\n",
      "Epoch: 8330 mean train loss:  3.92565429e-05, mean val. rec. loss:  4.06371838e-05\n",
      "Epoch: 8331 mean train loss:  3.90684095e-05, mean val. rec. loss:  4.03613306e-05\n",
      "Epoch: 8332 mean train loss:  3.93784596e-05, mean val. rec. loss:  4.02905547e-05\n",
      "Epoch: 8333 mean train loss:  3.93526395e-05, mean val. rec. loss:  4.16278203e-05\n",
      "Epoch: 8334 mean train loss:  3.93718535e-05, mean val. rec. loss:  4.07807892e-05\n",
      "Epoch: 8335 mean train loss:  3.89208127e-05, mean val. rec. loss:  4.09010661e-05\n",
      "Epoch: 8336 mean train loss:  3.87368268e-05, mean val. rec. loss:  3.98185922e-05\n",
      "Epoch: 8337 mean train loss:  3.82824266e-05, mean val. rec. loss:  4.12316311e-05\n",
      "Epoch: 8338 mean train loss:  3.87359927e-05, mean val. rec. loss:  4.01561217e-05\n",
      "Epoch: 8339 mean train loss:  3.85079543e-05, mean val. rec. loss:  4.15994627e-05\n",
      "Epoch: 8340 mean train loss:  3.92875588e-05, mean val. rec. loss:  4.02681802e-05\n",
      "Epoch: 8341 mean train loss:  3.83578120e-05, mean val. rec. loss:  4.02443611e-05\n",
      "Epoch: 8342 mean train loss:  3.84183150e-05, mean val. rec. loss:  3.99222234e-05\n",
      "Epoch: 8343 mean train loss:  3.84885491e-05, mean val. rec. loss:  4.01077429e-05\n",
      "Epoch: 8344 mean train loss:  3.90276894e-05, mean val. rec. loss:  4.05250617e-05\n",
      "Epoch: 8345 mean train loss:  3.88209580e-05, mean val. rec. loss:  4.07942003e-05\n",
      "Epoch: 8346 mean train loss:  3.87460502e-05, mean val. rec. loss:  4.03093765e-05\n",
      "Epoch: 8347 mean train loss:  3.85207855e-05, mean val. rec. loss:  4.05055448e-05\n",
      "Epoch: 8348 mean train loss:  3.84652574e-05, mean val. rec. loss:  4.07653247e-05\n",
      "Epoch: 8349 mean train loss:  3.81778051e-05, mean val. rec. loss:  3.94601057e-05\n",
      "Epoch: 8350 mean train loss:  3.83222748e-05, mean val. rec. loss:  3.98691017e-05\n",
      "Epoch: 8351 mean train loss:  3.82130292e-05, mean val. rec. loss:  4.20562105e-05\n",
      "Epoch: 8352 mean train loss:  3.86706352e-05, mean val. rec. loss:  4.03977885e-05\n",
      "Epoch: 8353 mean train loss:  3.82516020e-05, mean val. rec. loss:  4.01177466e-05\n",
      "Epoch: 8354 mean train loss:  3.82611856e-05, mean val. rec. loss:  3.96696351e-05\n",
      "Epoch: 8355 mean train loss:  3.86388298e-05, mean val. rec. loss:  3.97354455e-05\n",
      "Epoch: 8356 mean train loss:  3.88269754e-05, mean val. rec. loss:  3.99990325e-05\n",
      "Epoch: 8357 mean train loss:  3.81273386e-05, mean val. rec. loss:  3.99474781e-05\n",
      "Epoch: 8358 mean train loss:  3.83895866e-05, mean val. rec. loss:  4.00099313e-05\n",
      "Epoch: 8359 mean train loss:  3.79737253e-05, mean val. rec. loss:  4.08165339e-05\n",
      "Epoch: 8360 mean train loss:  3.90437202e-05, mean val. rec. loss:  3.94942057e-05\n",
      "Epoch: 8361 mean train loss:  3.96207258e-05, mean val. rec. loss:  4.19147949e-05\n",
      "Epoch: 8362 mean train loss:  3.90542140e-05, mean val. rec. loss:  3.99041194e-05\n",
      "Epoch: 8363 mean train loss:  3.85799839e-05, mean val. rec. loss:  4.13728696e-05\n",
      "Epoch: 8364 mean train loss:  3.85305746e-05, mean val. rec. loss:  3.97708540e-05\n",
      "Epoch: 8365 mean train loss:  3.87343410e-05, mean val. rec. loss:  3.97648117e-05\n",
      "Epoch: 8366 mean train loss:  3.80819745e-05, mean val. rec. loss:  4.01269008e-05\n",
      "Epoch: 8367 mean train loss:  3.83472495e-05, mean val. rec. loss:  4.02629557e-05\n",
      "Epoch: 8368 mean train loss:  3.83798499e-05, mean val. rec. loss:  4.10162503e-05\n",
      "Epoch: 8369 mean train loss:  3.86455395e-05, mean val. rec. loss:  3.98628868e-05\n",
      "Epoch: 8370 mean train loss:  3.89011233e-05, mean val. rec. loss:  4.03385882e-05\n",
      "Epoch: 8371 mean train loss:  3.83173544e-05, mean val. rec. loss:  3.99966883e-05\n",
      "Epoch: 8372 mean train loss:  3.80765475e-05, mean val. rec. loss:  4.03975114e-05\n",
      "Epoch: 8373 mean train loss:  3.80478292e-05, mean val. rec. loss:  3.91741897e-05\n",
      "Epoch: 8374 mean train loss:  3.80749518e-05, mean val. rec. loss:  3.99306598e-05\n",
      "Epoch: 8375 mean train loss:  3.81468878e-05, mean val. rec. loss:  4.02599482e-05\n",
      "Epoch: 8376 mean train loss:  3.81112000e-05, mean val. rec. loss:  4.06502860e-05\n",
      "Epoch: 8377 mean train loss:  3.93098955e-05, mean val. rec. loss:  4.01168516e-05\n",
      "Epoch: 8378 mean train loss:  3.89703875e-05, mean val. rec. loss:  4.11638854e-05\n",
      "Epoch: 8379 mean train loss:  3.87258010e-05, mean val. rec. loss:  4.00874082e-05\n",
      "Epoch: 8380 mean train loss:  3.82250409e-05, mean val. rec. loss:  3.96706209e-05\n",
      "Epoch: 8381 mean train loss:  3.89328338e-05, mean val. rec. loss:  3.93682228e-05\n",
      "Epoch: 8382 mean train loss:  3.84400891e-05, mean val. rec. loss:  4.04108361e-05\n",
      "Epoch: 8383 mean train loss:  3.79271458e-05, mean val. rec. loss:  4.02518525e-05\n",
      "Epoch: 8384 mean train loss:  3.84346240e-05, mean val. rec. loss:  4.00479292e-05\n",
      "Epoch: 8385 mean train loss:  3.84826390e-05, mean val. rec. loss:  3.97799491e-05\n",
      "Epoch: 8386 mean train loss:  3.85006217e-05, mean val. rec. loss:  4.08538776e-05\n",
      "Epoch: 8387 mean train loss:  3.82504581e-05, mean val. rec. loss:  4.02412945e-05\n",
      "Epoch: 8388 mean train loss:  3.79199429e-05, mean val. rec. loss:  3.95209688e-05\n",
      "Epoch: 8389 mean train loss:  3.80081550e-05, mean val. rec. loss:  3.93524403e-05\n",
      "Epoch: 8390 mean train loss:  3.77183243e-05, mean val. rec. loss:  3.95865702e-05\n",
      "Epoch: 8391 mean train loss:  3.77063314e-05, mean val. rec. loss:  4.05704557e-05\n",
      "Epoch: 8392 mean train loss:  3.80618644e-05, mean val. rec. loss:  4.02638189e-05\n",
      "Epoch: 8393 mean train loss:  3.77160350e-05, mean val. rec. loss:  3.96192119e-05\n",
      "Epoch: 8394 mean train loss:  3.79088233e-05, mean val. rec. loss:  4.03957214e-05\n",
      "Epoch: 8395 mean train loss:  3.82255644e-05, mean val. rec. loss:  4.01403982e-05\n",
      "Epoch: 8396 mean train loss:  3.82718842e-05, mean val. rec. loss:  4.06519487e-05\n",
      "Epoch: 8397 mean train loss:  3.79432751e-05, mean val. rec. loss:  3.93573240e-05\n",
      "Epoch: 8398 mean train loss:  3.76516417e-05, mean val. rec. loss:  3.91664938e-05\n",
      "Epoch: 8399 mean train loss:  3.76783703e-05, mean val. rec. loss:  4.14578153e-05\n",
      "Epoch: 8400 mean train loss:  3.82624296e-05, mean val. rec. loss:  3.93622078e-05\n",
      "Epoch: 8401 mean train loss:  3.82512234e-05, mean val. rec. loss:  3.98375367e-05\n",
      "Epoch: 8402 mean train loss:  3.88839769e-05, mean val. rec. loss:  3.99981421e-05\n",
      "Epoch: 8403 mean train loss:  3.83744064e-05, mean val. rec. loss:  4.09973740e-05\n",
      "Epoch: 8404 mean train loss:  3.84938663e-05, mean val. rec. loss:  4.18830209e-05\n",
      "Epoch: 8405 mean train loss:  3.87014978e-05, mean val. rec. loss:  3.96777671e-05\n",
      "Epoch: 8406 mean train loss:  3.81504796e-05, mean val. rec. loss:  3.91730357e-05\n",
      "Epoch: 8407 mean train loss:  3.85361885e-05, mean val. rec. loss:  4.01563034e-05\n",
      "Epoch: 8408 mean train loss:  3.80765539e-05, mean val. rec. loss:  4.13022708e-05\n",
      "Epoch: 8409 mean train loss:  3.77801808e-05, mean val. rec. loss:  3.89798885e-05\n",
      "Epoch: 8410 mean train loss:  3.78279532e-05, mean val. rec. loss:  3.96321369e-05\n",
      "Epoch: 8411 mean train loss:  3.80727346e-05, mean val. rec. loss:  3.90042437e-05\n",
      "Epoch: 8412 mean train loss:  3.77620072e-05, mean val. rec. loss:  3.97909524e-05\n",
      "Epoch: 8413 mean train loss:  3.76403219e-05, mean val. rec. loss:  3.85883196e-05\n",
      "Epoch: 8414 mean train loss:  3.77163760e-05, mean val. rec. loss:  3.95792196e-05\n",
      "Epoch: 8415 mean train loss:  3.74668269e-05, mean val. rec. loss:  3.88660900e-05\n",
      "Epoch: 8416 mean train loss:  3.74388417e-05, mean val. rec. loss:  3.94913482e-05\n",
      "Epoch: 8417 mean train loss:  3.75135434e-05, mean val. rec. loss:  3.89912279e-05\n",
      "Epoch: 8418 mean train loss:  3.75280332e-05, mean val. rec. loss:  3.98111371e-05\n",
      "Epoch: 8419 mean train loss:  3.76949577e-05, mean val. rec. loss:  3.90539991e-05\n",
      "Epoch: 8420 mean train loss:  3.75384040e-05, mean val. rec. loss:  3.90920697e-05\n",
      "Epoch: 8421 mean train loss:  3.76143753e-05, mean val. rec. loss:  3.95593256e-05\n",
      "Epoch: 8422 mean train loss:  3.77702308e-05, mean val. rec. loss:  3.89972111e-05\n",
      "Epoch: 8423 mean train loss:  3.75529102e-05, mean val. rec. loss:  4.05522699e-05\n",
      "Epoch: 8424 mean train loss:  3.76817613e-05, mean val. rec. loss:  3.94595741e-05\n",
      "Epoch: 8425 mean train loss:  3.79237867e-05, mean val. rec. loss:  3.93230241e-05\n",
      "Epoch: 8426 mean train loss:  3.75744910e-05, mean val. rec. loss:  3.90425597e-05\n",
      "Epoch: 8427 mean train loss:  3.78762290e-05, mean val. rec. loss:  4.17441857e-05\n",
      "Epoch: 8428 mean train loss:  3.78184492e-05, mean val. rec. loss:  3.90395340e-05\n",
      "Epoch: 8429 mean train loss:  3.77425003e-05, mean val. rec. loss:  3.93114621e-05\n",
      "Epoch: 8430 mean train loss:  3.76860433e-05, mean val. rec. loss:  3.87299124e-05\n",
      "Epoch: 8431 mean train loss:  3.78541175e-05, mean val. rec. loss:  3.90547487e-05\n",
      "Epoch: 8432 mean train loss:  3.80396562e-05, mean val. rec. loss:  4.01910485e-05\n",
      "Epoch: 8433 mean train loss:  3.76902056e-05, mean val. rec. loss:  3.83681913e-05\n",
      "Epoch: 8434 mean train loss:  3.73585973e-05, mean val. rec. loss:  3.86859949e-05\n",
      "Epoch: 8435 mean train loss:  3.72593512e-05, mean val. rec. loss:  3.88703741e-05\n",
      "Epoch: 8436 mean train loss:  3.71362102e-05, mean val. rec. loss:  3.97011729e-05\n",
      "Epoch: 8437 mean train loss:  3.80292397e-05, mean val. rec. loss:  4.02564683e-05\n",
      "Epoch: 8438 mean train loss:  3.82387851e-05, mean val. rec. loss:  3.95976961e-05\n",
      "Epoch: 8439 mean train loss:  3.87991139e-05, mean val. rec. loss:  4.20057328e-05\n",
      "Epoch: 8440 mean train loss:  3.89988191e-05, mean val. rec. loss:  3.93648427e-05\n",
      "Epoch: 8441 mean train loss:  3.80906981e-05, mean val. rec. loss:  4.05269380e-05\n",
      "Epoch: 8442 mean train loss:  3.78386162e-05, mean val. rec. loss:  3.84598425e-05\n",
      "Epoch: 8443 mean train loss:  3.75986248e-05, mean val. rec. loss:  3.92355252e-05\n",
      "Epoch: 8444 mean train loss:  3.77740692e-05, mean val. rec. loss:  3.95306999e-05\n",
      "Epoch: 8445 mean train loss:  3.81400810e-05, mean val. rec. loss:  3.94184688e-05\n",
      "Epoch: 8446 mean train loss:  3.74096563e-05, mean val. rec. loss:  4.03535711e-05\n",
      "Epoch: 8447 mean train loss:  3.80227363e-05, mean val. rec. loss:  3.98279963e-05\n",
      "Epoch: 8448 mean train loss:  3.77584525e-05, mean val. rec. loss:  3.90461260e-05\n",
      "Epoch: 8449 mean train loss:  3.73248957e-05, mean val. rec. loss:  3.95366877e-05\n",
      "Epoch: 8450 mean train loss:  3.82725116e-05, mean val. rec. loss:  3.93587460e-05\n",
      "Epoch: 8451 mean train loss:  3.75713710e-05, mean val. rec. loss:  4.01448822e-05\n",
      "Epoch: 8452 mean train loss:  3.79024504e-05, mean val. rec. loss:  3.89403232e-05\n",
      "Epoch: 8453 mean train loss:  3.69474991e-05, mean val. rec. loss:  3.91262606e-05\n",
      "Epoch: 8454 mean train loss:  3.73258668e-05, mean val. rec. loss:  3.98213089e-05\n",
      "Epoch: 8455 mean train loss:  3.75602588e-05, mean val. rec. loss:  3.95239762e-05\n",
      "Epoch: 8456 mean train loss:  3.76304082e-05, mean val. rec. loss:  3.92473098e-05\n",
      "Epoch: 8457 mean train loss:  3.74290142e-05, mean val. rec. loss:  3.78751900e-05\n",
      "Epoch: 8458 mean train loss:  3.75020426e-05, mean val. rec. loss:  3.93898794e-05\n",
      "Epoch: 8459 mean train loss:  3.73261628e-05, mean val. rec. loss:  3.87952504e-05\n",
      "Epoch: 8460 mean train loss:  3.71376177e-05, mean val. rec. loss:  3.89312190e-05\n",
      "Epoch: 8461 mean train loss:  3.69692112e-05, mean val. rec. loss:  3.84037542e-05\n",
      "Epoch: 8462 mean train loss:  3.68140940e-05, mean val. rec. loss:  3.96215925e-05\n",
      "Epoch: 8463 mean train loss:  3.74223227e-05, mean val. rec. loss:  3.90317836e-05\n",
      "Epoch: 8464 mean train loss:  3.71949264e-05, mean val. rec. loss:  3.89902921e-05\n",
      "Epoch: 8465 mean train loss:  3.71247156e-05, mean val. rec. loss:  3.87254194e-05\n",
      "Epoch: 8466 mean train loss:  3.71283345e-05, mean val. rec. loss:  3.88391589e-05\n",
      "Epoch: 8467 mean train loss:  3.78212841e-05, mean val. rec. loss:  3.92922496e-05\n",
      "Epoch: 8468 mean train loss:  3.73360505e-05, mean val. rec. loss:  4.20762271e-05\n",
      "Epoch: 8469 mean train loss:  3.79679307e-05, mean val. rec. loss:  3.85122147e-05\n",
      "Epoch: 8470 mean train loss:  3.71849456e-05, mean val. rec. loss:  3.96490006e-05\n",
      "Epoch: 8471 mean train loss:  3.71648161e-05, mean val. rec. loss:  3.84518740e-05\n",
      "Epoch: 8472 mean train loss:  3.72849505e-05, mean val. rec. loss:  4.09275657e-05\n",
      "Epoch: 8473 mean train loss:  3.76210179e-05, mean val. rec. loss:  3.84641084e-05\n",
      "Epoch: 8474 mean train loss:  3.71018246e-05, mean val. rec. loss:  3.86404828e-05\n",
      "Epoch: 8475 mean train loss:  3.71523767e-05, mean val. rec. loss:  3.85430164e-05\n",
      "Epoch: 8476 mean train loss:  3.68253850e-05, mean val. rec. loss:  3.99241360e-05\n",
      "Epoch: 8477 mean train loss:  3.72660319e-05, mean val. rec. loss:  3.83348636e-05\n",
      "Epoch: 8478 mean train loss:  3.73223980e-05, mean val. rec. loss:  3.90233018e-05\n",
      "Epoch: 8479 mean train loss:  3.72976525e-05, mean val. rec. loss:  3.83342503e-05\n",
      "Epoch: 8480 mean train loss:  3.67182107e-05, mean val. rec. loss:  3.92124375e-05\n",
      "Epoch: 8481 mean train loss:  3.68557314e-05, mean val. rec. loss:  3.82723241e-05\n",
      "Epoch: 8482 mean train loss:  3.68313984e-05, mean val. rec. loss:  3.81460686e-05\n",
      "Epoch: 8483 mean train loss:  3.70083495e-05, mean val. rec. loss:  3.95272427e-05\n",
      "Epoch: 8484 mean train loss:  3.70131518e-05, mean val. rec. loss:  3.84446415e-05\n",
      "Epoch: 8485 mean train loss:  3.67063170e-05, mean val. rec. loss:  3.85971604e-05\n",
      "Epoch: 8486 mean train loss:  3.68514826e-05, mean val. rec. loss:  3.83261500e-05\n",
      "Epoch: 8487 mean train loss:  3.67210066e-05, mean val. rec. loss:  3.89359573e-05\n",
      "Epoch: 8488 mean train loss:  3.66638338e-05, mean val. rec. loss:  3.89976018e-05\n",
      "Epoch: 8489 mean train loss:  3.72519137e-05, mean val. rec. loss:  3.92388462e-05\n",
      "Epoch: 8490 mean train loss:  3.70244052e-05, mean val. rec. loss:  3.86411551e-05\n",
      "Epoch: 8491 mean train loss:  3.70212362e-05, mean val. rec. loss:  3.88957060e-05\n",
      "Epoch: 8492 mean train loss:  3.68117151e-05, mean val. rec. loss:  3.86586731e-05\n",
      "Epoch: 8493 mean train loss:  3.68936405e-05, mean val. rec. loss:  3.92823957e-05\n",
      "Epoch: 8494 mean train loss:  3.68784201e-05, mean val. rec. loss:  3.93912741e-05\n",
      "Epoch: 8495 mean train loss:  3.72342561e-05, mean val. rec. loss:  3.81744626e-05\n",
      "Epoch: 8496 mean train loss:  3.73227201e-05, mean val. rec. loss:  3.90915473e-05\n",
      "Epoch: 8497 mean train loss:  3.68487047e-05, mean val. rec. loss:  3.80775142e-05\n",
      "Epoch: 8498 mean train loss:  3.67230720e-05, mean val. rec. loss:  3.83054747e-05\n",
      "Epoch: 8499 mean train loss:  3.68844583e-05, mean val. rec. loss:  3.85790791e-05\n",
      "Epoch: 8500 mean train loss:  3.70216348e-05, mean val. rec. loss:  3.91092197e-05\n",
      "Epoch: 8501 mean train loss:  3.70132606e-05, mean val. rec. loss:  3.76230310e-05\n",
      "Epoch: 8502 mean train loss:  3.67212548e-05, mean val. rec. loss:  3.88659446e-05\n",
      "Epoch: 8503 mean train loss:  3.72230276e-05, mean val. rec. loss:  3.80095503e-05\n",
      "Epoch: 8504 mean train loss:  3.65826671e-05, mean val. rec. loss:  3.91243298e-05\n",
      "Epoch: 8505 mean train loss:  3.68187031e-05, mean val. rec. loss:  3.81363510e-05\n",
      "Epoch: 8506 mean train loss:  3.66140050e-05, mean val. rec. loss:  3.83506234e-05\n",
      "Epoch: 8507 mean train loss:  3.65603822e-05, mean val. rec. loss:  3.75695481e-05\n",
      "Epoch: 8508 mean train loss:  3.65651116e-05, mean val. rec. loss:  4.10211341e-05\n",
      "Epoch: 8509 mean train loss:  3.75186821e-05, mean val. rec. loss:  3.88103424e-05\n",
      "Epoch: 8510 mean train loss:  3.67610090e-05, mean val. rec. loss:  3.83194263e-05\n",
      "Epoch: 8511 mean train loss:  3.70485576e-05, mean val. rec. loss:  3.93225153e-05\n",
      "Epoch: 8512 mean train loss:  3.65602948e-05, mean val. rec. loss:  3.83641753e-05\n",
      "Epoch: 8513 mean train loss:  3.65000305e-05, mean val. rec. loss:  3.78210325e-05\n",
      "Epoch: 8514 mean train loss:  3.66641036e-05, mean val. rec. loss:  3.86152598e-05\n",
      "Epoch: 8515 mean train loss:  3.66098468e-05, mean val. rec. loss:  3.82849356e-05\n",
      "Epoch: 8516 mean train loss:  3.64355934e-05, mean val. rec. loss:  3.81027894e-05\n",
      "Epoch: 8517 mean train loss:  3.70345110e-05, mean val. rec. loss:  3.89225690e-05\n",
      "Epoch: 8518 mean train loss:  3.68854728e-05, mean val. rec. loss:  3.92096299e-05\n",
      "Epoch: 8519 mean train loss:  3.69113404e-05, mean val. rec. loss:  4.01115636e-05\n",
      "Epoch: 8520 mean train loss:  3.71740979e-05, mean val. rec. loss:  3.74821628e-05\n",
      "Epoch: 8521 mean train loss:  3.68107374e-05, mean val. rec. loss:  3.95700108e-05\n",
      "Epoch: 8522 mean train loss:  3.68630889e-05, mean val. rec. loss:  3.75371381e-05\n",
      "Epoch: 8523 mean train loss:  3.69715149e-05, mean val. rec. loss:  3.87295354e-05\n",
      "Epoch: 8524 mean train loss:  3.65054563e-05, mean val. rec. loss:  3.81884370e-05\n",
      "Epoch: 8525 mean train loss:  3.67147310e-05, mean val. rec. loss:  3.79575144e-05\n",
      "Epoch: 8526 mean train loss:  3.64301147e-05, mean val. rec. loss:  3.77299128e-05\n",
      "Epoch: 8527 mean train loss:  3.61694593e-05, mean val. rec. loss:  3.86754778e-05\n",
      "Epoch: 8528 mean train loss:  3.63679629e-05, mean val. rec. loss:  3.76649519e-05\n",
      "Epoch: 8529 mean train loss:  3.65556767e-05, mean val. rec. loss:  3.82736234e-05\n",
      "Epoch: 8530 mean train loss:  3.66302524e-05, mean val. rec. loss:  3.87975719e-05\n",
      "Epoch: 8531 mean train loss:  3.67369291e-05, mean val. rec. loss:  3.89118429e-05\n",
      "Epoch: 8532 mean train loss:  3.67233784e-05, mean val. rec. loss:  3.84368638e-05\n",
      "Epoch: 8533 mean train loss:  3.69660899e-05, mean val. rec. loss:  3.96136467e-05\n",
      "Epoch: 8534 mean train loss:  3.68090625e-05, mean val. rec. loss:  3.86817381e-05\n",
      "Epoch: 8535 mean train loss:  3.68090824e-05, mean val. rec. loss:  3.85567273e-05\n",
      "Epoch: 8536 mean train loss:  3.67489429e-05, mean val. rec. loss:  3.76657378e-05\n",
      "Epoch: 8537 mean train loss:  3.60941316e-05, mean val. rec. loss:  3.92953843e-05\n",
      "Epoch: 8538 mean train loss:  3.64767821e-05, mean val. rec. loss:  3.96310147e-05\n",
      "Epoch: 8539 mean train loss:  3.68319898e-05, mean val. rec. loss:  3.84520830e-05\n",
      "Epoch: 8540 mean train loss:  3.65756632e-05, mean val. rec. loss:  3.93935457e-05\n",
      "Epoch: 8541 mean train loss:  3.66040276e-05, mean val. rec. loss:  3.89809743e-05\n",
      "Epoch: 8542 mean train loss:  3.67900863e-05, mean val. rec. loss:  3.82765219e-05\n",
      "Epoch: 8543 mean train loss:  3.65468315e-05, mean val. rec. loss:  3.83743925e-05\n",
      "Epoch: 8544 mean train loss:  3.65376704e-05, mean val. rec. loss:  3.87642215e-05\n",
      "Epoch: 8545 mean train loss:  3.63606633e-05, mean val. rec. loss:  3.75309119e-05\n",
      "Epoch: 8546 mean train loss:  3.62107780e-05, mean val. rec. loss:  4.07071966e-05\n",
      "Epoch: 8547 mean train loss:  3.67908635e-05, mean val. rec. loss:  3.76052382e-05\n",
      "Epoch: 8548 mean train loss:  3.59098559e-05, mean val. rec. loss:  3.69548888e-05\n",
      "Epoch: 8549 mean train loss:  3.60963786e-05, mean val. rec. loss:  3.83204940e-05\n",
      "Epoch: 8550 mean train loss:  3.63411491e-05, mean val. rec. loss:  3.82805652e-05\n",
      "Epoch: 8551 mean train loss:  3.65496103e-05, mean val. rec. loss:  3.80813803e-05\n",
      "Epoch: 8552 mean train loss:  3.68869612e-05, mean val. rec. loss:  3.80169487e-05\n",
      "Epoch: 8553 mean train loss:  3.73339795e-05, mean val. rec. loss:  3.90657519e-05\n",
      "Epoch: 8554 mean train loss:  3.70247227e-05, mean val. rec. loss:  3.79924821e-05\n",
      "Epoch: 8555 mean train loss:  3.75186068e-05, mean val. rec. loss:  3.99880293e-05\n",
      "Epoch: 8556 mean train loss:  3.69986531e-05, mean val. rec. loss:  3.79968934e-05\n",
      "Epoch: 8557 mean train loss:  3.66619052e-05, mean val. rec. loss:  3.98345700e-05\n",
      "Epoch: 8558 mean train loss:  3.68687617e-05, mean val. rec. loss:  3.69993220e-05\n",
      "Epoch: 8559 mean train loss:  3.63891603e-05, mean val. rec. loss:  3.80521458e-05\n",
      "Epoch: 8560 mean train loss:  3.59427711e-05, mean val. rec. loss:  3.77838068e-05\n",
      "Epoch: 8561 mean train loss:  3.65821782e-05, mean val. rec. loss:  3.97656476e-05\n",
      "Epoch: 8562 mean train loss:  3.61999427e-05, mean val. rec. loss:  3.74645267e-05\n",
      "Epoch: 8563 mean train loss:  3.63791440e-05, mean val. rec. loss:  3.96435944e-05\n",
      "Epoch: 8564 mean train loss:  3.62768155e-05, mean val. rec. loss:  3.71639208e-05\n",
      "Epoch: 8565 mean train loss:  3.64010055e-05, mean val. rec. loss:  3.87686282e-05\n",
      "Epoch: 8566 mean train loss:  3.58852808e-05, mean val. rec. loss:  3.71790059e-05\n",
      "Epoch: 8567 mean train loss:  3.60449580e-05, mean val. rec. loss:  3.79433038e-05\n",
      "Epoch: 8568 mean train loss:  3.62611299e-05, mean val. rec. loss:  3.73813823e-05\n",
      "Epoch: 8569 mean train loss:  3.62563042e-05, mean val. rec. loss:  3.85694615e-05\n",
      "Epoch: 8570 mean train loss:  3.61901826e-05, mean val. rec. loss:  3.72728628e-05\n",
      "Epoch: 8571 mean train loss:  3.62935643e-05, mean val. rec. loss:  3.85565365e-05\n",
      "Epoch: 8572 mean train loss:  3.60764180e-05, mean val. rec. loss:  3.71720687e-05\n",
      "Epoch: 8573 mean train loss:  3.60383580e-05, mean val. rec. loss:  3.86080500e-05\n",
      "Epoch: 8574 mean train loss:  3.66011682e-05, mean val. rec. loss:  3.72887634e-05\n",
      "Epoch: 8575 mean train loss:  3.66030377e-05, mean val. rec. loss:  4.08543774e-05\n",
      "Epoch: 8576 mean train loss:  3.68204887e-05, mean val. rec. loss:  3.73704086e-05\n",
      "Epoch: 8577 mean train loss:  3.56434022e-05, mean val. rec. loss:  3.76896047e-05\n",
      "Epoch: 8578 mean train loss:  3.60194180e-05, mean val. rec. loss:  3.74991946e-05\n",
      "Epoch: 8579 mean train loss:  3.63849499e-05, mean val. rec. loss:  3.85550010e-05\n",
      "Epoch: 8580 mean train loss:  3.64097342e-05, mean val. rec. loss:  3.71044524e-05\n",
      "Epoch: 8581 mean train loss:  3.57795532e-05, mean val. rec. loss:  3.73682893e-05\n",
      "Epoch: 8582 mean train loss:  3.58093325e-05, mean val. rec. loss:  3.76608109e-05\n",
      "Epoch: 8583 mean train loss:  3.59997384e-05, mean val. rec. loss:  3.68930490e-05\n",
      "Epoch: 8584 mean train loss:  3.54591095e-05, mean val. rec. loss:  3.75236680e-05\n",
      "Epoch: 8585 mean train loss:  3.55727092e-05, mean val. rec. loss:  3.71541374e-05\n",
      "Epoch: 8586 mean train loss:  3.56203909e-05, mean val. rec. loss:  3.80267094e-05\n",
      "Epoch: 8587 mean train loss:  3.59298245e-05, mean val. rec. loss:  3.76415462e-05\n",
      "Epoch: 8588 mean train loss:  3.61394525e-05, mean val. rec. loss:  3.81992539e-05\n",
      "Epoch: 8589 mean train loss:  3.61187099e-05, mean val. rec. loss:  3.84999212e-05\n",
      "Epoch: 8590 mean train loss:  3.63050443e-05, mean val. rec. loss:  3.81788875e-05\n",
      "Epoch: 8591 mean train loss:  3.62342475e-05, mean val. rec. loss:  3.75833908e-05\n",
      "Epoch: 8592 mean train loss:  3.65838947e-05, mean val. rec. loss:  3.91435787e-05\n",
      "Epoch: 8593 mean train loss:  3.67097819e-05, mean val. rec. loss:  3.76585553e-05\n",
      "Epoch: 8594 mean train loss:  3.59372472e-05, mean val. rec. loss:  3.86220380e-05\n",
      "Epoch: 8595 mean train loss:  3.61854237e-05, mean val. rec. loss:  3.71229676e-05\n",
      "Epoch: 8596 mean train loss:  3.59268007e-05, mean val. rec. loss:  3.82753453e-05\n",
      "Epoch: 8597 mean train loss:  3.57308953e-05, mean val. rec. loss:  3.68733686e-05\n",
      "Epoch: 8598 mean train loss:  3.60347498e-05, mean val. rec. loss:  3.77041242e-05\n",
      "Epoch: 8599 mean train loss:  3.56612903e-05, mean val. rec. loss:  3.77309304e-05\n",
      "Epoch: 8600 mean train loss:  3.60731524e-05, mean val. rec. loss:  3.80268684e-05\n",
      "Epoch: 8601 mean train loss:  3.58739457e-05, mean val. rec. loss:  3.75238406e-05\n",
      "Epoch: 8602 mean train loss:  3.61089406e-05, mean val. rec. loss:  3.83345819e-05\n",
      "Epoch: 8603 mean train loss:  3.58976453e-05, mean val. rec. loss:  3.68988709e-05\n",
      "Epoch: 8604 mean train loss:  3.55477506e-05, mean val. rec. loss:  3.72150095e-05\n",
      "Epoch: 8605 mean train loss:  3.57722971e-05, mean val. rec. loss:  3.73927149e-05\n",
      "Epoch: 8606 mean train loss:  3.57153925e-05, mean val. rec. loss:  3.82029701e-05\n",
      "Epoch: 8607 mean train loss:  3.56405619e-05, mean val. rec. loss:  3.69605222e-05\n",
      "Epoch: 8608 mean train loss:  3.53610315e-05, mean val. rec. loss:  3.76853524e-05\n",
      "Epoch: 8609 mean train loss:  3.57206852e-05, mean val. rec. loss:  3.82567233e-05\n",
      "Epoch: 8610 mean train loss:  3.59091277e-05, mean val. rec. loss:  3.76516181e-05\n",
      "Epoch: 8611 mean train loss:  3.57575048e-05, mean val. rec. loss:  3.73148200e-05\n",
      "Epoch: 8612 mean train loss:  3.57528547e-05, mean val. rec. loss:  3.81062625e-05\n",
      "Epoch: 8613 mean train loss:  3.55827749e-05, mean val. rec. loss:  3.73207169e-05\n",
      "Epoch: 8614 mean train loss:  3.57819024e-05, mean val. rec. loss:  3.70993983e-05\n",
      "Epoch: 8615 mean train loss:  3.55262571e-05, mean val. rec. loss:  3.69657172e-05\n",
      "Epoch: 8616 mean train loss:  3.58876762e-05, mean val. rec. loss:  3.96225965e-05\n",
      "Epoch: 8617 mean train loss:  3.68383135e-05, mean val. rec. loss:  3.73034738e-05\n",
      "Epoch: 8618 mean train loss:  3.57640090e-05, mean val. rec. loss:  3.79429744e-05\n",
      "Epoch: 8619 mean train loss:  3.63892950e-05, mean val. rec. loss:  3.78681188e-05\n",
      "Epoch: 8620 mean train loss:  3.57848086e-05, mean val. rec. loss:  3.65387898e-05\n",
      "Epoch: 8621 mean train loss:  3.57303043e-05, mean val. rec. loss:  3.63964519e-05\n",
      "Epoch: 8622 mean train loss:  3.56163617e-05, mean val. rec. loss:  3.76417029e-05\n",
      "Epoch: 8623 mean train loss:  3.55679201e-05, mean val. rec. loss:  3.77912869e-05\n",
      "Epoch: 8624 mean train loss:  3.53869183e-05, mean val. rec. loss:  3.71663172e-05\n",
      "Epoch: 8625 mean train loss:  3.55125064e-05, mean val. rec. loss:  3.79210179e-05\n",
      "Epoch: 8626 mean train loss:  3.55027585e-05, mean val. rec. loss:  3.60444847e-05\n",
      "Epoch: 8627 mean train loss:  3.54929863e-05, mean val. rec. loss:  3.81879872e-05\n",
      "Epoch: 8628 mean train loss:  3.55866466e-05, mean val. rec. loss:  3.71754987e-05\n",
      "Epoch: 8629 mean train loss:  3.61687240e-05, mean val. rec. loss:  3.84876823e-05\n",
      "Epoch: 8630 mean train loss:  3.61591664e-05, mean val. rec. loss:  3.69782037e-05\n",
      "Epoch: 8631 mean train loss:  3.58701391e-05, mean val. rec. loss:  3.85441249e-05\n",
      "Epoch: 8632 mean train loss:  3.58269651e-05, mean val. rec. loss:  3.65564736e-05\n",
      "Epoch: 8633 mean train loss:  3.53317456e-05, mean val. rec. loss:  3.69050903e-05\n",
      "Epoch: 8634 mean train loss:  3.52320979e-05, mean val. rec. loss:  3.71574174e-05\n",
      "Epoch: 8635 mean train loss:  3.50851357e-05, mean val. rec. loss:  3.66240944e-05\n",
      "Epoch: 8636 mean train loss:  3.50628280e-05, mean val. rec. loss:  3.66964877e-05\n",
      "Epoch: 8637 mean train loss:  3.53090122e-05, mean val. rec. loss:  3.61226999e-05\n",
      "Epoch: 8638 mean train loss:  3.54660461e-05, mean val. rec. loss:  3.84374090e-05\n",
      "Epoch: 8639 mean train loss:  3.54796540e-05, mean val. rec. loss:  3.74455346e-05\n",
      "Epoch: 8640 mean train loss:  3.54239407e-05, mean val. rec. loss:  3.73667901e-05\n",
      "Epoch: 8641 mean train loss:  3.54725807e-05, mean val. rec. loss:  3.73587398e-05\n",
      "Epoch: 8642 mean train loss:  3.57627087e-05, mean val. rec. loss:  3.71942955e-05\n",
      "Epoch: 8643 mean train loss:  3.54179917e-05, mean val. rec. loss:  3.74733266e-05\n",
      "Epoch: 8644 mean train loss:  3.59033881e-05, mean val. rec. loss:  3.68193110e-05\n",
      "Epoch: 8645 mean train loss:  3.53746678e-05, mean val. rec. loss:  3.68791110e-05\n",
      "Epoch: 8646 mean train loss:  3.57885020e-05, mean val. rec. loss:  3.67458296e-05\n",
      "Epoch: 8647 mean train loss:  3.53712662e-05, mean val. rec. loss:  3.62518357e-05\n",
      "Epoch: 8648 mean train loss:  3.54715962e-05, mean val. rec. loss:  3.73596644e-05\n",
      "Epoch: 8649 mean train loss:  3.52674449e-05, mean val. rec. loss:  3.59765140e-05\n",
      "Epoch: 8650 mean train loss:  3.53805610e-05, mean val. rec. loss:  3.96055874e-05\n",
      "Epoch: 8651 mean train loss:  3.55496249e-05, mean val. rec. loss:  3.66455035e-05\n",
      "Epoch: 8652 mean train loss:  3.56102575e-05, mean val. rec. loss:  3.75001487e-05\n",
      "Epoch: 8653 mean train loss:  3.54077818e-05, mean val. rec. loss:  3.60541614e-05\n",
      "Epoch: 8654 mean train loss:  3.53232322e-05, mean val. rec. loss:  3.67072275e-05\n",
      "Epoch: 8655 mean train loss:  3.51195327e-05, mean val. rec. loss:  3.60448004e-05\n",
      "Epoch: 8656 mean train loss:  3.51832914e-05, mean val. rec. loss:  3.65668544e-05\n",
      "Epoch: 8657 mean train loss:  3.51067850e-05, mean val. rec. loss:  3.71009929e-05\n",
      "Epoch: 8658 mean train loss:  3.54296150e-05, mean val. rec. loss:  3.69222380e-05\n",
      "Epoch: 8659 mean train loss:  3.51996862e-05, mean val. rec. loss:  3.69814520e-05\n",
      "Epoch: 8660 mean train loss:  3.52336174e-05, mean val. rec. loss:  3.60742098e-05\n",
      "Epoch: 8661 mean train loss:  3.50091195e-05, mean val. rec. loss:  3.66176865e-05\n",
      "Epoch: 8662 mean train loss:  3.49123006e-05, mean val. rec. loss:  3.60778419e-05\n",
      "Epoch: 8663 mean train loss:  3.49978562e-05, mean val. rec. loss:  3.67888885e-05\n",
      "Epoch: 8664 mean train loss:  3.49100604e-05, mean val. rec. loss:  3.66774683e-05\n",
      "Epoch: 8665 mean train loss:  3.48170956e-05, mean val. rec. loss:  3.61572497e-05\n",
      "Epoch: 8666 mean train loss:  3.50676086e-05, mean val. rec. loss:  3.65630019e-05\n",
      "Epoch: 8667 mean train loss:  3.49208592e-05, mean val. rec. loss:  3.62490440e-05\n",
      "Epoch: 8668 mean train loss:  3.50867678e-05, mean val. rec. loss:  3.62941291e-05\n",
      "Epoch: 8669 mean train loss:  3.49703593e-05, mean val. rec. loss:  3.66384323e-05\n",
      "Epoch: 8670 mean train loss:  3.51232602e-05, mean val. rec. loss:  3.67027866e-05\n",
      "Epoch: 8671 mean train loss:  3.52840194e-05, mean val. rec. loss:  3.80021724e-05\n",
      "Epoch: 8672 mean train loss:  3.52085305e-05, mean val. rec. loss:  3.70955367e-05\n",
      "Epoch: 8673 mean train loss:  3.50011931e-05, mean val. rec. loss:  3.63421808e-05\n",
      "Epoch: 8674 mean train loss:  3.48499381e-05, mean val. rec. loss:  3.71535967e-05\n",
      "Epoch: 8675 mean train loss:  3.55428893e-05, mean val. rec. loss:  3.58460380e-05\n",
      "Epoch: 8676 mean train loss:  3.46766350e-05, mean val. rec. loss:  3.60158340e-05\n",
      "Epoch: 8677 mean train loss:  3.48914351e-05, mean val. rec. loss:  3.62087359e-05\n",
      "Epoch: 8678 mean train loss:  3.51889314e-05, mean val. rec. loss:  3.72581615e-05\n",
      "Epoch: 8679 mean train loss:  3.51669448e-05, mean val. rec. loss:  3.76355539e-05\n",
      "Epoch: 8680 mean train loss:  3.54399066e-05, mean val. rec. loss:  3.56496334e-05\n",
      "Epoch: 8681 mean train loss:  3.53953841e-05, mean val. rec. loss:  3.66920719e-05\n",
      "Epoch: 8682 mean train loss:  3.53306260e-05, mean val. rec. loss:  3.63278543e-05\n",
      "Epoch: 8683 mean train loss:  3.47854212e-05, mean val. rec. loss:  3.69648744e-05\n",
      "Epoch: 8684 mean train loss:  3.51150951e-05, mean val. rec. loss:  3.71926510e-05\n",
      "Epoch: 8685 mean train loss:  3.48686417e-05, mean val. rec. loss:  3.53067659e-05\n",
      "Epoch: 8686 mean train loss:  3.52666566e-05, mean val. rec. loss:  3.70970904e-05\n",
      "Epoch: 8687 mean train loss:  3.51453980e-05, mean val. rec. loss:  3.80565503e-05\n",
      "Epoch: 8688 mean train loss:  3.59789572e-05, mean val. rec. loss:  3.74179015e-05\n",
      "Epoch: 8689 mean train loss:  3.48987152e-05, mean val. rec. loss:  3.56322745e-05\n",
      "Epoch: 8690 mean train loss:  3.50306582e-05, mean val. rec. loss:  3.68934125e-05\n",
      "Epoch: 8691 mean train loss:  3.53854090e-05, mean val. rec. loss:  3.64503164e-05\n",
      "Epoch: 8692 mean train loss:  3.50297155e-05, mean val. rec. loss:  3.79742782e-05\n",
      "Epoch: 8693 mean train loss:  3.52139596e-05, mean val. rec. loss:  3.65993440e-05\n",
      "Epoch: 8694 mean train loss:  3.49806172e-05, mean val. rec. loss:  3.66189812e-05\n",
      "Epoch: 8695 mean train loss:  3.46766935e-05, mean val. rec. loss:  3.64352427e-05\n",
      "Epoch: 8696 mean train loss:  3.50492013e-05, mean val. rec. loss:  3.63146773e-05\n",
      "Epoch: 8697 mean train loss:  3.53776787e-05, mean val. rec. loss:  3.69192533e-05\n",
      "Epoch: 8698 mean train loss:  3.52951461e-05, mean val. rec. loss:  3.85759535e-05\n",
      "Epoch: 8699 mean train loss:  3.56281884e-05, mean val. rec. loss:  3.76771386e-05\n",
      "Epoch: 8700 mean train loss:  3.55634462e-05, mean val. rec. loss:  3.57351039e-05\n",
      "Epoch: 8701 mean train loss:  3.51239726e-05, mean val. rec. loss:  3.62181173e-05\n",
      "Epoch: 8702 mean train loss:  3.58729630e-05, mean val. rec. loss:  3.70464833e-05\n",
      "Epoch: 8703 mean train loss:  3.49132080e-05, mean val. rec. loss:  3.70718062e-05\n",
      "Epoch: 8704 mean train loss:  3.47661458e-05, mean val. rec. loss:  3.79370662e-05\n",
      "Epoch: 8705 mean train loss:  3.48693067e-05, mean val. rec. loss:  3.58834090e-05\n",
      "Epoch: 8706 mean train loss:  3.44620346e-05, mean val. rec. loss:  3.68277020e-05\n",
      "Epoch: 8707 mean train loss:  3.45187858e-05, mean val. rec. loss:  3.59609042e-05\n",
      "Epoch: 8708 mean train loss:  3.48581682e-05, mean val. rec. loss:  3.62793665e-05\n",
      "Epoch: 8709 mean train loss:  3.46280444e-05, mean val. rec. loss:  3.55456070e-05\n",
      "Epoch: 8710 mean train loss:  3.44449202e-05, mean val. rec. loss:  3.55003629e-05\n",
      "Epoch: 8711 mean train loss:  3.46885358e-05, mean val. rec. loss:  3.60788687e-05\n",
      "Epoch: 8712 mean train loss:  3.43515615e-05, mean val. rec. loss:  3.55070184e-05\n",
      "Epoch: 8713 mean train loss:  3.46625246e-05, mean val. rec. loss:  3.74160048e-05\n",
      "Epoch: 8714 mean train loss:  3.44224816e-05, mean val. rec. loss:  3.58358162e-05\n",
      "Epoch: 8715 mean train loss:  3.46940285e-05, mean val. rec. loss:  3.62530464e-05\n",
      "Epoch: 8716 mean train loss:  3.44815332e-05, mean val. rec. loss:  3.67262605e-05\n",
      "Epoch: 8717 mean train loss:  3.47946640e-05, mean val. rec. loss:  3.78925580e-05\n",
      "Epoch: 8718 mean train loss:  3.51478365e-05, mean val. rec. loss:  3.59372895e-05\n",
      "Epoch: 8719 mean train loss:  3.48341585e-05, mean val. rec. loss:  3.70315435e-05\n",
      "Epoch: 8720 mean train loss:  3.54035275e-05, mean val. rec. loss:  3.64750351e-05\n",
      "Epoch: 8721 mean train loss:  3.48503875e-05, mean val. rec. loss:  3.88020922e-05\n",
      "Epoch: 8722 mean train loss:  3.55014296e-05, mean val. rec. loss:  3.54608657e-05\n",
      "Epoch: 8723 mean train loss:  3.49576215e-05, mean val. rec. loss:  3.77125879e-05\n",
      "Epoch: 8724 mean train loss:  3.49234355e-05, mean val. rec. loss:  3.58317025e-05\n",
      "Epoch: 8725 mean train loss:  3.48581811e-05, mean val. rec. loss:  3.82495181e-05\n",
      "Epoch: 8726 mean train loss:  3.56777454e-05, mean val. rec. loss:  3.55736716e-05\n",
      "Epoch: 8727 mean train loss:  3.46682722e-05, mean val. rec. loss:  3.59787810e-05\n",
      "Epoch: 8728 mean train loss:  3.44582723e-05, mean val. rec. loss:  3.66271042e-05\n",
      "Epoch: 8729 mean train loss:  3.43788336e-05, mean val. rec. loss:  3.59058221e-05\n",
      "Epoch: 8730 mean train loss:  3.45096902e-05, mean val. rec. loss:  3.57659488e-05\n",
      "Epoch: 8731 mean train loss:  3.49885455e-05, mean val. rec. loss:  3.57680432e-05\n",
      "Epoch: 8732 mean train loss:  3.49236067e-05, mean val. rec. loss:  3.61266001e-05\n",
      "Epoch: 8733 mean train loss:  3.44338419e-05, mean val. rec. loss:  3.70207742e-05\n",
      "Epoch: 8734 mean train loss:  3.45155204e-05, mean val. rec. loss:  3.52299318e-05\n",
      "Epoch: 8735 mean train loss:  3.46130569e-05, mean val. rec. loss:  3.69925188e-05\n",
      "Epoch: 8736 mean train loss:  3.44942181e-05, mean val. rec. loss:  3.57568627e-05\n",
      "Epoch: 8737 mean train loss:  3.43664334e-05, mean val. rec. loss:  3.59857296e-05\n",
      "Epoch: 8738 mean train loss:  3.40339301e-05, mean val. rec. loss:  3.50785328e-05\n",
      "Epoch: 8739 mean train loss:  3.46296463e-05, mean val. rec. loss:  3.69620714e-05\n",
      "Epoch: 8740 mean train loss:  3.50481099e-05, mean val. rec. loss:  3.74573442e-05\n",
      "Epoch: 8741 mean train loss:  3.57110507e-05, mean val. rec. loss:  3.57041363e-05\n",
      "Epoch: 8742 mean train loss:  3.48787681e-05, mean val. rec. loss:  3.65795636e-05\n",
      "Epoch: 8743 mean train loss:  3.50030605e-05, mean val. rec. loss:  3.57211908e-05\n",
      "Epoch: 8744 mean train loss:  3.45344560e-05, mean val. rec. loss:  3.59539715e-05\n",
      "Epoch: 8745 mean train loss:  3.43726370e-05, mean val. rec. loss:  3.63453450e-05\n",
      "Epoch: 8746 mean train loss:  3.49891384e-05, mean val. rec. loss:  3.65105662e-05\n",
      "Epoch: 8747 mean train loss:  3.51065456e-05, mean val. rec. loss:  3.58736097e-05\n",
      "Epoch: 8748 mean train loss:  3.45980119e-05, mean val. rec. loss:  3.64030689e-05\n",
      "Epoch: 8749 mean train loss:  3.47983192e-05, mean val. rec. loss:  3.62062668e-05\n",
      "Epoch: 8750 mean train loss:  3.47693460e-05, mean val. rec. loss:  3.69207525e-05\n",
      "Epoch: 8751 mean train loss:  3.45525138e-05, mean val. rec. loss:  3.63141616e-05\n",
      "Epoch: 8752 mean train loss:  3.42624413e-05, mean val. rec. loss:  3.78306614e-05\n",
      "Epoch: 8753 mean train loss:  3.47926393e-05, mean val. rec. loss:  3.68882175e-05\n",
      "Epoch: 8754 mean train loss:  3.46906154e-05, mean val. rec. loss:  3.53839112e-05\n",
      "Epoch: 8755 mean train loss:  3.45779896e-05, mean val. rec. loss:  3.64429431e-05\n",
      "Epoch: 8756 mean train loss:  3.40468773e-05, mean val. rec. loss:  3.57753938e-05\n",
      "Epoch: 8757 mean train loss:  3.41734616e-05, mean val. rec. loss:  3.62604629e-05\n",
      "Epoch: 8758 mean train loss:  3.45324600e-05, mean val. rec. loss:  3.55558811e-05\n",
      "Epoch: 8759 mean train loss:  3.41520676e-05, mean val. rec. loss:  3.52037639e-05\n",
      "Epoch: 8760 mean train loss:  3.42467343e-05, mean val. rec. loss:  3.48690034e-05\n",
      "Epoch: 8761 mean train loss:  3.41023593e-05, mean val. rec. loss:  3.62762091e-05\n",
      "Epoch: 8762 mean train loss:  3.41405529e-05, mean val. rec. loss:  3.52724841e-05\n",
      "Epoch: 8763 mean train loss:  3.40382187e-05, mean val. rec. loss:  3.56104588e-05\n",
      "Epoch: 8764 mean train loss:  3.42489873e-05, mean val. rec. loss:  3.53903373e-05\n",
      "Epoch: 8765 mean train loss:  3.44944967e-05, mean val. rec. loss:  3.57005563e-05\n",
      "Epoch: 8766 mean train loss:  3.40460850e-05, mean val. rec. loss:  3.61441476e-05\n",
      "Epoch: 8767 mean train loss:  3.42923943e-05, mean val. rec. loss:  3.56926560e-05\n",
      "Epoch: 8768 mean train loss:  3.41438202e-05, mean val. rec. loss:  3.64444309e-05\n",
      "Epoch: 8769 mean train loss:  3.40342731e-05, mean val. rec. loss:  3.47626009e-05\n",
      "Epoch: 8770 mean train loss:  3.40762403e-05, mean val. rec. loss:  3.57966643e-05\n",
      "Epoch: 8771 mean train loss:  3.44377636e-05, mean val. rec. loss:  3.65745980e-05\n",
      "Epoch: 8772 mean train loss:  3.43673870e-05, mean val. rec. loss:  3.46320159e-05\n",
      "Epoch: 8773 mean train loss:  3.38613696e-05, mean val. rec. loss:  3.57183355e-05\n",
      "Epoch: 8774 mean train loss:  3.38544287e-05, mean val. rec. loss:  3.47148127e-05\n",
      "Epoch: 8775 mean train loss:  3.42887426e-05, mean val. rec. loss:  3.48926454e-05\n",
      "Epoch: 8776 mean train loss:  3.40781457e-05, mean val. rec. loss:  3.75636422e-05\n",
      "Epoch: 8777 mean train loss:  3.46581402e-05, mean val. rec. loss:  3.68209965e-05\n",
      "Epoch: 8778 mean train loss:  3.43435282e-05, mean val. rec. loss:  3.62787577e-05\n",
      "Epoch: 8779 mean train loss:  3.46210262e-05, mean val. rec. loss:  3.49048616e-05\n",
      "Epoch: 8780 mean train loss:  3.37733226e-05, mean val. rec. loss:  3.63942031e-05\n",
      "Epoch: 8781 mean train loss:  3.46152654e-05, mean val. rec. loss:  3.75754518e-05\n",
      "Epoch: 8782 mean train loss:  3.43658479e-05, mean val. rec. loss:  3.51346257e-05\n",
      "Epoch: 8783 mean train loss:  3.39391407e-05, mean val. rec. loss:  3.70837771e-05\n",
      "Epoch: 8784 mean train loss:  3.43306264e-05, mean val. rec. loss:  3.64469819e-05\n",
      "Epoch: 8785 mean train loss:  3.46656371e-05, mean val. rec. loss:  3.80887082e-05\n",
      "Epoch: 8786 mean train loss:  3.49927079e-05, mean val. rec. loss:  3.56517323e-05\n",
      "Epoch: 8787 mean train loss:  3.42293247e-05, mean val. rec. loss:  3.48261807e-05\n",
      "Epoch: 8788 mean train loss:  3.40552479e-05, mean val. rec. loss:  3.49929693e-05\n",
      "Epoch: 8789 mean train loss:  3.41334193e-05, mean val. rec. loss:  3.62606083e-05\n",
      "Epoch: 8790 mean train loss:  3.38699762e-05, mean val. rec. loss:  3.49921606e-05\n",
      "Epoch: 8791 mean train loss:  3.38170377e-05, mean val. rec. loss:  3.54624558e-05\n",
      "Epoch: 8792 mean train loss:  3.37965581e-05, mean val. rec. loss:  3.51090438e-05\n",
      "Epoch: 8793 mean train loss:  3.40702660e-05, mean val. rec. loss:  3.71152921e-05\n",
      "Epoch: 8794 mean train loss:  3.47935545e-05, mean val. rec. loss:  3.51520301e-05\n",
      "Epoch: 8795 mean train loss:  3.40415134e-05, mean val. rec. loss:  3.51978602e-05\n",
      "Epoch: 8796 mean train loss:  3.41207210e-05, mean val. rec. loss:  3.51140639e-05\n",
      "Epoch: 8797 mean train loss:  3.36842528e-05, mean val. rec. loss:  3.65633245e-05\n",
      "Epoch: 8798 mean train loss:  3.40395968e-05, mean val. rec. loss:  3.57643406e-05\n",
      "Epoch: 8799 mean train loss:  3.36164141e-05, mean val. rec. loss:  3.49543080e-05\n",
      "Epoch: 8800 mean train loss:  3.37282496e-05, mean val. rec. loss:  3.53667431e-05\n",
      "Epoch: 8801 mean train loss:  3.39287107e-05, mean val. rec. loss:  3.54435022e-05\n",
      "Epoch: 8802 mean train loss:  3.39855763e-05, mean val. rec. loss:  3.59208982e-05\n",
      "Epoch: 8803 mean train loss:  3.38691698e-05, mean val. rec. loss:  3.44457241e-05\n",
      "Epoch: 8804 mean train loss:  3.36913486e-05, mean val. rec. loss:  3.50535189e-05\n",
      "Epoch: 8805 mean train loss:  3.34248471e-05, mean val. rec. loss:  3.52830058e-05\n",
      "Epoch: 8806 mean train loss:  3.34927431e-05, mean val. rec. loss:  3.49747698e-05\n",
      "Epoch: 8807 mean train loss:  3.36708068e-05, mean val. rec. loss:  3.58320864e-05\n",
      "Epoch: 8808 mean train loss:  3.36420767e-05, mean val. rec. loss:  3.51441388e-05\n",
      "Epoch: 8809 mean train loss:  3.38006460e-05, mean val. rec. loss:  3.59272221e-05\n",
      "Epoch: 8810 mean train loss:  3.36381222e-05, mean val. rec. loss:  3.45735652e-05\n",
      "Epoch: 8811 mean train loss:  3.37710896e-05, mean val. rec. loss:  3.57319487e-05\n",
      "Epoch: 8812 mean train loss:  3.39129039e-05, mean val. rec. loss:  3.45728928e-05\n",
      "Epoch: 8813 mean train loss:  3.40893779e-05, mean val. rec. loss:  3.64307132e-05\n",
      "Epoch: 8814 mean train loss:  3.38322038e-05, mean val. rec. loss:  3.50221855e-05\n",
      "Epoch: 8815 mean train loss:  3.38688605e-05, mean val. rec. loss:  3.58829684e-05\n",
      "Epoch: 8816 mean train loss:  3.35155751e-05, mean val. rec. loss:  3.56889966e-05\n",
      "Epoch: 8817 mean train loss:  3.45727144e-05, mean val. rec. loss:  3.58947303e-05\n",
      "Epoch: 8818 mean train loss:  3.39582109e-05, mean val. rec. loss:  3.51364838e-05\n",
      "Epoch: 8819 mean train loss:  3.35899070e-05, mean val. rec. loss:  3.51746680e-05\n",
      "Epoch: 8820 mean train loss:  3.32879970e-05, mean val. rec. loss:  3.56357181e-05\n",
      "Epoch: 8821 mean train loss:  3.35052681e-05, mean val. rec. loss:  3.53793818e-05\n",
      "Epoch: 8822 mean train loss:  3.37214369e-05, mean val. rec. loss:  3.60929180e-05\n",
      "Epoch: 8823 mean train loss:  3.36967521e-05, mean val. rec. loss:  3.46034720e-05\n",
      "Epoch: 8824 mean train loss:  3.35925697e-05, mean val. rec. loss:  3.54746856e-05\n",
      "Epoch: 8825 mean train loss:  3.35887675e-05, mean val. rec. loss:  3.47727683e-05\n",
      "Epoch: 8826 mean train loss:  3.34013367e-05, mean val. rec. loss:  3.51155495e-05\n",
      "Epoch: 8827 mean train loss:  3.31458715e-05, mean val. rec. loss:  3.52076232e-05\n",
      "Epoch: 8828 mean train loss:  3.34446012e-05, mean val. rec. loss:  3.53447411e-05\n",
      "Epoch: 8829 mean train loss:  3.34388148e-05, mean val. rec. loss:  3.52131157e-05\n",
      "Epoch: 8830 mean train loss:  3.35634171e-05, mean val. rec. loss:  3.51369177e-05\n",
      "Epoch: 8831 mean train loss:  3.36142679e-05, mean val. rec. loss:  3.44861867e-05\n",
      "Epoch: 8832 mean train loss:  3.37035872e-05, mean val. rec. loss:  3.57195258e-05\n",
      "Epoch: 8833 mean train loss:  3.38810604e-05, mean val. rec. loss:  3.49823567e-05\n",
      "Epoch: 8834 mean train loss:  3.33992181e-05, mean val. rec. loss:  3.59395337e-05\n",
      "Epoch: 8835 mean train loss:  3.35655669e-05, mean val. rec. loss:  3.40777563e-05\n",
      "Epoch: 8836 mean train loss:  3.34779551e-05, mean val. rec. loss:  3.49933849e-05\n",
      "Epoch: 8837 mean train loss:  3.32438786e-05, mean val. rec. loss:  3.47832854e-05\n",
      "Epoch: 8838 mean train loss:  3.31143300e-05, mean val. rec. loss:  3.49664334e-05\n",
      "Epoch: 8839 mean train loss:  3.33589016e-05, mean val. rec. loss:  3.44686483e-05\n",
      "Epoch: 8840 mean train loss:  3.36016352e-05, mean val. rec. loss:  3.47532014e-05\n",
      "Epoch: 8841 mean train loss:  3.37406116e-05, mean val. rec. loss:  3.83034212e-05\n",
      "Epoch: 8842 mean train loss:  3.48006164e-05, mean val. rec. loss:  3.68670083e-05\n",
      "Epoch: 8843 mean train loss:  3.45221068e-05, mean val. rec. loss:  3.56822479e-05\n",
      "Epoch: 8844 mean train loss:  3.37954100e-05, mean val. rec. loss:  3.43758840e-05\n",
      "Epoch: 8845 mean train loss:  3.45123811e-05, mean val. rec. loss:  3.58705432e-05\n",
      "Epoch: 8846 mean train loss:  3.39951279e-05, mean val. rec. loss:  3.55621573e-05\n",
      "Epoch: 8847 mean train loss:  3.33816050e-05, mean val. rec. loss:  3.46902894e-05\n",
      "Epoch: 8848 mean train loss:  3.32500100e-05, mean val. rec. loss:  3.50467520e-05\n",
      "Epoch: 8849 mean train loss:  3.32576227e-05, mean val. rec. loss:  3.47804255e-05\n",
      "Epoch: 8850 mean train loss:  3.32790397e-05, mean val. rec. loss:  3.48425311e-05\n",
      "Epoch: 8851 mean train loss:  3.30120156e-05, mean val. rec. loss:  3.51570456e-05\n",
      "Epoch: 8852 mean train loss:  3.33906537e-05, mean val. rec. loss:  3.43359144e-05\n",
      "Epoch: 8853 mean train loss:  3.32343126e-05, mean val. rec. loss:  3.50851066e-05\n",
      "Epoch: 8854 mean train loss:  3.36407010e-05, mean val. rec. loss:  3.55941312e-05\n",
      "Epoch: 8855 mean train loss:  3.32749569e-05, mean val. rec. loss:  3.41489593e-05\n",
      "Epoch: 8856 mean train loss:  3.33492280e-05, mean val. rec. loss:  3.45674866e-05\n",
      "Epoch: 8857 mean train loss:  3.31913615e-05, mean val. rec. loss:  3.51345167e-05\n",
      "Epoch: 8858 mean train loss:  3.32268040e-05, mean val. rec. loss:  3.50324369e-05\n",
      "Epoch: 8859 mean train loss:  3.32040639e-05, mean val. rec. loss:  3.45614989e-05\n",
      "Epoch: 8860 mean train loss:  3.33690007e-05, mean val. rec. loss:  3.39714742e-05\n",
      "Epoch: 8861 mean train loss:  3.32612957e-05, mean val. rec. loss:  3.63702818e-05\n",
      "Epoch: 8862 mean train loss:  3.34509905e-05, mean val. rec. loss:  3.42426617e-05\n",
      "Epoch: 8863 mean train loss:  3.30539701e-05, mean val. rec. loss:  3.55758636e-05\n",
      "Epoch: 8864 mean train loss:  3.30189548e-05, mean val. rec. loss:  3.40115506e-05\n",
      "Epoch: 8865 mean train loss:  3.29529557e-05, mean val. rec. loss:  3.38958349e-05\n",
      "Epoch: 8866 mean train loss:  3.32037211e-05, mean val. rec. loss:  3.54574857e-05\n",
      "Epoch: 8867 mean train loss:  3.30638376e-05, mean val. rec. loss:  3.46749112e-05\n",
      "Epoch: 8868 mean train loss:  3.34933744e-05, mean val. rec. loss:  3.62876212e-05\n",
      "Epoch: 8869 mean train loss:  3.38154779e-05, mean val. rec. loss:  3.44422033e-05\n",
      "Epoch: 8870 mean train loss:  3.33300643e-05, mean val. rec. loss:  3.40612787e-05\n",
      "Epoch: 8871 mean train loss:  3.35114391e-05, mean val. rec. loss:  3.57244436e-05\n",
      "Epoch: 8872 mean train loss:  3.39508004e-05, mean val. rec. loss:  3.50288002e-05\n",
      "Epoch: 8873 mean train loss:  3.30924448e-05, mean val. rec. loss:  3.47719619e-05\n",
      "Epoch: 8874 mean train loss:  3.32417492e-05, mean val. rec. loss:  3.39068154e-05\n",
      "Epoch: 8875 mean train loss:  3.34465566e-05, mean val. rec. loss:  3.52869651e-05\n",
      "Epoch: 8876 mean train loss:  3.32950244e-05, mean val. rec. loss:  3.42102903e-05\n",
      "Epoch: 8877 mean train loss:  3.32425330e-05, mean val. rec. loss:  3.48730967e-05\n",
      "Epoch: 8878 mean train loss:  3.31934902e-05, mean val. rec. loss:  3.42629123e-05\n",
      "Epoch: 8879 mean train loss:  3.34871235e-05, mean val. rec. loss:  3.51303643e-05\n",
      "Epoch: 8880 mean train loss:  3.34584358e-05, mean val. rec. loss:  3.42702175e-05\n",
      "Epoch: 8881 mean train loss:  3.36696849e-05, mean val. rec. loss:  3.44314363e-05\n",
      "Epoch: 8882 mean train loss:  3.34307966e-05, mean val. rec. loss:  3.49047139e-05\n",
      "Epoch: 8883 mean train loss:  3.31140051e-05, mean val. rec. loss:  3.38834938e-05\n",
      "Epoch: 8884 mean train loss:  3.31622335e-05, mean val. rec. loss:  3.58618228e-05\n",
      "Epoch: 8885 mean train loss:  3.42378786e-05, mean val. rec. loss:  3.63164945e-05\n",
      "Epoch: 8886 mean train loss:  3.41199333e-05, mean val. rec. loss:  3.56234837e-05\n",
      "Epoch: 8887 mean train loss:  3.35945152e-05, mean val. rec. loss:  3.46494611e-05\n",
      "Epoch: 8888 mean train loss:  3.33715198e-05, mean val. rec. loss:  3.48119883e-05\n",
      "Epoch: 8889 mean train loss:  3.32551616e-05, mean val. rec. loss:  3.43567442e-05\n",
      "Epoch: 8890 mean train loss:  3.28954418e-05, mean val. rec. loss:  3.46775235e-05\n",
      "Epoch: 8891 mean train loss:  3.28269848e-05, mean val. rec. loss:  3.38761545e-05\n",
      "Epoch: 8892 mean train loss:  3.28872001e-05, mean val. rec. loss:  3.42758509e-05\n",
      "Epoch: 8893 mean train loss:  3.27319003e-05, mean val. rec. loss:  3.34488819e-05\n",
      "Epoch: 8894 mean train loss:  3.28894938e-05, mean val. rec. loss:  3.43709775e-05\n",
      "Epoch: 8895 mean train loss:  3.29088454e-05, mean val. rec. loss:  3.43985742e-05\n",
      "Epoch: 8896 mean train loss:  3.30853931e-05, mean val. rec. loss:  3.43095920e-05\n",
      "Epoch: 8897 mean train loss:  3.27403988e-05, mean val. rec. loss:  3.43404029e-05\n",
      "Epoch: 8898 mean train loss:  3.27046080e-05, mean val. rec. loss:  3.56874610e-05\n",
      "Epoch: 8899 mean train loss:  3.28296055e-05, mean val. rec. loss:  3.50657941e-05\n",
      "Epoch: 8900 mean train loss:  3.33327013e-05, mean val. rec. loss:  3.58461834e-05\n",
      "Epoch: 8901 mean train loss:  3.38339855e-05, mean val. rec. loss:  3.55287160e-05\n",
      "Epoch: 8902 mean train loss:  3.33929573e-05, mean val. rec. loss:  3.35623715e-05\n",
      "Epoch: 8903 mean train loss:  3.28885850e-05, mean val. rec. loss:  3.50885275e-05\n",
      "Epoch: 8904 mean train loss:  3.32408199e-05, mean val. rec. loss:  3.39653366e-05\n",
      "Epoch: 8905 mean train loss:  3.31022076e-05, mean val. rec. loss:  3.51461650e-05\n",
      "Epoch: 8906 mean train loss:  3.28860449e-05, mean val. rec. loss:  3.44695387e-05\n",
      "Epoch: 8907 mean train loss:  3.32747835e-05, mean val. rec. loss:  3.55472493e-05\n",
      "Epoch: 8908 mean train loss:  3.28765022e-05, mean val. rec. loss:  3.38364983e-05\n",
      "Epoch: 8909 mean train loss:  3.29707730e-05, mean val. rec. loss:  3.37546737e-05\n",
      "Epoch: 8910 mean train loss:  3.28181274e-05, mean val. rec. loss:  3.39084918e-05\n",
      "Epoch: 8911 mean train loss:  3.29407689e-05, mean val. rec. loss:  3.33055286e-05\n",
      "Epoch: 8912 mean train loss:  3.26095863e-05, mean val. rec. loss:  3.57468681e-05\n",
      "Epoch: 8913 mean train loss:  3.30459864e-05, mean val. rec. loss:  3.50921937e-05\n",
      "Epoch: 8914 mean train loss:  3.33778372e-05, mean val. rec. loss:  3.40127863e-05\n",
      "Epoch: 8915 mean train loss:  3.32106909e-05, mean val. rec. loss:  3.46946553e-05\n",
      "Epoch: 8916 mean train loss:  3.33876087e-05, mean val. rec. loss:  3.51154314e-05\n",
      "Epoch: 8917 mean train loss:  3.32341628e-05, mean val. rec. loss:  3.40626734e-05\n",
      "Epoch: 8918 mean train loss:  3.25492013e-05, mean val. rec. loss:  3.47544076e-05\n",
      "Epoch: 8919 mean train loss:  3.25823291e-05, mean val. rec. loss:  3.39349277e-05\n",
      "Epoch: 8920 mean train loss:  3.28792945e-05, mean val. rec. loss:  3.53358959e-05\n",
      "Epoch: 8921 mean train loss:  3.31350544e-05, mean val. rec. loss:  3.32667470e-05\n",
      "Epoch: 8922 mean train loss:  3.30269354e-05, mean val. rec. loss:  3.60346445e-05\n",
      "Epoch: 8923 mean train loss:  3.38015615e-05, mean val. rec. loss:  3.63339943e-05\n",
      "Epoch: 8924 mean train loss:  3.46286277e-05, mean val. rec. loss:  3.65392419e-05\n",
      "Epoch: 8925 mean train loss:  3.37369111e-05, mean val. rec. loss:  3.45608674e-05\n",
      "Epoch: 8926 mean train loss:  3.34034450e-05, mean val. rec. loss:  3.51215781e-05\n",
      "Epoch: 8927 mean train loss:  3.30298663e-05, mean val. rec. loss:  3.50201957e-05\n",
      "Epoch: 8928 mean train loss:  3.31502201e-05, mean val. rec. loss:  3.47956106e-05\n",
      "Epoch: 8929 mean train loss:  3.26014573e-05, mean val. rec. loss:  3.33649243e-05\n",
      "Epoch: 8930 mean train loss:  3.27833353e-05, mean val. rec. loss:  3.47017969e-05\n",
      "Epoch: 8931 mean train loss:  3.35051293e-05, mean val. rec. loss:  3.46370950e-05\n",
      "Epoch: 8932 mean train loss:  3.30837088e-05, mean val. rec. loss:  3.57621327e-05\n",
      "Epoch: 8933 mean train loss:  3.35129345e-05, mean val. rec. loss:  3.43528962e-05\n",
      "Epoch: 8934 mean train loss:  3.28915677e-05, mean val. rec. loss:  3.51527797e-05\n",
      "Epoch: 8935 mean train loss:  3.31374971e-05, mean val. rec. loss:  3.54003865e-05\n",
      "Epoch: 8936 mean train loss:  3.29918702e-05, mean val. rec. loss:  3.54803712e-05\n",
      "Epoch: 8937 mean train loss:  3.34316921e-05, mean val. rec. loss:  3.52162868e-05\n",
      "Epoch: 8938 mean train loss:  3.32922033e-05, mean val. rec. loss:  3.33285391e-05\n",
      "Epoch: 8939 mean train loss:  3.27545308e-05, mean val. rec. loss:  3.56054570e-05\n",
      "Epoch: 8940 mean train loss:  3.31676633e-05, mean val. rec. loss:  3.34143207e-05\n",
      "Epoch: 8941 mean train loss:  3.23917900e-05, mean val. rec. loss:  3.53897808e-05\n",
      "Epoch: 8942 mean train loss:  3.23963079e-05, mean val. rec. loss:  3.39650526e-05\n",
      "Epoch: 8943 mean train loss:  3.24588325e-05, mean val. rec. loss:  3.41684535e-05\n",
      "Epoch: 8944 mean train loss:  3.24842176e-05, mean val. rec. loss:  3.38942539e-05\n",
      "Epoch: 8945 mean train loss:  3.25481597e-05, mean val. rec. loss:  3.49830700e-05\n",
      "Epoch: 8946 mean train loss:  3.26505543e-05, mean val. rec. loss:  3.37280469e-05\n",
      "Epoch: 8947 mean train loss:  3.23070913e-05, mean val. rec. loss:  3.41175169e-05\n",
      "Epoch: 8948 mean train loss:  3.26421333e-05, mean val. rec. loss:  3.34355935e-05\n",
      "Epoch: 8949 mean train loss:  3.22340363e-05, mean val. rec. loss:  3.32884082e-05\n",
      "Epoch: 8950 mean train loss:  3.26624146e-05, mean val. rec. loss:  3.41113657e-05\n",
      "Epoch: 8951 mean train loss:  3.25354032e-05, mean val. rec. loss:  3.41655368e-05\n",
      "Epoch: 8952 mean train loss:  3.25527849e-05, mean val. rec. loss:  3.44033716e-05\n",
      "Epoch: 8953 mean train loss:  3.23934835e-05, mean val. rec. loss:  3.32362995e-05\n",
      "Epoch: 8954 mean train loss:  3.22736315e-05, mean val. rec. loss:  3.45088360e-05\n",
      "Epoch: 8955 mean train loss:  3.24257762e-05, mean val. rec. loss:  3.43941765e-05\n",
      "Epoch: 8956 mean train loss:  3.25101146e-05, mean val. rec. loss:  3.31919504e-05\n",
      "Epoch: 8957 mean train loss:  3.21635321e-05, mean val. rec. loss:  3.47998493e-05\n",
      "Epoch: 8958 mean train loss:  3.26501001e-05, mean val. rec. loss:  3.31350443e-05\n",
      "Epoch: 8959 mean train loss:  3.24552698e-05, mean val. rec. loss:  3.43846248e-05\n",
      "Epoch: 8960 mean train loss:  3.22354339e-05, mean val. rec. loss:  3.33282188e-05\n",
      "Epoch: 8961 mean train loss:  3.25240788e-05, mean val. rec. loss:  3.34675765e-05\n",
      "Epoch: 8962 mean train loss:  3.21980772e-05, mean val. rec. loss:  3.43420134e-05\n",
      "Epoch: 8963 mean train loss:  3.20187405e-05, mean val. rec. loss:  3.33013422e-05\n",
      "Epoch: 8964 mean train loss:  3.20317072e-05, mean val. rec. loss:  3.35899295e-05\n",
      "Epoch: 8965 mean train loss:  3.19983321e-05, mean val. rec. loss:  3.30788242e-05\n",
      "Epoch: 8966 mean train loss:  3.22653003e-05, mean val. rec. loss:  3.40408941e-05\n",
      "Epoch: 8967 mean train loss:  3.19204254e-05, mean val. rec. loss:  3.39572227e-05\n",
      "Epoch: 8968 mean train loss:  3.23699637e-05, mean val. rec. loss:  3.48128628e-05\n",
      "Epoch: 8969 mean train loss:  3.24630430e-05, mean val. rec. loss:  3.31926546e-05\n",
      "Epoch: 8970 mean train loss:  3.23732428e-05, mean val. rec. loss:  3.37914791e-05\n",
      "Epoch: 8971 mean train loss:  3.25700729e-05, mean val. rec. loss:  3.42100495e-05\n",
      "Epoch: 8972 mean train loss:  3.28965561e-05, mean val. rec. loss:  3.34933446e-05\n",
      "Epoch: 8973 mean train loss:  3.22300469e-05, mean val. rec. loss:  3.37772753e-05\n",
      "Epoch: 8974 mean train loss:  3.23160010e-05, mean val. rec. loss:  3.34066725e-05\n",
      "Epoch: 8975 mean train loss:  3.21721463e-05, mean val. rec. loss:  3.41462448e-05\n",
      "Epoch: 8976 mean train loss:  3.21563688e-05, mean val. rec. loss:  3.36673247e-05\n",
      "Epoch: 8977 mean train loss:  3.22441399e-05, mean val. rec. loss:  3.33027278e-05\n",
      "Epoch: 8978 mean train loss:  3.18609566e-05, mean val. rec. loss:  3.42485972e-05\n",
      "Epoch: 8979 mean train loss:  3.25234632e-05, mean val. rec. loss:  3.45037523e-05\n",
      "Epoch: 8980 mean train loss:  3.22998177e-05, mean val. rec. loss:  3.29753793e-05\n",
      "Epoch: 8981 mean train loss:  3.23091281e-05, mean val. rec. loss:  3.44838629e-05\n",
      "Epoch: 8982 mean train loss:  3.23457069e-05, mean val. rec. loss:  3.39197245e-05\n",
      "Epoch: 8983 mean train loss:  3.21774642e-05, mean val. rec. loss:  3.30578718e-05\n",
      "Epoch: 8984 mean train loss:  3.18736540e-05, mean val. rec. loss:  3.38238323e-05\n",
      "Epoch: 8985 mean train loss:  3.26695917e-05, mean val. rec. loss:  3.38620302e-05\n",
      "Epoch: 8986 mean train loss:  3.22422487e-05, mean val. rec. loss:  3.54180067e-05\n",
      "Epoch: 8987 mean train loss:  3.22801094e-05, mean val. rec. loss:  3.30466436e-05\n",
      "Epoch: 8988 mean train loss:  3.19236334e-05, mean val. rec. loss:  3.34080672e-05\n",
      "Epoch: 8989 mean train loss:  3.23113978e-05, mean val. rec. loss:  3.38952307e-05\n",
      "Epoch: 8990 mean train loss:  3.26721252e-05, mean val. rec. loss:  3.71557910e-05\n",
      "Epoch: 8991 mean train loss:  3.33485459e-05, mean val. rec. loss:  3.44797265e-05\n",
      "Epoch: 8992 mean train loss:  3.30470201e-05, mean val. rec. loss:  3.28474201e-05\n",
      "Epoch: 8993 mean train loss:  3.23115014e-05, mean val. rec. loss:  3.42066468e-05\n",
      "Epoch: 8994 mean train loss:  3.22353190e-05, mean val. rec. loss:  3.26955373e-05\n",
      "Epoch: 8995 mean train loss:  3.16585329e-05, mean val. rec. loss:  3.37788881e-05\n",
      "Epoch: 8996 mean train loss:  3.20207728e-05, mean val. rec. loss:  3.25124802e-05\n",
      "Epoch: 8997 mean train loss:  3.19084329e-05, mean val. rec. loss:  3.38659872e-05\n",
      "Epoch: 8998 mean train loss:  3.21114366e-05, mean val. rec. loss:  3.35243758e-05\n",
      "Epoch: 8999 mean train loss:  3.20671940e-05, mean val. rec. loss:  3.33942268e-05\n",
      "Epoch: 9000 mean train loss:  3.25417094e-05, mean val. rec. loss:  3.33854929e-05\n",
      "Epoch: 9001 mean train loss:  3.24767178e-05, mean val. rec. loss:  3.36175739e-05\n",
      "Epoch: 9002 mean train loss:  3.28831967e-05, mean val. rec. loss:  3.35627485e-05\n",
      "Epoch: 9003 mean train loss:  3.23960370e-05, mean val. rec. loss:  3.29569096e-05\n",
      "Epoch: 9004 mean train loss:  3.19324339e-05, mean val. rec. loss:  3.31752729e-05\n",
      "Epoch: 9005 mean train loss:  3.19141337e-05, mean val. rec. loss:  3.37216662e-05\n",
      "Epoch: 9006 mean train loss:  3.17596460e-05, mean val. rec. loss:  3.32423281e-05\n",
      "Epoch: 9007 mean train loss:  3.15237474e-05, mean val. rec. loss:  3.26959303e-05\n",
      "Epoch: 9008 mean train loss:  3.19684266e-05, mean val. rec. loss:  3.28474792e-05\n",
      "Epoch: 9009 mean train loss:  3.18632784e-05, mean val. rec. loss:  3.38612397e-05\n",
      "Epoch: 9010 mean train loss:  3.21546799e-05, mean val. rec. loss:  3.45097877e-05\n",
      "Epoch: 9011 mean train loss:  3.22301261e-05, mean val. rec. loss:  3.33174586e-05\n",
      "Epoch: 9012 mean train loss:  3.22104127e-05, mean val. rec. loss:  3.28757164e-05\n",
      "Epoch: 9013 mean train loss:  3.18668151e-05, mean val. rec. loss:  3.31975293e-05\n",
      "Epoch: 9014 mean train loss:  3.22638588e-05, mean val. rec. loss:  3.35455986e-05\n",
      "Epoch: 9015 mean train loss:  3.19627349e-05, mean val. rec. loss:  3.33972321e-05\n",
      "Epoch: 9016 mean train loss:  3.21310798e-05, mean val. rec. loss:  3.31518240e-05\n",
      "Epoch: 9017 mean train loss:  3.16506699e-05, mean val. rec. loss:  3.30994996e-05\n",
      "Epoch: 9018 mean train loss:  3.15418311e-05, mean val. rec. loss:  3.28202641e-05\n",
      "Epoch: 9019 mean train loss:  3.15637144e-05, mean val. rec. loss:  3.33153575e-05\n",
      "Epoch: 9020 mean train loss:  3.18127699e-05, mean val. rec. loss:  3.33285573e-05\n",
      "Epoch: 9021 mean train loss:  3.15911770e-05, mean val. rec. loss:  3.29967952e-05\n",
      "Epoch: 9022 mean train loss:  3.19016414e-05, mean val. rec. loss:  3.33269195e-05\n",
      "Epoch: 9023 mean train loss:  3.16166233e-05, mean val. rec. loss:  3.25709763e-05\n",
      "Epoch: 9024 mean train loss:  3.15278223e-05, mean val. rec. loss:  3.33667574e-05\n",
      "Epoch: 9025 mean train loss:  3.16765067e-05, mean val. rec. loss:  3.27015000e-05\n",
      "Epoch: 9026 mean train loss:  3.18453465e-05, mean val. rec. loss:  3.28028257e-05\n",
      "Epoch: 9027 mean train loss:  3.17391729e-05, mean val. rec. loss:  3.34397481e-05\n",
      "Epoch: 9028 mean train loss:  3.17887820e-05, mean val. rec. loss:  3.24072339e-05\n",
      "Epoch: 9029 mean train loss:  3.18898908e-05, mean val. rec. loss:  3.40891502e-05\n",
      "Epoch: 9030 mean train loss:  3.24894965e-05, mean val. rec. loss:  3.20536857e-05\n",
      "Epoch: 9031 mean train loss:  3.18070639e-05, mean val. rec. loss:  3.36149707e-05\n",
      "Epoch: 9032 mean train loss:  3.19835677e-05, mean val. rec. loss:  3.40142174e-05\n",
      "Epoch: 9033 mean train loss:  3.24740226e-05, mean val. rec. loss:  3.48830187e-05\n",
      "Epoch: 9034 mean train loss:  3.24787983e-05, mean val. rec. loss:  3.23248482e-05\n",
      "Epoch: 9035 mean train loss:  3.18160358e-05, mean val. rec. loss:  3.34037650e-05\n",
      "Epoch: 9036 mean train loss:  3.18130967e-05, mean val. rec. loss:  3.31468812e-05\n",
      "Epoch: 9037 mean train loss:  3.17022648e-05, mean val. rec. loss:  3.29431260e-05\n",
      "Epoch: 9038 mean train loss:  3.16444574e-05, mean val. rec. loss:  3.45598747e-05\n",
      "Epoch: 9039 mean train loss:  3.21645132e-05, mean val. rec. loss:  3.39272228e-05\n",
      "Epoch: 9040 mean train loss:  3.16014196e-05, mean val. rec. loss:  3.34339194e-05\n",
      "Epoch: 9041 mean train loss:  3.16935125e-05, mean val. rec. loss:  3.30228222e-05\n",
      "Epoch: 9042 mean train loss:  3.15159420e-05, mean val. rec. loss:  3.35025124e-05\n",
      "Epoch: 9043 mean train loss:  3.19313977e-05, mean val. rec. loss:  3.52262497e-05\n",
      "Epoch: 9044 mean train loss:  3.22303629e-05, mean val. rec. loss:  3.27645483e-05\n",
      "Epoch: 9045 mean train loss:  3.18149639e-05, mean val. rec. loss:  3.23468842e-05\n",
      "Epoch: 9046 mean train loss:  3.19207688e-05, mean val. rec. loss:  3.25710263e-05\n",
      "Epoch: 9047 mean train loss:  3.16060398e-05, mean val. rec. loss:  3.32166373e-05\n",
      "Epoch: 9048 mean train loss:  3.14286123e-05, mean val. rec. loss:  3.23945111e-05\n",
      "Epoch: 9049 mean train loss:  3.15925967e-05, mean val. rec. loss:  3.33607379e-05\n",
      "Epoch: 9050 mean train loss:  3.16951409e-05, mean val. rec. loss:  3.22162923e-05\n",
      "Epoch: 9051 mean train loss:  3.18319971e-05, mean val. rec. loss:  3.39874521e-05\n",
      "Epoch: 9052 mean train loss:  3.18488858e-05, mean val. rec. loss:  3.33432721e-05\n",
      "Epoch: 9053 mean train loss:  3.16295102e-05, mean val. rec. loss:  3.22004826e-05\n",
      "Epoch: 9054 mean train loss:  3.16429069e-05, mean val. rec. loss:  3.37579810e-05\n",
      "Epoch: 9055 mean train loss:  3.15320120e-05, mean val. rec. loss:  3.31031704e-05\n",
      "Epoch: 9056 mean train loss:  3.13677653e-05, mean val. rec. loss:  3.29669769e-05\n",
      "Epoch: 9057 mean train loss:  3.12312157e-05, mean val. rec. loss:  3.28097674e-05\n",
      "Epoch: 9058 mean train loss:  3.16196395e-05, mean val. rec. loss:  3.37903683e-05\n",
      "Epoch: 9059 mean train loss:  3.24129448e-05, mean val. rec. loss:  3.24319412e-05\n",
      "Epoch: 9060 mean train loss:  3.21758396e-05, mean val. rec. loss:  3.40271423e-05\n",
      "Epoch: 9061 mean train loss:  3.14548844e-05, mean val. rec. loss:  3.28143604e-05\n",
      "Epoch: 9062 mean train loss:  3.16409423e-05, mean val. rec. loss:  3.26203500e-05\n",
      "Epoch: 9063 mean train loss:  3.18107778e-05, mean val. rec. loss:  3.33288412e-05\n",
      "Epoch: 9064 mean train loss:  3.14836561e-05, mean val. rec. loss:  3.34898282e-05\n",
      "Epoch: 9065 mean train loss:  3.18248849e-05, mean val. rec. loss:  3.29521280e-05\n",
      "Epoch: 9066 mean train loss:  3.14343552e-05, mean val. rec. loss:  3.27329038e-05\n",
      "Epoch: 9067 mean train loss:  3.14739296e-05, mean val. rec. loss:  3.26775900e-05\n",
      "Epoch: 9068 mean train loss:  3.14232023e-05, mean val. rec. loss:  3.38733105e-05\n",
      "Epoch: 9069 mean train loss:  3.19096238e-05, mean val. rec. loss:  3.33031390e-05\n",
      "Epoch: 9070 mean train loss:  3.25359270e-05, mean val. rec. loss:  3.30997336e-05\n",
      "Epoch: 9071 mean train loss:  3.19913134e-05, mean val. rec. loss:  3.23677776e-05\n",
      "Epoch: 9072 mean train loss:  3.17147860e-05, mean val. rec. loss:  3.27642098e-05\n",
      "Epoch: 9073 mean train loss:  3.14889270e-05, mean val. rec. loss:  3.41324113e-05\n",
      "Epoch: 9074 mean train loss:  3.17828970e-05, mean val. rec. loss:  3.33313353e-05\n",
      "Epoch: 9075 mean train loss:  3.15954848e-05, mean val. rec. loss:  3.25099474e-05\n",
      "Epoch: 9076 mean train loss:  3.13459603e-05, mean val. rec. loss:  3.38083360e-05\n",
      "Epoch: 9077 mean train loss:  3.22823866e-05, mean val. rec. loss:  3.29896308e-05\n",
      "Epoch: 9078 mean train loss:  3.19525992e-05, mean val. rec. loss:  3.34303531e-05\n",
      "Epoch: 9079 mean train loss:  3.22882879e-05, mean val. rec. loss:  3.53398846e-05\n",
      "Epoch: 9080 mean train loss:  3.20087475e-05, mean val. rec. loss:  3.38495982e-05\n",
      "Epoch: 9081 mean train loss:  3.15279990e-05, mean val. rec. loss:  3.28749237e-05\n",
      "Epoch: 9082 mean train loss:  3.15821210e-05, mean val. rec. loss:  3.26878209e-05\n",
      "Epoch: 9083 mean train loss:  3.17597998e-05, mean val. rec. loss:  3.41716836e-05\n",
      "Epoch: 9084 mean train loss:  3.19644762e-05, mean val. rec. loss:  3.34601463e-05\n",
      "Epoch: 9085 mean train loss:  3.14787084e-05, mean val. rec. loss:  3.28800596e-05\n",
      "Epoch: 9086 mean train loss:  3.18231846e-05, mean val. rec. loss:  3.28490874e-05\n",
      "Epoch: 9087 mean train loss:  3.14367059e-05, mean val. rec. loss:  3.29929949e-05\n",
      "Epoch: 9088 mean train loss:  3.17460465e-05, mean val. rec. loss:  3.22878951e-05\n",
      "Epoch: 9089 mean train loss:  3.14362841e-05, mean val. rec. loss:  3.45083635e-05\n",
      "Epoch: 9090 mean train loss:  3.16780567e-05, mean val. rec. loss:  3.31856765e-05\n",
      "Epoch: 9091 mean train loss:  3.18989629e-05, mean val. rec. loss:  3.37023402e-05\n",
      "Epoch: 9092 mean train loss:  3.15617133e-05, mean val. rec. loss:  3.20710128e-05\n",
      "Epoch: 9093 mean train loss:  3.16160168e-05, mean val. rec. loss:  3.32130460e-05\n",
      "Epoch: 9094 mean train loss:  3.14631454e-05, mean val. rec. loss:  3.27139366e-05\n",
      "Epoch: 9095 mean train loss:  3.14428474e-05, mean val. rec. loss:  3.29971541e-05\n",
      "Epoch: 9096 mean train loss:  3.16459019e-05, mean val. rec. loss:  3.23267835e-05\n",
      "Epoch: 9097 mean train loss:  3.16042418e-05, mean val. rec. loss:  3.22997956e-05\n",
      "Epoch: 9098 mean train loss:  3.12252406e-05, mean val. rec. loss:  3.30266020e-05\n",
      "Epoch: 9099 mean train loss:  3.10960213e-05, mean val. rec. loss:  3.22202266e-05\n",
      "Epoch: 9100 mean train loss:  3.11462165e-05, mean val. rec. loss:  3.34070314e-05\n",
      "Epoch: 9101 mean train loss:  3.14568171e-05, mean val. rec. loss:  3.27377421e-05\n",
      "Epoch: 9102 mean train loss:  3.18080670e-05, mean val. rec. loss:  3.32019814e-05\n",
      "Epoch: 9103 mean train loss:  3.09733602e-05, mean val. rec. loss:  3.18405059e-05\n",
      "Epoch: 9104 mean train loss:  3.11372221e-05, mean val. rec. loss:  3.28913649e-05\n",
      "Epoch: 9105 mean train loss:  3.10451101e-05, mean val. rec. loss:  3.21771950e-05\n",
      "Epoch: 9106 mean train loss:  3.09195976e-05, mean val. rec. loss:  3.33274692e-05\n",
      "Epoch: 9107 mean train loss:  3.18375717e-05, mean val. rec. loss:  3.36649169e-05\n",
      "Epoch: 9108 mean train loss:  3.14714904e-05, mean val. rec. loss:  3.27445817e-05\n",
      "Epoch: 9109 mean train loss:  3.12205714e-05, mean val. rec. loss:  3.18377301e-05\n",
      "Epoch: 9110 mean train loss:  3.13078191e-05, mean val. rec. loss:  3.40356423e-05\n",
      "Epoch: 9111 mean train loss:  3.13170455e-05, mean val. rec. loss:  3.22653435e-05\n",
      "Epoch: 9112 mean train loss:  3.12440261e-05, mean val. rec. loss:  3.29861917e-05\n",
      "Epoch: 9113 mean train loss:  3.14877598e-05, mean val. rec. loss:  3.25276448e-05\n",
      "Epoch: 9114 mean train loss:  3.13851694e-05, mean val. rec. loss:  3.38607672e-05\n",
      "Epoch: 9115 mean train loss:  3.19726488e-05, mean val. rec. loss:  3.41204109e-05\n",
      "Epoch: 9116 mean train loss:  3.22380377e-05, mean val. rec. loss:  3.30276583e-05\n",
      "Epoch: 9117 mean train loss:  3.15852911e-05, mean val. rec. loss:  3.31955803e-05\n",
      "Epoch: 9118 mean train loss:  3.14963606e-05, mean val. rec. loss:  3.24508766e-05\n",
      "Epoch: 9119 mean train loss:  3.15913400e-05, mean val. rec. loss:  3.63343781e-05\n",
      "Epoch: 9120 mean train loss:  3.24056820e-05, mean val. rec. loss:  3.21935726e-05\n",
      "Epoch: 9121 mean train loss:  3.11739210e-05, mean val. rec. loss:  3.24571551e-05\n",
      "Epoch: 9122 mean train loss:  3.12108348e-05, mean val. rec. loss:  3.31034520e-05\n",
      "Epoch: 9123 mean train loss:  3.14236687e-05, mean val. rec. loss:  3.25651158e-05\n",
      "Epoch: 9124 mean train loss:  3.19929573e-05, mean val. rec. loss:  3.26640995e-05\n",
      "Epoch: 9125 mean train loss:  3.14901683e-05, mean val. rec. loss:  3.39335671e-05\n",
      "Epoch: 9126 mean train loss:  3.12779919e-05, mean val. rec. loss:  3.23426501e-05\n",
      "Epoch: 9127 mean train loss:  3.12784135e-05, mean val. rec. loss:  3.24252493e-05\n",
      "Epoch: 9128 mean train loss:  3.12238412e-05, mean val. rec. loss:  3.25338688e-05\n",
      "Epoch: 9129 mean train loss:  3.12294769e-05, mean val. rec. loss:  3.22115994e-05\n",
      "Epoch: 9130 mean train loss:  3.06825968e-05, mean val. rec. loss:  3.21818243e-05\n",
      "Epoch: 9131 mean train loss:  3.08136355e-05, mean val. rec. loss:  3.22186661e-05\n",
      "Epoch: 9132 mean train loss:  3.12188208e-05, mean val. rec. loss:  3.22372630e-05\n",
      "Epoch: 9133 mean train loss:  3.15179267e-05, mean val. rec. loss:  3.20371104e-05\n",
      "Epoch: 9134 mean train loss:  3.10686955e-05, mean val. rec. loss:  3.32232383e-05\n",
      "Epoch: 9135 mean train loss:  3.10620631e-05, mean val. rec. loss:  3.14420907e-05\n",
      "Epoch: 9136 mean train loss:  3.08871102e-05, mean val. rec. loss:  3.24815966e-05\n",
      "Epoch: 9137 mean train loss:  3.08849863e-05, mean val. rec. loss:  3.19853561e-05\n",
      "Epoch: 9138 mean train loss:  3.12261833e-05, mean val. rec. loss:  3.19659755e-05\n",
      "Epoch: 9139 mean train loss:  3.09467831e-05, mean val. rec. loss:  3.25049092e-05\n",
      "Epoch: 9140 mean train loss:  3.08216952e-05, mean val. rec. loss:  3.18036619e-05\n",
      "Epoch: 9141 mean train loss:  3.10892540e-05, mean val. rec. loss:  3.30483359e-05\n",
      "Epoch: 9142 mean train loss:  3.11447483e-05, mean val. rec. loss:  3.30306703e-05\n",
      "Epoch: 9143 mean train loss:  3.10637968e-05, mean val. rec. loss:  3.28961215e-05\n",
      "Epoch: 9144 mean train loss:  3.11791562e-05, mean val. rec. loss:  3.22067042e-05\n",
      "Epoch: 9145 mean train loss:  3.06547556e-05, mean val. rec. loss:  3.19523691e-05\n",
      "Epoch: 9146 mean train loss:  3.07014788e-05, mean val. rec. loss:  3.29211627e-05\n",
      "Epoch: 9147 mean train loss:  3.13908707e-05, mean val. rec. loss:  3.18877172e-05\n",
      "Epoch: 9148 mean train loss:  3.07294327e-05, mean val. rec. loss:  3.20504942e-05\n",
      "Epoch: 9149 mean train loss:  3.05431130e-05, mean val. rec. loss:  3.19599606e-05\n",
      "Epoch: 9150 mean train loss:  3.05484609e-05, mean val. rec. loss:  3.20556142e-05\n",
      "Epoch: 9151 mean train loss:  3.08033500e-05, mean val. rec. loss:  3.16811180e-05\n",
      "Epoch: 9152 mean train loss:  3.09103872e-05, mean val. rec. loss:  3.19667706e-05\n",
      "Epoch: 9153 mean train loss:  3.06381103e-05, mean val. rec. loss:  3.28969188e-05\n",
      "Epoch: 9154 mean train loss:  3.10102810e-05, mean val. rec. loss:  3.25173412e-05\n",
      "Epoch: 9155 mean train loss:  3.06379969e-05, mean val. rec. loss:  3.25211301e-05\n",
      "Epoch: 9156 mean train loss:  3.14888654e-05, mean val. rec. loss:  3.30460099e-05\n",
      "Epoch: 9157 mean train loss:  3.11168322e-05, mean val. rec. loss:  3.27057955e-05\n",
      "Epoch: 9158 mean train loss:  3.08993101e-05, mean val. rec. loss:  3.22390257e-05\n",
      "Epoch: 9159 mean train loss:  3.07760386e-05, mean val. rec. loss:  3.15679714e-05\n",
      "Epoch: 9160 mean train loss:  3.10211318e-05, mean val. rec. loss:  3.28008744e-05\n",
      "Epoch: 9161 mean train loss:  3.11501359e-05, mean val. rec. loss:  3.29763311e-05\n",
      "Epoch: 9162 mean train loss:  3.12080229e-05, mean val. rec. loss:  3.24941672e-05\n",
      "Epoch: 9163 mean train loss:  3.12396489e-05, mean val. rec. loss:  3.11352790e-05\n",
      "Epoch: 9164 mean train loss:  3.08355011e-05, mean val. rec. loss:  3.31116749e-05\n",
      "Epoch: 9165 mean train loss:  3.07343972e-05, mean val. rec. loss:  3.18402152e-05\n",
      "Epoch: 9166 mean train loss:  3.09795561e-05, mean val. rec. loss:  3.35771545e-05\n",
      "Epoch: 9167 mean train loss:  3.08512578e-05, mean val. rec. loss:  3.15146816e-05\n",
      "Epoch: 9168 mean train loss:  3.10782630e-05, mean val. rec. loss:  3.34844720e-05\n",
      "Epoch: 9169 mean train loss:  3.09150908e-05, mean val. rec. loss:  3.15339872e-05\n",
      "Epoch: 9170 mean train loss:  3.07685946e-05, mean val. rec. loss:  3.35216000e-05\n",
      "Epoch: 9171 mean train loss:  3.06066873e-05, mean val. rec. loss:  3.23104241e-05\n",
      "Epoch: 9172 mean train loss:  3.12012502e-05, mean val. rec. loss:  3.28576419e-05\n",
      "Epoch: 9173 mean train loss:  3.10029487e-05, mean val. rec. loss:  3.09999169e-05\n",
      "Epoch: 9174 mean train loss:  3.07226155e-05, mean val. rec. loss:  3.34592286e-05\n",
      "Epoch: 9175 mean train loss:  3.08663255e-05, mean val. rec. loss:  3.16701330e-05\n",
      "Epoch: 9176 mean train loss:  3.04232844e-05, mean val. rec. loss:  3.20106745e-05\n",
      "Epoch: 9177 mean train loss:  3.03884185e-05, mean val. rec. loss:  3.23438017e-05\n",
      "Epoch: 9178 mean train loss:  3.05500510e-05, mean val. rec. loss:  3.19872869e-05\n",
      "Epoch: 9179 mean train loss:  3.07022501e-05, mean val. rec. loss:  3.22514418e-05\n",
      "Epoch: 9180 mean train loss:  3.06886758e-05, mean val. rec. loss:  3.19570848e-05\n",
      "Epoch: 9181 mean train loss:  3.10056941e-05, mean val. rec. loss:  3.35108171e-05\n",
      "Epoch: 9182 mean train loss:  3.07194269e-05, mean val. rec. loss:  3.15765691e-05\n",
      "Epoch: 9183 mean train loss:  3.06120474e-05, mean val. rec. loss:  3.19334792e-05\n",
      "Epoch: 9184 mean train loss:  3.06911581e-05, mean val. rec. loss:  3.24307532e-05\n",
      "Epoch: 9185 mean train loss:  3.08084642e-05, mean val. rec. loss:  3.15498697e-05\n",
      "Epoch: 9186 mean train loss:  3.03279383e-05, mean val. rec. loss:  3.16596431e-05\n",
      "Epoch: 9187 mean train loss:  3.04248487e-05, mean val. rec. loss:  3.12535547e-05\n",
      "Epoch: 9188 mean train loss:  3.09781345e-05, mean val. rec. loss:  3.26250430e-05\n",
      "Epoch: 9189 mean train loss:  3.11587494e-05, mean val. rec. loss:  3.11433497e-05\n",
      "Epoch: 9190 mean train loss:  3.06707889e-05, mean val. rec. loss:  3.27792269e-05\n",
      "Epoch: 9191 mean train loss:  3.01718943e-05, mean val. rec. loss:  3.15517732e-05\n",
      "Epoch: 9192 mean train loss:  3.03545354e-05, mean val. rec. loss:  3.18115236e-05\n",
      "Epoch: 9193 mean train loss:  3.03985620e-05, mean val. rec. loss:  3.14724791e-05\n",
      "Epoch: 9194 mean train loss:  3.00770844e-05, mean val. rec. loss:  3.22704908e-05\n",
      "Epoch: 9195 mean train loss:  3.05028427e-05, mean val. rec. loss:  3.20762782e-05\n",
      "Epoch: 9196 mean train loss:  3.04418161e-05, mean val. rec. loss:  3.14401781e-05\n",
      "Epoch: 9197 mean train loss:  3.01734256e-05, mean val. rec. loss:  3.19450889e-05\n",
      "Epoch: 9198 mean train loss:  3.09385905e-05, mean val. rec. loss:  3.13069263e-05\n",
      "Epoch: 9199 mean train loss:  3.09065989e-05, mean val. rec. loss:  3.10161878e-05\n",
      "Epoch: 9200 mean train loss:  3.01619877e-05, mean val. rec. loss:  3.20897574e-05\n",
      "Epoch: 9201 mean train loss:  3.03372735e-05, mean val. rec. loss:  3.27245082e-05\n",
      "Epoch: 9202 mean train loss:  3.07427799e-05, mean val. rec. loss:  3.18462847e-05\n",
      "Epoch: 9203 mean train loss:  3.05676694e-05, mean val. rec. loss:  3.12313119e-05\n",
      "Epoch: 9204 mean train loss:  3.08754464e-05, mean val. rec. loss:  3.35035618e-05\n",
      "Epoch: 9205 mean train loss:  3.05768699e-05, mean val. rec. loss:  3.14515129e-05\n",
      "Epoch: 9206 mean train loss:  3.06406801e-05, mean val. rec. loss:  3.31186440e-05\n",
      "Epoch: 9207 mean train loss:  3.06787757e-05, mean val. rec. loss:  3.12611143e-05\n",
      "Epoch: 9208 mean train loss:  3.04503729e-05, mean val. rec. loss:  3.19034134e-05\n",
      "Epoch: 9209 mean train loss:  3.06849964e-05, mean val. rec. loss:  3.17390440e-05\n",
      "Epoch: 9210 mean train loss:  3.08722307e-05, mean val. rec. loss:  3.19282138e-05\n",
      "Epoch: 9211 mean train loss:  3.06349069e-05, mean val. rec. loss:  3.24411386e-05\n",
      "Epoch: 9212 mean train loss:  3.06684214e-05, mean val. rec. loss:  3.27446430e-05\n",
      "Epoch: 9213 mean train loss:  3.04117920e-05, mean val. rec. loss:  3.12750682e-05\n",
      "Epoch: 9214 mean train loss:  3.02191967e-05, mean val. rec. loss:  3.11941181e-05\n",
      "Epoch: 9215 mean train loss:  3.01665295e-05, mean val. rec. loss:  3.17808468e-05\n",
      "Epoch: 9216 mean train loss:  3.01226632e-05, mean val. rec. loss:  3.10549240e-05\n",
      "Epoch: 9217 mean train loss:  3.01109051e-05, mean val. rec. loss:  3.17935786e-05\n",
      "Epoch: 9218 mean train loss:  3.03649827e-05, mean val. rec. loss:  3.12696007e-05\n",
      "Epoch: 9219 mean train loss:  3.02476350e-05, mean val. rec. loss:  3.19421246e-05\n",
      "Epoch: 9220 mean train loss:  3.05369305e-05, mean val. rec. loss:  3.16586504e-05\n",
      "Epoch: 9221 mean train loss:  3.01401534e-05, mean val. rec. loss:  3.12392327e-05\n",
      "Epoch: 9222 mean train loss:  3.00617694e-05, mean val. rec. loss:  3.18755964e-05\n",
      "Epoch: 9223 mean train loss:  3.03924224e-05, mean val. rec. loss:  3.09644767e-05\n",
      "Epoch: 9224 mean train loss:  3.04328926e-05, mean val. rec. loss:  3.29019593e-05\n",
      "Epoch: 9225 mean train loss:  3.05596929e-05, mean val. rec. loss:  3.21552226e-05\n",
      "Epoch: 9226 mean train loss:  3.04403628e-05, mean val. rec. loss:  3.23902043e-05\n",
      "Epoch: 9227 mean train loss:  3.02637642e-05, mean val. rec. loss:  3.08158648e-05\n",
      "Epoch: 9228 mean train loss:  3.05070112e-05, mean val. rec. loss:  3.28665281e-05\n",
      "Epoch: 9229 mean train loss:  3.03205960e-05, mean val. rec. loss:  3.16631435e-05\n",
      "Epoch: 9230 mean train loss:  3.03118451e-05, mean val. rec. loss:  3.16943610e-05\n",
      "Epoch: 9231 mean train loss:  3.03569482e-05, mean val. rec. loss:  3.12792137e-05\n",
      "Epoch: 9232 mean train loss:  3.01612251e-05, mean val. rec. loss:  3.22147159e-05\n",
      "Epoch: 9233 mean train loss:  3.02730579e-05, mean val. rec. loss:  3.03962086e-05\n",
      "Epoch: 9234 mean train loss:  3.00178286e-05, mean val. rec. loss:  3.31670000e-05\n",
      "Epoch: 9235 mean train loss:  3.05289257e-05, mean val. rec. loss:  3.10478278e-05\n",
      "Epoch: 9236 mean train loss:  3.02601611e-05, mean val. rec. loss:  3.17638717e-05\n",
      "Epoch: 9237 mean train loss:  3.04142273e-05, mean val. rec. loss:  3.18181769e-05\n",
      "Epoch: 9238 mean train loss:  3.08128126e-05, mean val. rec. loss:  3.17111134e-05\n",
      "Epoch: 9239 mean train loss:  3.00175571e-05, mean val. rec. loss:  3.14370752e-05\n",
      "Epoch: 9240 mean train loss:  3.00015157e-05, mean val. rec. loss:  3.17573502e-05\n",
      "Epoch: 9241 mean train loss:  3.00714395e-05, mean val. rec. loss:  3.15281063e-05\n",
      "Epoch: 9242 mean train loss:  3.02916298e-05, mean val. rec. loss:  3.09324074e-05\n",
      "Epoch: 9243 mean train loss:  3.00540265e-05, mean val. rec. loss:  3.14489847e-05\n",
      "Epoch: 9244 mean train loss:  3.05358703e-05, mean val. rec. loss:  3.07473059e-05\n",
      "Epoch: 9245 mean train loss:  3.01454529e-05, mean val. rec. loss:  3.19050511e-05\n",
      "Epoch: 9246 mean train loss:  2.99502533e-05, mean val. rec. loss:  3.07251540e-05\n",
      "Epoch: 9247 mean train loss:  2.98690157e-05, mean val. rec. loss:  3.16777244e-05\n",
      "Epoch: 9248 mean train loss:  2.99562867e-05, mean val. rec. loss:  3.18580284e-05\n",
      "Epoch: 9249 mean train loss:  3.05780928e-05, mean val. rec. loss:  3.23538169e-05\n",
      "Epoch: 9250 mean train loss:  3.04149710e-05, mean val. rec. loss:  3.08036940e-05\n",
      "Epoch: 9251 mean train loss:  2.99764904e-05, mean val. rec. loss:  3.23245529e-05\n",
      "Epoch: 9252 mean train loss:  3.02782146e-05, mean val. rec. loss:  3.09395763e-05\n",
      "Epoch: 9253 mean train loss:  2.98635229e-05, mean val. rec. loss:  3.16737447e-05\n",
      "Epoch: 9254 mean train loss:  3.03363043e-05, mean val. rec. loss:  3.06626328e-05\n",
      "Epoch: 9255 mean train loss:  2.99963004e-05, mean val. rec. loss:  3.07497727e-05\n",
      "Epoch: 9256 mean train loss:  2.99258634e-05, mean val. rec. loss:  3.08303639e-05\n",
      "Epoch: 9257 mean train loss:  2.97607151e-05, mean val. rec. loss:  3.14891884e-05\n",
      "Epoch: 9258 mean train loss:  2.98931270e-05, mean val. rec. loss:  3.18201259e-05\n",
      "Epoch: 9259 mean train loss:  2.96053792e-05, mean val. rec. loss:  3.03450290e-05\n",
      "Epoch: 9260 mean train loss:  2.98632060e-05, mean val. rec. loss:  3.11253183e-05\n",
      "Epoch: 9261 mean train loss:  2.98258420e-05, mean val. rec. loss:  3.14960211e-05\n",
      "Epoch: 9262 mean train loss:  2.96676779e-05, mean val. rec. loss:  3.07864146e-05\n",
      "Epoch: 9263 mean train loss:  2.99914286e-05, mean val. rec. loss:  3.16399717e-05\n",
      "Epoch: 9264 mean train loss:  2.97842385e-05, mean val. rec. loss:  3.10499857e-05\n",
      "Epoch: 9265 mean train loss:  2.98818621e-05, mean val. rec. loss:  3.06734815e-05\n",
      "Epoch: 9266 mean train loss:  2.99243684e-05, mean val. rec. loss:  3.12168446e-05\n",
      "Epoch: 9267 mean train loss:  3.01755098e-05, mean val. rec. loss:  3.16151895e-05\n",
      "Epoch: 9268 mean train loss:  2.98373780e-05, mean val. rec. loss:  3.12554900e-05\n",
      "Epoch: 9269 mean train loss:  2.96089079e-05, mean val. rec. loss:  3.07635245e-05\n",
      "Epoch: 9270 mean train loss:  2.95719662e-05, mean val. rec. loss:  3.10107657e-05\n",
      "Epoch: 9271 mean train loss:  2.99098198e-05, mean val. rec. loss:  3.08103269e-05\n",
      "Epoch: 9272 mean train loss:  2.97396966e-05, mean val. rec. loss:  3.13484224e-05\n",
      "Epoch: 9273 mean train loss:  3.02561397e-05, mean val. rec. loss:  3.11369803e-05\n",
      "Epoch: 9274 mean train loss:  3.06332404e-05, mean val. rec. loss:  3.11012357e-05\n",
      "Epoch: 9275 mean train loss:  3.01334562e-05, mean val. rec. loss:  3.12004784e-05\n",
      "Epoch: 9276 mean train loss:  2.99802992e-05, mean val. rec. loss:  3.05537044e-05\n",
      "Epoch: 9277 mean train loss:  3.00018358e-05, mean val. rec. loss:  3.13837944e-05\n",
      "Epoch: 9278 mean train loss:  3.00003714e-05, mean val. rec. loss:  3.18623852e-05\n",
      "Epoch: 9279 mean train loss:  3.03933547e-05, mean val. rec. loss:  3.16942474e-05\n",
      "Epoch: 9280 mean train loss:  2.97085138e-05, mean val. rec. loss:  3.06862883e-05\n",
      "Epoch: 9281 mean train loss:  3.01207688e-05, mean val. rec. loss:  3.10621679e-05\n",
      "Epoch: 9282 mean train loss:  3.04944673e-05, mean val. rec. loss:  3.25868451e-05\n",
      "Epoch: 9283 mean train loss:  3.06744992e-05, mean val. rec. loss:  3.20106245e-05\n",
      "Epoch: 9284 mean train loss:  3.00993235e-05, mean val. rec. loss:  3.20888647e-05\n",
      "Epoch: 9285 mean train loss:  3.00671324e-05, mean val. rec. loss:  3.09635135e-05\n",
      "Epoch: 9286 mean train loss:  3.04101669e-05, mean val. rec. loss:  3.16922530e-05\n",
      "Epoch: 9287 mean train loss:  2.99591785e-05, mean val. rec. loss:  3.16003951e-05\n",
      "Epoch: 9288 mean train loss:  3.01653169e-05, mean val. rec. loss:  3.22233931e-05\n",
      "Epoch: 9289 mean train loss:  2.99931592e-05, mean val. rec. loss:  3.06620989e-05\n",
      "Epoch: 9290 mean train loss:  2.93570365e-05, mean val. rec. loss:  3.18532741e-05\n",
      "Epoch: 9291 mean train loss:  3.07917665e-05, mean val. rec. loss:  3.01155012e-05\n",
      "Epoch: 9292 mean train loss:  2.97091472e-05, mean val. rec. loss:  3.16851931e-05\n",
      "Epoch: 9293 mean train loss:  2.97184707e-05, mean val. rec. loss:  3.09842571e-05\n",
      "Epoch: 9294 mean train loss:  2.95713635e-05, mean val. rec. loss:  3.23388044e-05\n",
      "Epoch: 9295 mean train loss:  3.00755037e-05, mean val. rec. loss:  3.06220157e-05\n",
      "Epoch: 9296 mean train loss:  2.93459429e-05, mean val. rec. loss:  3.13059109e-05\n",
      "Epoch: 9297 mean train loss:  2.99139000e-05, mean val. rec. loss:  3.01586532e-05\n",
      "Epoch: 9298 mean train loss:  2.97175858e-05, mean val. rec. loss:  3.15305482e-05\n",
      "Epoch: 9299 mean train loss:  2.97125595e-05, mean val. rec. loss:  3.01403789e-05\n",
      "Epoch: 9300 mean train loss:  2.95426107e-05, mean val. rec. loss:  3.13174252e-05\n",
      "Epoch: 9301 mean train loss:  2.96672346e-05, mean val. rec. loss:  3.12454839e-05\n",
      "Epoch: 9302 mean train loss:  2.96043248e-05, mean val. rec. loss:  3.10328244e-05\n",
      "Epoch: 9303 mean train loss:  2.96803359e-05, mean val. rec. loss:  3.04749758e-05\n",
      "Epoch: 9304 mean train loss:  2.96847622e-05, mean val. rec. loss:  3.07181032e-05\n",
      "Epoch: 9305 mean train loss:  2.99502531e-05, mean val. rec. loss:  3.29070974e-05\n",
      "Epoch: 9306 mean train loss:  3.03984277e-05, mean val. rec. loss:  3.06486175e-05\n",
      "Epoch: 9307 mean train loss:  3.01649429e-05, mean val. rec. loss:  3.27804785e-05\n",
      "Epoch: 9308 mean train loss:  2.98893718e-05, mean val. rec. loss:  3.09050333e-05\n",
      "Epoch: 9309 mean train loss:  3.00514438e-05, mean val. rec. loss:  3.21293431e-05\n",
      "Epoch: 9310 mean train loss:  3.00747908e-05, mean val. rec. loss:  3.12184847e-05\n",
      "Epoch: 9311 mean train loss:  2.98660040e-05, mean val. rec. loss:  3.13113148e-05\n",
      "Epoch: 9312 mean train loss:  2.97787188e-05, mean val. rec. loss:  3.15528590e-05\n",
      "Epoch: 9313 mean train loss:  2.97757686e-05, mean val. rec. loss:  3.01744971e-05\n",
      "Epoch: 9314 mean train loss:  2.95728352e-05, mean val. rec. loss:  3.13744085e-05\n",
      "Epoch: 9315 mean train loss:  2.97554029e-05, mean val. rec. loss:  3.00477600e-05\n",
      "Epoch: 9316 mean train loss:  2.95453860e-05, mean val. rec. loss:  3.14115138e-05\n",
      "Epoch: 9317 mean train loss:  2.92727897e-05, mean val. rec. loss:  3.07189869e-05\n",
      "Epoch: 9318 mean train loss:  2.95165946e-05, mean val. rec. loss:  3.10268299e-05\n",
      "Epoch: 9319 mean train loss:  2.94083421e-05, mean val. rec. loss:  3.00121653e-05\n",
      "Epoch: 9320 mean train loss:  2.92539457e-05, mean val. rec. loss:  3.04654104e-05\n",
      "Epoch: 9321 mean train loss:  2.94877311e-05, mean val. rec. loss:  3.03726098e-05\n",
      "Epoch: 9322 mean train loss:  2.93671795e-05, mean val. rec. loss:  3.07096214e-05\n",
      "Epoch: 9323 mean train loss:  2.95407109e-05, mean val. rec. loss:  3.11948677e-05\n",
      "Epoch: 9324 mean train loss:  2.96143642e-05, mean val. rec. loss:  3.01638845e-05\n",
      "Epoch: 9325 mean train loss:  2.94718035e-05, mean val. rec. loss:  3.15272658e-05\n",
      "Epoch: 9326 mean train loss:  2.95627291e-05, mean val. rec. loss:  2.99795326e-05\n",
      "Epoch: 9327 mean train loss:  2.94321605e-05, mean val. rec. loss:  3.09001745e-05\n",
      "Epoch: 9328 mean train loss:  2.96204569e-05, mean val. rec. loss:  2.96990773e-05\n",
      "Epoch: 9329 mean train loss:  2.94289900e-05, mean val. rec. loss:  3.06738745e-05\n",
      "Epoch: 9330 mean train loss:  2.96411376e-05, mean val. rec. loss:  3.05373585e-05\n",
      "Epoch: 9331 mean train loss:  2.91594112e-05, mean val. rec. loss:  3.07181350e-05\n",
      "Epoch: 9332 mean train loss:  2.93957802e-05, mean val. rec. loss:  3.04646086e-05\n",
      "Epoch: 9333 mean train loss:  2.94185935e-05, mean val. rec. loss:  3.14084745e-05\n",
      "Epoch: 9334 mean train loss:  2.96221787e-05, mean val. rec. loss:  3.04905334e-05\n",
      "Epoch: 9335 mean train loss:  2.93585873e-05, mean val. rec. loss:  2.97155277e-05\n",
      "Epoch: 9336 mean train loss:  2.92504752e-05, mean val. rec. loss:  3.03986369e-05\n",
      "Epoch: 9337 mean train loss:  2.91803794e-05, mean val. rec. loss:  3.11721661e-05\n",
      "Epoch: 9338 mean train loss:  2.96477993e-05, mean val. rec. loss:  3.14350490e-05\n",
      "Epoch: 9339 mean train loss:  2.96164311e-05, mean val. rec. loss:  3.11175384e-05\n",
      "Epoch: 9340 mean train loss:  3.04448959e-05, mean val. rec. loss:  3.03448428e-05\n",
      "Epoch: 9341 mean train loss:  2.95700114e-05, mean val. rec. loss:  3.09658827e-05\n",
      "Epoch: 9342 mean train loss:  2.94835582e-05, mean val. rec. loss:  3.12535024e-05\n",
      "Epoch: 9343 mean train loss:  2.92736487e-05, mean val. rec. loss:  2.99303225e-05\n",
      "Epoch: 9344 mean train loss:  2.92224248e-05, mean val. rec. loss:  3.03111834e-05\n",
      "Epoch: 9345 mean train loss:  2.94587803e-05, mean val. rec. loss:  3.10418514e-05\n",
      "Epoch: 9346 mean train loss:  2.97431057e-05, mean val. rec. loss:  2.93845856e-05\n",
      "Epoch: 9347 mean train loss:  2.95054488e-05, mean val. rec. loss:  3.16496439e-05\n",
      "Epoch: 9348 mean train loss:  2.93622433e-05, mean val. rec. loss:  3.04501867e-05\n",
      "Epoch: 9349 mean train loss:  2.94485264e-05, mean val. rec. loss:  3.13847848e-05\n",
      "Epoch: 9350 mean train loss:  2.95125310e-05, mean val. rec. loss:  3.10830976e-05\n",
      "Epoch: 9351 mean train loss:  2.95630581e-05, mean val. rec. loss:  3.06570130e-05\n",
      "Epoch: 9352 mean train loss:  2.93568885e-05, mean val. rec. loss:  3.08592645e-05\n",
      "Epoch: 9353 mean train loss:  2.94573407e-05, mean val. rec. loss:  3.03296736e-05\n",
      "Epoch: 9354 mean train loss:  2.95562251e-05, mean val. rec. loss:  3.17207742e-05\n",
      "Epoch: 9355 mean train loss:  3.01436100e-05, mean val. rec. loss:  3.00754475e-05\n",
      "Epoch: 9356 mean train loss:  2.95787096e-05, mean val. rec. loss:  3.18819475e-05\n",
      "Epoch: 9357 mean train loss:  2.96972527e-05, mean val. rec. loss:  3.35080254e-05\n",
      "Epoch: 9358 mean train loss:  3.04363643e-05, mean val. rec. loss:  3.13102449e-05\n",
      "Epoch: 9359 mean train loss:  3.02695637e-05, mean val. rec. loss:  3.03447837e-05\n",
      "Epoch: 9360 mean train loss:  2.90822771e-05, mean val. rec. loss:  3.00120290e-05\n",
      "Epoch: 9361 mean train loss:  2.90778737e-05, mean val. rec. loss:  3.04809476e-05\n",
      "Epoch: 9362 mean train loss:  2.87786083e-05, mean val. rec. loss:  3.06144311e-05\n",
      "Epoch: 9363 mean train loss:  2.91691477e-05, mean val. rec. loss:  3.11472612e-05\n",
      "Epoch: 9364 mean train loss:  2.92193295e-05, mean val. rec. loss:  2.98164921e-05\n",
      "Epoch: 9365 mean train loss:  2.89248573e-05, mean val. rec. loss:  2.99162527e-05\n",
      "Epoch: 9366 mean train loss:  2.90918140e-05, mean val. rec. loss:  3.01835831e-05\n",
      "Epoch: 9367 mean train loss:  2.87893260e-05, mean val. rec. loss:  3.00310916e-05\n",
      "Epoch: 9368 mean train loss:  2.88805732e-05, mean val. rec. loss:  2.96838763e-05\n",
      "Epoch: 9369 mean train loss:  2.89214864e-05, mean val. rec. loss:  3.11223517e-05\n",
      "Epoch: 9370 mean train loss:  2.94390610e-05, mean val. rec. loss:  3.01825132e-05\n",
      "Epoch: 9371 mean train loss:  2.94187451e-05, mean val. rec. loss:  3.07911689e-05\n",
      "Epoch: 9372 mean train loss:  2.93140262e-05, mean val. rec. loss:  3.00147957e-05\n",
      "Epoch: 9373 mean train loss:  2.91187509e-05, mean val. rec. loss:  3.11490285e-05\n",
      "Epoch: 9374 mean train loss:  2.99222580e-05, mean val. rec. loss:  3.00528845e-05\n",
      "Epoch: 9375 mean train loss:  2.92947874e-05, mean val. rec. loss:  2.99282849e-05\n",
      "Epoch: 9376 mean train loss:  2.88813204e-05, mean val. rec. loss:  2.97874939e-05\n",
      "Epoch: 9377 mean train loss:  2.87775470e-05, mean val. rec. loss:  3.04332821e-05\n",
      "Epoch: 9378 mean train loss:  2.86958127e-05, mean val. rec. loss:  2.97442760e-05\n",
      "Epoch: 9379 mean train loss:  2.88567610e-05, mean val. rec. loss:  3.04671300e-05\n",
      "Epoch: 9380 mean train loss:  2.91087431e-05, mean val. rec. loss:  2.95937175e-05\n",
      "Epoch: 9381 mean train loss:  2.90986321e-05, mean val. rec. loss:  3.02364005e-05\n",
      "Epoch: 9382 mean train loss:  2.89098254e-05, mean val. rec. loss:  2.97287524e-05\n",
      "Epoch: 9383 mean train loss:  2.90497877e-05, mean val. rec. loss:  3.08839695e-05\n",
      "Epoch: 9384 mean train loss:  2.89990770e-05, mean val. rec. loss:  3.00336811e-05\n",
      "Epoch: 9385 mean train loss:  2.87480453e-05, mean val. rec. loss:  3.01355314e-05\n",
      "Epoch: 9386 mean train loss:  2.89066493e-05, mean val. rec. loss:  3.04024598e-05\n",
      "Epoch: 9387 mean train loss:  2.89623468e-05, mean val. rec. loss:  3.05566210e-05\n",
      "Epoch: 9388 mean train loss:  2.89429499e-05, mean val. rec. loss:  3.06469161e-05\n",
      "Epoch: 9389 mean train loss:  2.89204505e-05, mean val. rec. loss:  3.05058934e-05\n",
      "Epoch: 9390 mean train loss:  2.89401912e-05, mean val. rec. loss:  3.02778216e-05\n",
      "Epoch: 9391 mean train loss:  2.96200623e-05, mean val. rec. loss:  3.01918333e-05\n",
      "Epoch: 9392 mean train loss:  2.87552679e-05, mean val. rec. loss:  3.10714834e-05\n",
      "Epoch: 9393 mean train loss:  2.91278879e-05, mean val. rec. loss:  3.12531503e-05\n",
      "Epoch: 9394 mean train loss:  2.92202250e-05, mean val. rec. loss:  3.01625216e-05\n",
      "Epoch: 9395 mean train loss:  2.92339967e-05, mean val. rec. loss:  2.99254796e-05\n",
      "Epoch: 9396 mean train loss:  2.87333062e-05, mean val. rec. loss:  3.04837370e-05\n",
      "Epoch: 9397 mean train loss:  2.88102188e-05, mean val. rec. loss:  2.95835297e-05\n",
      "Epoch: 9398 mean train loss:  2.87829256e-05, mean val. rec. loss:  2.97984108e-05\n",
      "Epoch: 9399 mean train loss:  2.87808393e-05, mean val. rec. loss:  3.03437388e-05\n",
      "Epoch: 9400 mean train loss:  2.87446434e-05, mean val. rec. loss:  3.00763447e-05\n",
      "Epoch: 9401 mean train loss:  2.86705869e-05, mean val. rec. loss:  2.95545724e-05\n",
      "Epoch: 9402 mean train loss:  2.86734596e-05, mean val. rec. loss:  2.99294366e-05\n",
      "Epoch: 9403 mean train loss:  2.89347292e-05, mean val. rec. loss:  2.95732488e-05\n",
      "Epoch: 9404 mean train loss:  2.89775619e-05, mean val. rec. loss:  3.03788497e-05\n",
      "Epoch: 9405 mean train loss:  2.92859661e-05, mean val. rec. loss:  3.09888773e-05\n",
      "Epoch: 9406 mean train loss:  2.89846735e-05, mean val. rec. loss:  3.05174282e-05\n",
      "Epoch: 9407 mean train loss:  2.89334561e-05, mean val. rec. loss:  3.10588174e-05\n",
      "Epoch: 9408 mean train loss:  2.90460717e-05, mean val. rec. loss:  2.95951508e-05\n",
      "Epoch: 9409 mean train loss:  2.85437622e-05, mean val. rec. loss:  3.06396495e-05\n",
      "Epoch: 9410 mean train loss:  2.90733458e-05, mean val. rec. loss:  3.04785762e-05\n",
      "Epoch: 9411 mean train loss:  2.88746176e-05, mean val. rec. loss:  3.01503826e-05\n",
      "Epoch: 9412 mean train loss:  2.85380747e-05, mean val. rec. loss:  3.00269483e-05\n",
      "Epoch: 9413 mean train loss:  2.85590340e-05, mean val. rec. loss:  2.95500703e-05\n",
      "Epoch: 9414 mean train loss:  2.85261873e-05, mean val. rec. loss:  2.94937366e-05\n",
      "Epoch: 9415 mean train loss:  2.86076249e-05, mean val. rec. loss:  3.01618810e-05\n",
      "Epoch: 9416 mean train loss:  2.88397498e-05, mean val. rec. loss:  3.04179198e-05\n",
      "Epoch: 9417 mean train loss:  2.92239416e-05, mean val. rec. loss:  3.09772676e-05\n",
      "Epoch: 9418 mean train loss:  2.88827862e-05, mean val. rec. loss:  2.94106990e-05\n",
      "Epoch: 9419 mean train loss:  2.89353705e-05, mean val. rec. loss:  3.09718273e-05\n",
      "Epoch: 9420 mean train loss:  2.91345024e-05, mean val. rec. loss:  2.93440572e-05\n",
      "Epoch: 9421 mean train loss:  2.90827117e-05, mean val. rec. loss:  3.08382620e-05\n",
      "Epoch: 9422 mean train loss:  2.93110951e-05, mean val. rec. loss:  2.94652381e-05\n",
      "Epoch: 9423 mean train loss:  2.90417403e-05, mean val. rec. loss:  3.12537500e-05\n",
      "Epoch: 9424 mean train loss:  2.92604766e-05, mean val. rec. loss:  3.16070484e-05\n",
      "Epoch: 9425 mean train loss:  2.93110958e-05, mean val. rec. loss:  3.09833893e-05\n",
      "Epoch: 9426 mean train loss:  2.87804601e-05, mean val. rec. loss:  2.95087604e-05\n",
      "Epoch: 9427 mean train loss:  2.84214362e-05, mean val. rec. loss:  3.07018392e-05\n",
      "Epoch: 9428 mean train loss:  2.85792806e-05, mean val. rec. loss:  2.93091121e-05\n",
      "Epoch: 9429 mean train loss:  2.88653263e-05, mean val. rec. loss:  3.08229565e-05\n",
      "Epoch: 9430 mean train loss:  2.89773545e-05, mean val. rec. loss:  3.02370729e-05\n",
      "Epoch: 9431 mean train loss:  2.86946715e-05, mean val. rec. loss:  3.00617571e-05\n",
      "Epoch: 9432 mean train loss:  2.85841361e-05, mean val. rec. loss:  2.97243957e-05\n",
      "Epoch: 9433 mean train loss:  2.85880964e-05, mean val. rec. loss:  2.93378627e-05\n",
      "Epoch: 9434 mean train loss:  2.85531755e-05, mean val. rec. loss:  2.90173628e-05\n",
      "Epoch: 9435 mean train loss:  2.84819731e-05, mean val. rec. loss:  3.16402193e-05\n",
      "Epoch: 9436 mean train loss:  2.91808934e-05, mean val. rec. loss:  3.05895943e-05\n",
      "Epoch: 9437 mean train loss:  2.87537186e-05, mean val. rec. loss:  3.04423704e-05\n",
      "Epoch: 9438 mean train loss:  2.85295940e-05, mean val. rec. loss:  2.89835172e-05\n",
      "Epoch: 9439 mean train loss:  2.85917077e-05, mean val. rec. loss:  2.99102513e-05\n",
      "Epoch: 9440 mean train loss:  2.84019481e-05, mean val. rec. loss:  2.95177466e-05\n",
      "Epoch: 9441 mean train loss:  2.84360835e-05, mean val. rec. loss:  2.94940137e-05\n",
      "Epoch: 9442 mean train loss:  2.83680020e-05, mean val. rec. loss:  2.98582858e-05\n",
      "Epoch: 9443 mean train loss:  2.87047449e-05, mean val. rec. loss:  2.98379852e-05\n",
      "Epoch: 9444 mean train loss:  2.89689399e-05, mean val. rec. loss:  3.04527513e-05\n",
      "Epoch: 9445 mean train loss:  2.86654152e-05, mean val. rec. loss:  2.98629401e-05\n",
      "Epoch: 9446 mean train loss:  2.86133328e-05, mean val. rec. loss:  2.91576518e-05\n",
      "Epoch: 9447 mean train loss:  2.86041605e-05, mean val. rec. loss:  3.00261942e-05\n",
      "Epoch: 9448 mean train loss:  2.86092481e-05, mean val. rec. loss:  2.95068160e-05\n",
      "Epoch: 9449 mean train loss:  2.83611858e-05, mean val. rec. loss:  3.00043603e-05\n",
      "Epoch: 9450 mean train loss:  2.88028680e-05, mean val. rec. loss:  3.04846320e-05\n",
      "Epoch: 9451 mean train loss:  2.90007506e-05, mean val. rec. loss:  3.16004246e-05\n",
      "Epoch: 9452 mean train loss:  2.93303376e-05, mean val. rec. loss:  3.06458826e-05\n",
      "Epoch: 9453 mean train loss:  2.86829483e-05, mean val. rec. loss:  2.90449436e-05\n",
      "Epoch: 9454 mean train loss:  2.84267045e-05, mean val. rec. loss:  2.95198182e-05\n",
      "Epoch: 9455 mean train loss:  2.84968666e-05, mean val. rec. loss:  2.94626077e-05\n",
      "Epoch: 9456 mean train loss:  2.84766052e-05, mean val. rec. loss:  3.03680372e-05\n",
      "Epoch: 9457 mean train loss:  2.88872590e-05, mean val. rec. loss:  2.96078441e-05\n",
      "Epoch: 9458 mean train loss:  2.87272245e-05, mean val. rec. loss:  3.01636392e-05\n",
      "Epoch: 9459 mean train loss:  2.86470523e-05, mean val. rec. loss:  2.89050499e-05\n",
      "Epoch: 9460 mean train loss:  2.84704639e-05, mean val. rec. loss:  3.02741259e-05\n",
      "Epoch: 9461 mean train loss:  2.85648574e-05, mean val. rec. loss:  2.89602341e-05\n",
      "Epoch: 9462 mean train loss:  2.86282211e-05, mean val. rec. loss:  2.97129586e-05\n",
      "Epoch: 9463 mean train loss:  2.86426538e-05, mean val. rec. loss:  2.90229553e-05\n",
      "Epoch: 9464 mean train loss:  2.82693835e-05, mean val. rec. loss:  2.99423956e-05\n",
      "Epoch: 9465 mean train loss:  2.81730216e-05, mean val. rec. loss:  2.99425364e-05\n",
      "Epoch: 9466 mean train loss:  2.85605698e-05, mean val. rec. loss:  2.93616796e-05\n",
      "Epoch: 9467 mean train loss:  2.81401808e-05, mean val. rec. loss:  3.01079279e-05\n",
      "Epoch: 9468 mean train loss:  2.83537849e-05, mean val. rec. loss:  3.00223826e-05\n",
      "Epoch: 9469 mean train loss:  2.84151001e-05, mean val. rec. loss:  3.01195581e-05\n",
      "Epoch: 9470 mean train loss:  2.86577521e-05, mean val. rec. loss:  2.91624947e-05\n",
      "Epoch: 9471 mean train loss:  2.84754176e-05, mean val. rec. loss:  2.91802353e-05\n",
      "Epoch: 9472 mean train loss:  2.85517077e-05, mean val. rec. loss:  3.08021381e-05\n",
      "Epoch: 9473 mean train loss:  2.85043933e-05, mean val. rec. loss:  2.92719410e-05\n",
      "Epoch: 9474 mean train loss:  2.82409719e-05, mean val. rec. loss:  3.03922289e-05\n",
      "Epoch: 9475 mean train loss:  2.84168095e-05, mean val. rec. loss:  2.89024285e-05\n",
      "Epoch: 9476 mean train loss:  2.84611096e-05, mean val. rec. loss:  3.05839837e-05\n",
      "Epoch: 9477 mean train loss:  2.84354221e-05, mean val. rec. loss:  2.84137931e-05\n",
      "Epoch: 9478 mean train loss:  2.85460925e-05, mean val. rec. loss:  3.02721905e-05\n",
      "Epoch: 9479 mean train loss:  2.84743039e-05, mean val. rec. loss:  2.87323145e-05\n",
      "Epoch: 9480 mean train loss:  2.82678644e-05, mean val. rec. loss:  2.95863828e-05\n",
      "Epoch: 9481 mean train loss:  2.84720324e-05, mean val. rec. loss:  3.00461972e-05\n",
      "Epoch: 9482 mean train loss:  2.84976431e-05, mean val. rec. loss:  3.04732881e-05\n",
      "Epoch: 9483 mean train loss:  2.84650336e-05, mean val. rec. loss:  2.88700503e-05\n",
      "Epoch: 9484 mean train loss:  2.85224124e-05, mean val. rec. loss:  3.04835644e-05\n",
      "Epoch: 9485 mean train loss:  2.89447727e-05, mean val. rec. loss:  3.01790287e-05\n",
      "Epoch: 9486 mean train loss:  2.90930958e-05, mean val. rec. loss:  2.94551571e-05\n",
      "Epoch: 9487 mean train loss:  2.87156540e-05, mean val. rec. loss:  2.99414211e-05\n",
      "Epoch: 9488 mean train loss:  2.83737611e-05, mean val. rec. loss:  2.90286818e-05\n",
      "Epoch: 9489 mean train loss:  2.86193611e-05, mean val. rec. loss:  3.00484187e-05\n",
      "Epoch: 9490 mean train loss:  2.84831253e-05, mean val. rec. loss:  2.87386611e-05\n",
      "Epoch: 9491 mean train loss:  2.79277557e-05, mean val. rec. loss:  2.97561015e-05\n",
      "Epoch: 9492 mean train loss:  2.82458811e-05, mean val. rec. loss:  3.06824245e-05\n",
      "Epoch: 9493 mean train loss:  2.88299008e-05, mean val. rec. loss:  2.95907168e-05\n",
      "Epoch: 9494 mean train loss:  2.85212662e-05, mean val. rec. loss:  2.90386492e-05\n",
      "Epoch: 9495 mean train loss:  2.84456975e-05, mean val. rec. loss:  2.96269430e-05\n",
      "Epoch: 9496 mean train loss:  2.83841188e-05, mean val. rec. loss:  2.89550415e-05\n",
      "Epoch: 9497 mean train loss:  2.80783793e-05, mean val. rec. loss:  2.95298492e-05\n",
      "Epoch: 9498 mean train loss:  2.80661916e-05, mean val. rec. loss:  2.94958945e-05\n",
      "Epoch: 9499 mean train loss:  2.84495050e-05, mean val. rec. loss:  2.90723972e-05\n",
      "Epoch: 9500 mean train loss:  2.82126537e-05, mean val. rec. loss:  2.99366577e-05\n",
      "Epoch: 9501 mean train loss:  2.82420600e-05, mean val. rec. loss:  2.96806939e-05\n",
      "Epoch: 9502 mean train loss:  2.88044267e-05, mean val. rec. loss:  2.83946715e-05\n",
      "Epoch: 9503 mean train loss:  2.82436214e-05, mean val. rec. loss:  2.95764517e-05\n",
      "Epoch: 9504 mean train loss:  2.81973781e-05, mean val. rec. loss:  2.83463881e-05\n",
      "Epoch: 9505 mean train loss:  2.78727394e-05, mean val. rec. loss:  2.89610678e-05\n",
      "Epoch: 9506 mean train loss:  2.80633365e-05, mean val. rec. loss:  2.91944277e-05\n",
      "Epoch: 9507 mean train loss:  2.79297836e-05, mean val. rec. loss:  3.01327647e-05\n",
      "Epoch: 9508 mean train loss:  2.80403835e-05, mean val. rec. loss:  2.91978531e-05\n",
      "Epoch: 9509 mean train loss:  2.81876540e-05, mean val. rec. loss:  2.95889723e-05\n",
      "Epoch: 9510 mean train loss:  2.80404447e-05, mean val. rec. loss:  2.87327234e-05\n",
      "Epoch: 9511 mean train loss:  2.80782352e-05, mean val. rec. loss:  3.00356301e-05\n",
      "Epoch: 9512 mean train loss:  2.79801871e-05, mean val. rec. loss:  2.94645589e-05\n",
      "Epoch: 9513 mean train loss:  2.80999642e-05, mean val. rec. loss:  2.92920894e-05\n",
      "Epoch: 9514 mean train loss:  2.81724217e-05, mean val. rec. loss:  3.13257049e-05\n",
      "Epoch: 9515 mean train loss:  2.90907838e-05, mean val. rec. loss:  3.09144896e-05\n",
      "Epoch: 9516 mean train loss:  2.85900629e-05, mean val. rec. loss:  2.86614522e-05\n",
      "Epoch: 9517 mean train loss:  2.84850554e-05, mean val. rec. loss:  3.11849434e-05\n",
      "Epoch: 9518 mean train loss:  2.93405174e-05, mean val. rec. loss:  2.94602635e-05\n",
      "Epoch: 9519 mean train loss:  2.89222226e-05, mean val. rec. loss:  3.09978952e-05\n",
      "Epoch: 9520 mean train loss:  2.88068867e-05, mean val. rec. loss:  2.98299736e-05\n",
      "Epoch: 9521 mean train loss:  2.80510234e-05, mean val. rec. loss:  2.95782507e-05\n",
      "Epoch: 9522 mean train loss:  2.80051874e-05, mean val. rec. loss:  2.83968181e-05\n",
      "Epoch: 9523 mean train loss:  2.76131159e-05, mean val. rec. loss:  2.88608779e-05\n",
      "Epoch: 9524 mean train loss:  2.77972304e-05, mean val. rec. loss:  2.88188776e-05\n",
      "Epoch: 9525 mean train loss:  2.76683448e-05, mean val. rec. loss:  2.89316448e-05\n",
      "Epoch: 9526 mean train loss:  2.79706443e-05, mean val. rec. loss:  2.85349650e-05\n",
      "Epoch: 9527 mean train loss:  2.78929626e-05, mean val. rec. loss:  2.95130559e-05\n",
      "Epoch: 9528 mean train loss:  2.79452756e-05, mean val. rec. loss:  2.83568484e-05\n",
      "Epoch: 9529 mean train loss:  2.76922618e-05, mean val. rec. loss:  2.95272688e-05\n",
      "Epoch: 9530 mean train loss:  2.81889408e-05, mean val. rec. loss:  2.86443227e-05\n",
      "Epoch: 9531 mean train loss:  2.76068923e-05, mean val. rec. loss:  2.91889806e-05\n",
      "Epoch: 9532 mean train loss:  2.75999139e-05, mean val. rec. loss:  2.83724288e-05\n",
      "Epoch: 9533 mean train loss:  2.76350715e-05, mean val. rec. loss:  2.90044811e-05\n",
      "Epoch: 9534 mean train loss:  2.80097374e-05, mean val. rec. loss:  2.96724551e-05\n",
      "Epoch: 9535 mean train loss:  2.82654969e-05, mean val. rec. loss:  2.87308335e-05\n",
      "Epoch: 9536 mean train loss:  2.78523638e-05, mean val. rec. loss:  3.01404970e-05\n",
      "Epoch: 9537 mean train loss:  2.82079705e-05, mean val. rec. loss:  2.86839971e-05\n",
      "Epoch: 9538 mean train loss:  2.82050925e-05, mean val. rec. loss:  2.97767996e-05\n",
      "Epoch: 9539 mean train loss:  2.81366066e-05, mean val. rec. loss:  2.86194973e-05\n",
      "Epoch: 9540 mean train loss:  2.76550050e-05, mean val. rec. loss:  2.92340316e-05\n",
      "Epoch: 9541 mean train loss:  2.81780850e-05, mean val. rec. loss:  3.01494604e-05\n",
      "Epoch: 9542 mean train loss:  2.85051907e-05, mean val. rec. loss:  2.92502980e-05\n",
      "Epoch: 9543 mean train loss:  2.77788601e-05, mean val. rec. loss:  2.96378099e-05\n",
      "Epoch: 9544 mean train loss:  2.86232421e-05, mean val. rec. loss:  3.01097315e-05\n",
      "Epoch: 9545 mean train loss:  2.82878534e-05, mean val. rec. loss:  2.82139199e-05\n",
      "Epoch: 9546 mean train loss:  2.78345188e-05, mean val. rec. loss:  2.96420372e-05\n",
      "Epoch: 9547 mean train loss:  2.79143693e-05, mean val. rec. loss:  2.90902922e-05\n",
      "Epoch: 9548 mean train loss:  2.79461962e-05, mean val. rec. loss:  3.12887087e-05\n",
      "Epoch: 9549 mean train loss:  2.81458483e-05, mean val. rec. loss:  2.87529990e-05\n",
      "Epoch: 9550 mean train loss:  2.76833169e-05, mean val. rec. loss:  2.95289383e-05\n",
      "Epoch: 9551 mean train loss:  2.76813668e-05, mean val. rec. loss:  2.91465554e-05\n",
      "Epoch: 9552 mean train loss:  2.76296645e-05, mean val. rec. loss:  2.85467678e-05\n",
      "Epoch: 9553 mean train loss:  2.82059812e-05, mean val. rec. loss:  2.92331344e-05\n",
      "Epoch: 9554 mean train loss:  2.79866692e-05, mean val. rec. loss:  2.82710918e-05\n",
      "Epoch: 9555 mean train loss:  2.79439077e-05, mean val. rec. loss:  2.96137069e-05\n",
      "Epoch: 9556 mean train loss:  2.82261462e-05, mean val. rec. loss:  3.03780842e-05\n",
      "Epoch: 9557 mean train loss:  2.78863553e-05, mean val. rec. loss:  2.84671216e-05\n",
      "Epoch: 9558 mean train loss:  2.78111293e-05, mean val. rec. loss:  2.84033850e-05\n",
      "Epoch: 9559 mean train loss:  2.78256169e-05, mean val. rec. loss:  2.93231092e-05\n",
      "Epoch: 9560 mean train loss:  2.84675103e-05, mean val. rec. loss:  2.84750969e-05\n",
      "Epoch: 9561 mean train loss:  2.77727347e-05, mean val. rec. loss:  2.90283365e-05\n",
      "Epoch: 9562 mean train loss:  2.78835216e-05, mean val. rec. loss:  3.00244087e-05\n",
      "Epoch: 9563 mean train loss:  2.85784099e-05, mean val. rec. loss:  2.92489578e-05\n",
      "Epoch: 9564 mean train loss:  2.83100927e-05, mean val. rec. loss:  2.90621663e-05\n",
      "Epoch: 9565 mean train loss:  2.79654481e-05, mean val. rec. loss:  2.92757026e-05\n",
      "Epoch: 9566 mean train loss:  2.82324849e-05, mean val. rec. loss:  2.94525812e-05\n",
      "Epoch: 9567 mean train loss:  2.75898950e-05, mean val. rec. loss:  2.84481203e-05\n",
      "Epoch: 9568 mean train loss:  2.77170779e-05, mean val. rec. loss:  2.89431478e-05\n",
      "Epoch: 9569 mean train loss:  2.78719218e-05, mean val. rec. loss:  2.88430760e-05\n",
      "Epoch: 9570 mean train loss:  2.79955545e-05, mean val. rec. loss:  2.93009574e-05\n",
      "Epoch: 9571 mean train loss:  2.77962193e-05, mean val. rec. loss:  2.85354920e-05\n",
      "Epoch: 9572 mean train loss:  2.74073683e-05, mean val. rec. loss:  2.89813275e-05\n",
      "Epoch: 9573 mean train loss:  2.77722689e-05, mean val. rec. loss:  2.85668798e-05\n",
      "Epoch: 9574 mean train loss:  2.79735181e-05, mean val. rec. loss:  2.84139203e-05\n",
      "Epoch: 9575 mean train loss:  2.75583933e-05, mean val. rec. loss:  2.95726537e-05\n",
      "Epoch: 9576 mean train loss:  2.76272710e-05, mean val. rec. loss:  2.91264230e-05\n",
      "Epoch: 9577 mean train loss:  2.79811526e-05, mean val. rec. loss:  2.85777013e-05\n",
      "Epoch: 9578 mean train loss:  2.72966337e-05, mean val. rec. loss:  2.80756935e-05\n",
      "Epoch: 9579 mean train loss:  2.73221660e-05, mean val. rec. loss:  2.92468634e-05\n",
      "Epoch: 9580 mean train loss:  2.77234626e-05, mean val. rec. loss:  2.82521746e-05\n",
      "Epoch: 9581 mean train loss:  2.75773016e-05, mean val. rec. loss:  2.88896626e-05\n",
      "Epoch: 9582 mean train loss:  2.81219782e-05, mean val. rec. loss:  2.84158148e-05\n",
      "Epoch: 9583 mean train loss:  2.80661664e-05, mean val. rec. loss:  2.87554340e-05\n",
      "Epoch: 9584 mean train loss:  2.80293955e-05, mean val. rec. loss:  3.14292316e-05\n",
      "Epoch: 9585 mean train loss:  2.84152503e-05, mean val. rec. loss:  2.85015918e-05\n",
      "Epoch: 9586 mean train loss:  2.79420499e-05, mean val. rec. loss:  2.87980318e-05\n",
      "Epoch: 9587 mean train loss:  2.73632622e-05, mean val. rec. loss:  2.89332848e-05\n",
      "Epoch: 9588 mean train loss:  2.84438325e-05, mean val. rec. loss:  2.83357074e-05\n",
      "Epoch: 9589 mean train loss:  2.79543095e-05, mean val. rec. loss:  2.98913659e-05\n",
      "Epoch: 9590 mean train loss:  2.81380143e-05, mean val. rec. loss:  2.85229078e-05\n",
      "Epoch: 9591 mean train loss:  2.75016179e-05, mean val. rec. loss:  2.86469531e-05\n",
      "Epoch: 9592 mean train loss:  2.75339487e-05, mean val. rec. loss:  2.92019510e-05\n",
      "Epoch: 9593 mean train loss:  2.76095788e-05, mean val. rec. loss:  2.81356548e-05\n",
      "Epoch: 9594 mean train loss:  2.76091609e-05, mean val. rec. loss:  2.85358373e-05\n",
      "Epoch: 9595 mean train loss:  2.76244572e-05, mean val. rec. loss:  2.88683535e-05\n",
      "Epoch: 9596 mean train loss:  2.78298821e-05, mean val. rec. loss:  2.76886063e-05\n",
      "Epoch: 9597 mean train loss:  2.75462455e-05, mean val. rec. loss:  2.91657475e-05\n",
      "Epoch: 9598 mean train loss:  2.75217360e-05, mean val. rec. loss:  2.82486219e-05\n",
      "Epoch: 9599 mean train loss:  2.78460216e-05, mean val. rec. loss:  2.87369939e-05\n",
      "Epoch: 9600 mean train loss:  2.76338913e-05, mean val. rec. loss:  2.78555129e-05\n",
      "Epoch: 9601 mean train loss:  2.73344870e-05, mean val. rec. loss:  2.87016172e-05\n",
      "Epoch: 9602 mean train loss:  2.75886027e-05, mean val. rec. loss:  2.83092533e-05\n",
      "Epoch: 9603 mean train loss:  2.74612998e-05, mean val. rec. loss:  2.91877494e-05\n",
      "Epoch: 9604 mean train loss:  2.75783110e-05, mean val. rec. loss:  2.85417841e-05\n",
      "Epoch: 9605 mean train loss:  2.73057643e-05, mean val. rec. loss:  2.84264455e-05\n",
      "Epoch: 9606 mean train loss:  2.74534458e-05, mean val. rec. loss:  2.88087988e-05\n",
      "Epoch: 9607 mean train loss:  2.75020756e-05, mean val. rec. loss:  2.76949166e-05\n",
      "Epoch: 9608 mean train loss:  2.72267310e-05, mean val. rec. loss:  2.96195015e-05\n",
      "Epoch: 9609 mean train loss:  2.72624660e-05, mean val. rec. loss:  2.85860310e-05\n",
      "Epoch: 9610 mean train loss:  2.73460019e-05, mean val. rec. loss:  2.93539678e-05\n",
      "Epoch: 9611 mean train loss:  2.80218920e-05, mean val. rec. loss:  2.99087339e-05\n",
      "Epoch: 9612 mean train loss:  2.81891946e-05, mean val. rec. loss:  2.78302105e-05\n",
      "Epoch: 9613 mean train loss:  2.74708275e-05, mean val. rec. loss:  2.92618623e-05\n",
      "Epoch: 9614 mean train loss:  2.74875659e-05, mean val. rec. loss:  2.88394689e-05\n",
      "Epoch: 9615 mean train loss:  2.75792558e-05, mean val. rec. loss:  2.89153535e-05\n",
      "Epoch: 9616 mean train loss:  2.74990170e-05, mean val. rec. loss:  2.78793048e-05\n",
      "Epoch: 9617 mean train loss:  2.71461188e-05, mean val. rec. loss:  2.82038389e-05\n",
      "Epoch: 9618 mean train loss:  2.72838323e-05, mean val. rec. loss:  2.87924075e-05\n",
      "Epoch: 9619 mean train loss:  2.72927026e-05, mean val. rec. loss:  2.80034569e-05\n",
      "Epoch: 9620 mean train loss:  2.74061164e-05, mean val. rec. loss:  2.83273800e-05\n",
      "Epoch: 9621 mean train loss:  2.71958393e-05, mean val. rec. loss:  2.83281660e-05\n",
      "Epoch: 9622 mean train loss:  2.72489466e-05, mean val. rec. loss:  2.90606534e-05\n",
      "Epoch: 9623 mean train loss:  2.74316462e-05, mean val. rec. loss:  2.86413743e-05\n",
      "Epoch: 9624 mean train loss:  2.73180311e-05, mean val. rec. loss:  2.87424932e-05\n",
      "Epoch: 9625 mean train loss:  2.80714618e-05, mean val. rec. loss:  2.80434220e-05\n",
      "Epoch: 9626 mean train loss:  2.75808591e-05, mean val. rec. loss:  2.86976103e-05\n",
      "Epoch: 9627 mean train loss:  2.77477368e-05, mean val. rec. loss:  2.81619953e-05\n",
      "Epoch: 9628 mean train loss:  2.76454920e-05, mean val. rec. loss:  3.00235183e-05\n",
      "Epoch: 9629 mean train loss:  2.76887171e-05, mean val. rec. loss:  2.85568397e-05\n",
      "Epoch: 9630 mean train loss:  2.76179393e-05, mean val. rec. loss:  2.81344554e-05\n",
      "Epoch: 9631 mean train loss:  2.71205776e-05, mean val. rec. loss:  2.79511984e-05\n",
      "Epoch: 9632 mean train loss:  2.71339611e-05, mean val. rec. loss:  2.87970982e-05\n",
      "Epoch: 9633 mean train loss:  2.71356462e-05, mean val. rec. loss:  2.81846105e-05\n",
      "Epoch: 9634 mean train loss:  2.72487607e-05, mean val. rec. loss:  2.88819803e-05\n",
      "Epoch: 9635 mean train loss:  2.74865226e-05, mean val. rec. loss:  2.74859823e-05\n",
      "Epoch: 9636 mean train loss:  2.73959176e-05, mean val. rec. loss:  2.93633878e-05\n",
      "Epoch: 9637 mean train loss:  2.71813629e-05, mean val. rec. loss:  2.75305904e-05\n",
      "Epoch: 9638 mean train loss:  2.69982574e-05, mean val. rec. loss:  2.83042014e-05\n",
      "Epoch: 9639 mean train loss:  2.70028284e-05, mean val. rec. loss:  2.79866931e-05\n",
      "Epoch: 9640 mean train loss:  2.73011789e-05, mean val. rec. loss:  2.88953505e-05\n",
      "Epoch: 9641 mean train loss:  2.70174163e-05, mean val. rec. loss:  2.80648106e-05\n",
      "Epoch: 9642 mean train loss:  2.70187776e-05, mean val. rec. loss:  2.85344494e-05\n",
      "Epoch: 9643 mean train loss:  2.69660292e-05, mean val. rec. loss:  2.81757334e-05\n",
      "Epoch: 9644 mean train loss:  2.70393729e-05, mean val. rec. loss:  2.77051657e-05\n",
      "Epoch: 9645 mean train loss:  2.69354204e-05, mean val. rec. loss:  2.84814276e-05\n",
      "Epoch: 9646 mean train loss:  2.76106912e-05, mean val. rec. loss:  2.72954701e-05\n",
      "Epoch: 9647 mean train loss:  2.75960732e-05, mean val. rec. loss:  3.04629299e-05\n",
      "Epoch: 9648 mean train loss:  2.79249801e-05, mean val. rec. loss:  2.81940623e-05\n",
      "Epoch: 9649 mean train loss:  2.75350393e-05, mean val. rec. loss:  2.85667004e-05\n",
      "Epoch: 9650 mean train loss:  2.74675243e-05, mean val. rec. loss:  2.83963274e-05\n",
      "Epoch: 9651 mean train loss:  2.75251655e-05, mean val. rec. loss:  2.82843257e-05\n",
      "Epoch: 9652 mean train loss:  2.70191576e-05, mean val. rec. loss:  2.90554471e-05\n",
      "Epoch: 9653 mean train loss:  2.73425515e-05, mean val. rec. loss:  2.86706428e-05\n",
      "Epoch: 9654 mean train loss:  2.73681641e-05, mean val. rec. loss:  2.81518439e-05\n",
      "Epoch: 9655 mean train loss:  2.72901028e-05, mean val. rec. loss:  2.80523991e-05\n",
      "Epoch: 9656 mean train loss:  2.71471138e-05, mean val. rec. loss:  2.77776953e-05\n",
      "Epoch: 9657 mean train loss:  2.73971781e-05, mean val. rec. loss:  2.87544891e-05\n",
      "Epoch: 9658 mean train loss:  2.74187615e-05, mean val. rec. loss:  2.93240269e-05\n",
      "Epoch: 9659 mean train loss:  2.78100653e-05, mean val. rec. loss:  3.01383527e-05\n",
      "Epoch: 9660 mean train loss:  2.73773023e-05, mean val. rec. loss:  2.80462342e-05\n",
      "Epoch: 9661 mean train loss:  2.70765918e-05, mean val. rec. loss:  2.81880678e-05\n",
      "Epoch: 9662 mean train loss:  2.72694030e-05, mean val. rec. loss:  2.83304829e-05\n",
      "Epoch: 9663 mean train loss:  2.69018580e-05, mean val. rec. loss:  2.85097829e-05\n",
      "Epoch: 9664 mean train loss:  2.70356234e-05, mean val. rec. loss:  2.91725802e-05\n",
      "Epoch: 9665 mean train loss:  2.72770475e-05, mean val. rec. loss:  2.72642662e-05\n",
      "Epoch: 9666 mean train loss:  2.68004831e-05, mean val. rec. loss:  2.83759314e-05\n",
      "Epoch: 9667 mean train loss:  2.69085488e-05, mean val. rec. loss:  2.75069939e-05\n",
      "Epoch: 9668 mean train loss:  2.67798220e-05, mean val. rec. loss:  2.87374527e-05\n",
      "Epoch: 9669 mean train loss:  2.68151979e-05, mean val. rec. loss:  2.83256014e-05\n",
      "Epoch: 9670 mean train loss:  2.69884205e-05, mean val. rec. loss:  2.79428483e-05\n",
      "Epoch: 9671 mean train loss:  2.67108645e-05, mean val. rec. loss:  2.75292457e-05\n",
      "Epoch: 9672 mean train loss:  2.69275152e-05, mean val. rec. loss:  2.81304666e-05\n",
      "Epoch: 9673 mean train loss:  2.70289663e-05, mean val. rec. loss:  2.81457857e-05\n",
      "Epoch: 9674 mean train loss:  2.73113219e-05, mean val. rec. loss:  3.03019792e-05\n",
      "Epoch: 9675 mean train loss:  2.77254608e-05, mean val. rec. loss:  2.85269102e-05\n",
      "Epoch: 9676 mean train loss:  2.74443380e-05, mean val. rec. loss:  2.88538612e-05\n",
      "Epoch: 9677 mean train loss:  2.72679583e-05, mean val. rec. loss:  2.83550835e-05\n",
      "Epoch: 9678 mean train loss:  2.74070987e-05, mean val. rec. loss:  2.88370224e-05\n",
      "Epoch: 9679 mean train loss:  2.71172625e-05, mean val. rec. loss:  2.94074144e-05\n",
      "Epoch: 9680 mean train loss:  2.76398324e-05, mean val. rec. loss:  2.83740870e-05\n",
      "Epoch: 9681 mean train loss:  2.70657854e-05, mean val. rec. loss:  2.79534245e-05\n",
      "Epoch: 9682 mean train loss:  2.67708865e-05, mean val. rec. loss:  2.75176177e-05\n",
      "Epoch: 9683 mean train loss:  2.68931149e-05, mean val. rec. loss:  2.77593641e-05\n",
      "Epoch: 9684 mean train loss:  2.67950551e-05, mean val. rec. loss:  2.83305011e-05\n",
      "Epoch: 9685 mean train loss:  2.68473044e-05, mean val. rec. loss:  2.84139976e-05\n",
      "Epoch: 9686 mean train loss:  2.69376944e-05, mean val. rec. loss:  2.86850987e-05\n",
      "Epoch: 9687 mean train loss:  2.71321094e-05, mean val. rec. loss:  2.73541297e-05\n",
      "Epoch: 9688 mean train loss:  2.69303703e-05, mean val. rec. loss:  2.78013736e-05\n",
      "Epoch: 9689 mean train loss:  2.65972372e-05, mean val. rec. loss:  2.78267964e-05\n",
      "Epoch: 9690 mean train loss:  2.66017903e-05, mean val. rec. loss:  2.80258087e-05\n",
      "Epoch: 9691 mean train loss:  2.67220527e-05, mean val. rec. loss:  2.71405184e-05\n",
      "Epoch: 9692 mean train loss:  2.65606134e-05, mean val. rec. loss:  2.85062621e-05\n",
      "Epoch: 9693 mean train loss:  2.68284192e-05, mean val. rec. loss:  2.77129638e-05\n",
      "Epoch: 9694 mean train loss:  2.66524078e-05, mean val. rec. loss:  2.88934470e-05\n",
      "Epoch: 9695 mean train loss:  2.71650852e-05, mean val. rec. loss:  2.74923153e-05\n",
      "Epoch: 9696 mean train loss:  2.70691917e-05, mean val. rec. loss:  2.78766199e-05\n",
      "Epoch: 9697 mean train loss:  2.70404097e-05, mean val. rec. loss:  2.86301325e-05\n",
      "Epoch: 9698 mean train loss:  2.67499300e-05, mean val. rec. loss:  2.72190675e-05\n",
      "Epoch: 9699 mean train loss:  2.66497764e-05, mean val. rec. loss:  2.77758849e-05\n",
      "Epoch: 9700 mean train loss:  2.68877009e-05, mean val. rec. loss:  2.74619542e-05\n",
      "Epoch: 9701 mean train loss:  2.68721701e-05, mean val. rec. loss:  2.79982529e-05\n",
      "Epoch: 9702 mean train loss:  2.72178184e-05, mean val. rec. loss:  2.83193616e-05\n",
      "Epoch: 9703 mean train loss:  2.65935893e-05, mean val. rec. loss:  2.79930148e-05\n",
      "Epoch: 9704 mean train loss:  2.73373468e-05, mean val. rec. loss:  2.98527092e-05\n",
      "Epoch: 9705 mean train loss:  2.75791856e-05, mean val. rec. loss:  2.81791384e-05\n",
      "Epoch: 9706 mean train loss:  2.76239404e-05, mean val. rec. loss:  2.90452412e-05\n",
      "Epoch: 9707 mean train loss:  2.76313071e-05, mean val. rec. loss:  2.82690747e-05\n",
      "Epoch: 9708 mean train loss:  2.69637080e-05, mean val. rec. loss:  2.74987755e-05\n",
      "Epoch: 9709 mean train loss:  2.66690876e-05, mean val. rec. loss:  2.87020670e-05\n",
      "Epoch: 9710 mean train loss:  2.65685987e-05, mean val. rec. loss:  2.77987863e-05\n",
      "Epoch: 9711 mean train loss:  2.68184884e-05, mean val. rec. loss:  2.76264939e-05\n",
      "Epoch: 9712 mean train loss:  2.70042538e-05, mean val. rec. loss:  2.84529405e-05\n",
      "Epoch: 9713 mean train loss:  2.71187087e-05, mean val. rec. loss:  2.72940163e-05\n",
      "Epoch: 9714 mean train loss:  2.70502033e-05, mean val. rec. loss:  2.80296657e-05\n",
      "Epoch: 9715 mean train loss:  2.68619002e-05, mean val. rec. loss:  2.75162617e-05\n",
      "Epoch: 9716 mean train loss:  2.65992547e-05, mean val. rec. loss:  2.81089463e-05\n",
      "Epoch: 9717 mean train loss:  2.67386916e-05, mean val. rec. loss:  2.73940744e-05\n",
      "Epoch: 9718 mean train loss:  2.69482372e-05, mean val. rec. loss:  2.80317691e-05\n",
      "Epoch: 9719 mean train loss:  2.65980496e-05, mean val. rec. loss:  2.78222261e-05\n",
      "Epoch: 9720 mean train loss:  2.67080264e-05, mean val. rec. loss:  2.78268873e-05\n",
      "Epoch: 9721 mean train loss:  2.65431497e-05, mean val. rec. loss:  2.79226318e-05\n",
      "Epoch: 9722 mean train loss:  2.63106354e-05, mean val. rec. loss:  2.78158386e-05\n",
      "Epoch: 9723 mean train loss:  2.68477140e-05, mean val. rec. loss:  2.82505891e-05\n",
      "Epoch: 9724 mean train loss:  2.66345604e-05, mean val. rec. loss:  2.71050646e-05\n",
      "Epoch: 9725 mean train loss:  2.68293798e-05, mean val. rec. loss:  2.80767906e-05\n",
      "Epoch: 9726 mean train loss:  2.67862034e-05, mean val. rec. loss:  2.87728339e-05\n",
      "Epoch: 9727 mean train loss:  2.66625534e-05, mean val. rec. loss:  2.72883375e-05\n",
      "Epoch: 9728 mean train loss:  2.62845754e-05, mean val. rec. loss:  2.73873984e-05\n",
      "Epoch: 9729 mean train loss:  2.66251605e-05, mean val. rec. loss:  2.68961939e-05\n",
      "Epoch: 9730 mean train loss:  2.67144942e-05, mean val. rec. loss:  2.86773529e-05\n",
      "Epoch: 9731 mean train loss:  2.64733525e-05, mean val. rec. loss:  2.69771667e-05\n",
      "Epoch: 9732 mean train loss:  2.63420215e-05, mean val. rec. loss:  2.78008011e-05\n",
      "Epoch: 9733 mean train loss:  2.65989650e-05, mean val. rec. loss:  2.69005416e-05\n",
      "Epoch: 9734 mean train loss:  2.64845332e-05, mean val. rec. loss:  2.75051085e-05\n",
      "Epoch: 9735 mean train loss:  2.64640879e-05, mean val. rec. loss:  2.73517833e-05\n",
      "Epoch: 9736 mean train loss:  2.67486695e-05, mean val. rec. loss:  2.70458892e-05\n",
      "Epoch: 9737 mean train loss:  2.65063835e-05, mean val. rec. loss:  2.77829993e-05\n",
      "Epoch: 9738 mean train loss:  2.67845350e-05, mean val. rec. loss:  2.78947511e-05\n",
      "Epoch: 9739 mean train loss:  2.67510862e-05, mean val. rec. loss:  2.84530790e-05\n",
      "Epoch: 9740 mean train loss:  2.65021039e-05, mean val. rec. loss:  2.67512051e-05\n",
      "Epoch: 9741 mean train loss:  2.63902243e-05, mean val. rec. loss:  2.77103992e-05\n",
      "Epoch: 9742 mean train loss:  2.62430290e-05, mean val. rec. loss:  2.73680950e-05\n",
      "Epoch: 9743 mean train loss:  2.63811825e-05, mean val. rec. loss:  2.75494395e-05\n",
      "Epoch: 9744 mean train loss:  2.63445593e-05, mean val. rec. loss:  2.74707495e-05\n",
      "Epoch: 9745 mean train loss:  2.63747247e-05, mean val. rec. loss:  2.71205858e-05\n",
      "Epoch: 9746 mean train loss:  2.62576470e-05, mean val. rec. loss:  2.75311401e-05\n",
      "Epoch: 9747 mean train loss:  2.65695806e-05, mean val. rec. loss:  2.75072256e-05\n",
      "Epoch: 9748 mean train loss:  2.65399938e-05, mean val. rec. loss:  2.79320586e-05\n",
      "Epoch: 9749 mean train loss:  2.66895281e-05, mean val. rec. loss:  2.77228881e-05\n",
      "Epoch: 9750 mean train loss:  2.66426147e-05, mean val. rec. loss:  2.90777830e-05\n",
      "Epoch: 9751 mean train loss:  2.68444963e-05, mean val. rec. loss:  2.72792855e-05\n",
      "Epoch: 9752 mean train loss:  2.66393350e-05, mean val. rec. loss:  2.79741225e-05\n",
      "Epoch: 9753 mean train loss:  2.63994951e-05, mean val. rec. loss:  2.75388837e-05\n",
      "Epoch: 9754 mean train loss:  2.66210641e-05, mean val. rec. loss:  2.74916838e-05\n",
      "Epoch: 9755 mean train loss:  2.61834080e-05, mean val. rec. loss:  2.71283113e-05\n",
      "Epoch: 9756 mean train loss:  2.63081036e-05, mean val. rec. loss:  2.90141509e-05\n",
      "Epoch: 9757 mean train loss:  2.66430440e-05, mean val. rec. loss:  2.74125850e-05\n",
      "Epoch: 9758 mean train loss:  2.73156965e-05, mean val. rec. loss:  2.91366584e-05\n",
      "Epoch: 9759 mean train loss:  2.71063458e-05, mean val. rec. loss:  2.74934102e-05\n",
      "Epoch: 9760 mean train loss:  2.72357520e-05, mean val. rec. loss:  2.76829866e-05\n",
      "Epoch: 9761 mean train loss:  2.64989677e-05, mean val. rec. loss:  2.71496022e-05\n",
      "Epoch: 9762 mean train loss:  2.63069222e-05, mean val. rec. loss:  2.74657976e-05\n",
      "Epoch: 9763 mean train loss:  2.61071565e-05, mean val. rec. loss:  2.78450003e-05\n",
      "Epoch: 9764 mean train loss:  2.63141552e-05, mean val. rec. loss:  2.72959789e-05\n",
      "Epoch: 9765 mean train loss:  2.64061258e-05, mean val. rec. loss:  2.81991437e-05\n",
      "Epoch: 9766 mean train loss:  2.68494399e-05, mean val. rec. loss:  2.70705738e-05\n",
      "Epoch: 9767 mean train loss:  2.65298931e-05, mean val. rec. loss:  2.85440306e-05\n",
      "Epoch: 9768 mean train loss:  2.65145058e-05, mean val. rec. loss:  2.91476571e-05\n",
      "Epoch: 9769 mean train loss:  2.75338217e-05, mean val. rec. loss:  2.82444628e-05\n",
      "Epoch: 9770 mean train loss:  2.69994744e-05, mean val. rec. loss:  2.87760980e-05\n",
      "Epoch: 9771 mean train loss:  2.69521481e-05, mean val. rec. loss:  2.71170287e-05\n",
      "Epoch: 9772 mean train loss:  2.63030596e-05, mean val. rec. loss:  2.74433777e-05\n",
      "Epoch: 9773 mean train loss:  2.60974591e-05, mean val. rec. loss:  2.74681622e-05\n",
      "Epoch: 9774 mean train loss:  2.62126930e-05, mean val. rec. loss:  2.69889377e-05\n",
      "Epoch: 9775 mean train loss:  2.61282804e-05, mean val. rec. loss:  2.71505335e-05\n",
      "Epoch: 9776 mean train loss:  2.63617798e-05, mean val. rec. loss:  2.68018873e-05\n",
      "Epoch: 9777 mean train loss:  2.63690668e-05, mean val. rec. loss:  2.86312365e-05\n",
      "Epoch: 9778 mean train loss:  2.73563862e-05, mean val. rec. loss:  2.74878677e-05\n",
      "Epoch: 9779 mean train loss:  2.63742059e-05, mean val. rec. loss:  2.71801337e-05\n",
      "Epoch: 9780 mean train loss:  2.61285282e-05, mean val. rec. loss:  2.70251344e-05\n",
      "Epoch: 9781 mean train loss:  2.59233036e-05, mean val. rec. loss:  2.69282836e-05\n",
      "Epoch: 9782 mean train loss:  2.60926650e-05, mean val. rec. loss:  2.74032127e-05\n",
      "Epoch: 9783 mean train loss:  2.61852705e-05, mean val. rec. loss:  2.70151374e-05\n",
      "Epoch: 9784 mean train loss:  2.61437521e-05, mean val. rec. loss:  2.75106056e-05\n",
      "Epoch: 9785 mean train loss:  2.66155055e-05, mean val. rec. loss:  2.73214517e-05\n",
      "Epoch: 9786 mean train loss:  2.61814527e-05, mean val. rec. loss:  2.75271559e-05\n",
      "Epoch: 9787 mean train loss:  2.63099887e-05, mean val. rec. loss:  2.70883076e-05\n",
      "Epoch: 9788 mean train loss:  2.63535314e-05, mean val. rec. loss:  2.72553210e-05\n",
      "Epoch: 9789 mean train loss:  2.62809943e-05, mean val. rec. loss:  2.71055348e-05\n",
      "Epoch: 9790 mean train loss:  2.63536150e-05, mean val. rec. loss:  2.83160360e-05\n",
      "Epoch: 9791 mean train loss:  2.60224214e-05, mean val. rec. loss:  2.75269469e-05\n",
      "Epoch: 9792 mean train loss:  2.63374028e-05, mean val. rec. loss:  2.76329473e-05\n",
      "Epoch: 9793 mean train loss:  2.61818339e-05, mean val. rec. loss:  2.71940354e-05\n",
      "Epoch: 9794 mean train loss:  2.59679673e-05, mean val. rec. loss:  2.67298801e-05\n",
      "Epoch: 9795 mean train loss:  2.58858446e-05, mean val. rec. loss:  2.72670011e-05\n",
      "Epoch: 9796 mean train loss:  2.61930548e-05, mean val. rec. loss:  2.62159536e-05\n",
      "Epoch: 9797 mean train loss:  2.61408993e-05, mean val. rec. loss:  2.69622451e-05\n",
      "Epoch: 9798 mean train loss:  2.61207636e-05, mean val. rec. loss:  2.64960636e-05\n",
      "Epoch: 9799 mean train loss:  2.61832255e-05, mean val. rec. loss:  2.81842062e-05\n",
      "Epoch: 9800 mean train loss:  2.63277991e-05, mean val. rec. loss:  2.63326779e-05\n",
      "Epoch: 9801 mean train loss:  2.67409339e-05, mean val. rec. loss:  2.76808786e-05\n",
      "Epoch: 9802 mean train loss:  2.67686527e-05, mean val. rec. loss:  2.70089589e-05\n",
      "Epoch: 9803 mean train loss:  2.62820547e-05, mean val. rec. loss:  2.75244119e-05\n",
      "Epoch: 9804 mean train loss:  2.61737739e-05, mean val. rec. loss:  2.72468346e-05\n",
      "Epoch: 9805 mean train loss:  2.61821337e-05, mean val. rec. loss:  2.82384319e-05\n",
      "Epoch: 9806 mean train loss:  2.65690962e-05, mean val. rec. loss:  2.82542053e-05\n",
      "Epoch: 9807 mean train loss:  2.66559155e-05, mean val. rec. loss:  2.77290030e-05\n",
      "Epoch: 9808 mean train loss:  2.64276627e-05, mean val. rec. loss:  2.68704235e-05\n",
      "Epoch: 9809 mean train loss:  2.60688699e-05, mean val. rec. loss:  2.74346687e-05\n",
      "Epoch: 9810 mean train loss:  2.64618628e-05, mean val. rec. loss:  2.75978001e-05\n",
      "Epoch: 9811 mean train loss:  2.62641640e-05, mean val. rec. loss:  2.84308613e-05\n",
      "Epoch: 9812 mean train loss:  2.61706928e-05, mean val. rec. loss:  2.68132449e-05\n",
      "Epoch: 9813 mean train loss:  2.65485528e-05, mean val. rec. loss:  2.85793323e-05\n",
      "Epoch: 9814 mean train loss:  2.63649754e-05, mean val. rec. loss:  2.61255131e-05\n",
      "Epoch: 9815 mean train loss:  2.59344758e-05, mean val. rec. loss:  2.70531876e-05\n",
      "Epoch: 9816 mean train loss:  2.58412427e-05, mean val. rec. loss:  2.66621207e-05\n",
      "Epoch: 9817 mean train loss:  2.60139435e-05, mean val. rec. loss:  2.63785489e-05\n",
      "Epoch: 9818 mean train loss:  2.63622556e-05, mean val. rec. loss:  2.75293683e-05\n",
      "Epoch: 9819 mean train loss:  2.60749788e-05, mean val. rec. loss:  2.72260615e-05\n",
      "Epoch: 9820 mean train loss:  2.59073297e-05, mean val. rec. loss:  2.75364895e-05\n",
      "Epoch: 9821 mean train loss:  2.58926271e-05, mean val. rec. loss:  2.69627040e-05\n",
      "Epoch: 9822 mean train loss:  2.57951492e-05, mean val. rec. loss:  2.67151675e-05\n",
      "Epoch: 9823 mean train loss:  2.62354777e-05, mean val. rec. loss:  2.72339301e-05\n",
      "Epoch: 9824 mean train loss:  2.61352929e-05, mean val. rec. loss:  2.61351353e-05\n",
      "Epoch: 9825 mean train loss:  2.57745265e-05, mean val. rec. loss:  2.70786763e-05\n",
      "Epoch: 9826 mean train loss:  2.60238809e-05, mean val. rec. loss:  2.70841598e-05\n",
      "Epoch: 9827 mean train loss:  2.58982730e-05, mean val. rec. loss:  2.76872252e-05\n",
      "Epoch: 9828 mean train loss:  2.60134218e-05, mean val. rec. loss:  2.77655245e-05\n",
      "Epoch: 9829 mean train loss:  2.59086148e-05, mean val. rec. loss:  2.71163835e-05\n",
      "Epoch: 9830 mean train loss:  2.59128110e-05, mean val. rec. loss:  2.62217142e-05\n",
      "Epoch: 9831 mean train loss:  2.57500843e-05, mean val. rec. loss:  2.66960845e-05\n",
      "Epoch: 9832 mean train loss:  2.56447736e-05, mean val. rec. loss:  2.68212542e-05\n",
      "Epoch: 9833 mean train loss:  2.57218190e-05, mean val. rec. loss:  2.80184808e-05\n",
      "Epoch: 9834 mean train loss:  2.62485038e-05, mean val. rec. loss:  2.81679603e-05\n",
      "Epoch: 9835 mean train loss:  2.65737255e-05, mean val. rec. loss:  2.68313579e-05\n",
      "Epoch: 9836 mean train loss:  2.59949168e-05, mean val. rec. loss:  2.64643714e-05\n",
      "Epoch: 9837 mean train loss:  2.62755704e-05, mean val. rec. loss:  2.79621221e-05\n",
      "Epoch: 9838 mean train loss:  2.61134139e-05, mean val. rec. loss:  2.66030772e-05\n",
      "Epoch: 9839 mean train loss:  2.55612332e-05, mean val. rec. loss:  2.64662477e-05\n",
      "Epoch: 9840 mean train loss:  2.56716387e-05, mean val. rec. loss:  2.72639982e-05\n",
      "Epoch: 9841 mean train loss:  2.59617424e-05, mean val. rec. loss:  2.59374268e-05\n",
      "Epoch: 9842 mean train loss:  2.59948778e-05, mean val. rec. loss:  2.81719718e-05\n",
      "Epoch: 9843 mean train loss:  2.63742363e-05, mean val. rec. loss:  2.80071550e-05\n",
      "Epoch: 9844 mean train loss:  2.64827277e-05, mean val. rec. loss:  2.78596289e-05\n",
      "Epoch: 9845 mean train loss:  2.61118720e-05, mean val. rec. loss:  2.67107948e-05\n",
      "Epoch: 9846 mean train loss:  2.62969324e-05, mean val. rec. loss:  2.84620447e-05\n",
      "Epoch: 9847 mean train loss:  2.60499487e-05, mean val. rec. loss:  2.62386506e-05\n",
      "Epoch: 9848 mean train loss:  2.57286262e-05, mean val. rec. loss:  2.67874631e-05\n",
      "Epoch: 9849 mean train loss:  2.56725441e-05, mean val. rec. loss:  2.66582410e-05\n",
      "Epoch: 9850 mean train loss:  2.55370114e-05, mean val. rec. loss:  2.64836157e-05\n",
      "Epoch: 9851 mean train loss:  2.55629717e-05, mean val. rec. loss:  2.65454465e-05\n",
      "Epoch: 9852 mean train loss:  2.56974820e-05, mean val. rec. loss:  2.69429758e-05\n",
      "Epoch: 9853 mean train loss:  2.57581147e-05, mean val. rec. loss:  2.90958302e-05\n",
      "Epoch: 9854 mean train loss:  2.71211368e-05, mean val. rec. loss:  2.82373098e-05\n",
      "Epoch: 9855 mean train loss:  2.62320996e-05, mean val. rec. loss:  2.67975191e-05\n",
      "Epoch: 9856 mean train loss:  2.60725713e-05, mean val. rec. loss:  2.80403782e-05\n",
      "Epoch: 9857 mean train loss:  2.64317760e-05, mean val. rec. loss:  2.71679379e-05\n",
      "Epoch: 9858 mean train loss:  2.70947128e-05, mean val. rec. loss:  2.81671403e-05\n",
      "Epoch: 9859 mean train loss:  2.67123402e-05, mean val. rec. loss:  2.73410390e-05\n",
      "Epoch: 9860 mean train loss:  2.62757478e-05, mean val. rec. loss:  2.70346588e-05\n",
      "Epoch: 9861 mean train loss:  2.58276311e-05, mean val. rec. loss:  2.74671333e-05\n",
      "Epoch: 9862 mean train loss:  2.57626230e-05, mean val. rec. loss:  2.66999415e-05\n",
      "Epoch: 9863 mean train loss:  2.56001347e-05, mean val. rec. loss:  2.72689751e-05\n",
      "Epoch: 9864 mean train loss:  2.57744219e-05, mean val. rec. loss:  2.72959766e-05\n",
      "Epoch: 9865 mean train loss:  2.56622213e-05, mean val. rec. loss:  2.67055772e-05\n",
      "Epoch: 9866 mean train loss:  2.59602856e-05, mean val. rec. loss:  2.66190891e-05\n",
      "Epoch: 9867 mean train loss:  2.57972077e-05, mean val. rec. loss:  2.69228593e-05\n",
      "Epoch: 9868 mean train loss:  2.59677035e-05, mean val. rec. loss:  2.68560266e-05\n",
      "Epoch: 9869 mean train loss:  2.60524973e-05, mean val. rec. loss:  2.76289562e-05\n",
      "Epoch: 9870 mean train loss:  2.57701076e-05, mean val. rec. loss:  2.59369385e-05\n",
      "Epoch: 9871 mean train loss:  2.56642121e-05, mean val. rec. loss:  2.70231445e-05\n",
      "Epoch: 9872 mean train loss:  2.58710768e-05, mean val. rec. loss:  2.60223385e-05\n",
      "Epoch: 9873 mean train loss:  2.55894145e-05, mean val. rec. loss:  2.83045967e-05\n",
      "Epoch: 9874 mean train loss:  2.57506031e-05, mean val. rec. loss:  2.61700281e-05\n",
      "Epoch: 9875 mean train loss:  2.56098848e-05, mean val. rec. loss:  2.66733057e-05\n",
      "Epoch: 9876 mean train loss:  2.56471315e-05, mean val. rec. loss:  2.65327441e-05\n",
      "Epoch: 9877 mean train loss:  2.57260226e-05, mean val. rec. loss:  2.75729633e-05\n",
      "Epoch: 9878 mean train loss:  2.57410421e-05, mean val. rec. loss:  2.63950583e-05\n",
      "Epoch: 9879 mean train loss:  2.57668081e-05, mean val. rec. loss:  2.67054681e-05\n",
      "Epoch: 9880 mean train loss:  2.59050368e-05, mean val. rec. loss:  2.66251291e-05\n",
      "Epoch: 9881 mean train loss:  2.57566440e-05, mean val. rec. loss:  2.73889953e-05\n",
      "Epoch: 9882 mean train loss:  2.56374687e-05, mean val. rec. loss:  2.66514991e-05\n",
      "Epoch: 9883 mean train loss:  2.57697043e-05, mean val. rec. loss:  2.70171250e-05\n",
      "Epoch: 9884 mean train loss:  2.58975842e-05, mean val. rec. loss:  2.64413678e-05\n",
      "Epoch: 9885 mean train loss:  2.55834234e-05, mean val. rec. loss:  2.70892889e-05\n",
      "Epoch: 9886 mean train loss:  2.59433529e-05, mean val. rec. loss:  2.61011556e-05\n",
      "Epoch: 9887 mean train loss:  2.59430715e-05, mean val. rec. loss:  2.70744967e-05\n",
      "Epoch: 9888 mean train loss:  2.58232933e-05, mean val. rec. loss:  2.76871275e-05\n",
      "Epoch: 9889 mean train loss:  2.58849775e-05, mean val. rec. loss:  2.62335397e-05\n",
      "Epoch: 9890 mean train loss:  2.55778688e-05, mean val. rec. loss:  2.69837859e-05\n",
      "Epoch: 9891 mean train loss:  2.56984109e-05, mean val. rec. loss:  2.71248245e-05\n",
      "Epoch: 9892 mean train loss:  2.55391865e-05, mean val. rec. loss:  2.70799393e-05\n",
      "Epoch: 9893 mean train loss:  2.58067041e-05, mean val. rec. loss:  2.66467244e-05\n",
      "Epoch: 9894 mean train loss:  2.58820187e-05, mean val. rec. loss:  2.74158446e-05\n",
      "Epoch: 9895 mean train loss:  2.55942966e-05, mean val. rec. loss:  2.60930826e-05\n",
      "Epoch: 9896 mean train loss:  2.56302470e-05, mean val. rec. loss:  2.68833530e-05\n",
      "Epoch: 9897 mean train loss:  2.56153048e-05, mean val. rec. loss:  2.74208579e-05\n",
      "Epoch: 9898 mean train loss:  2.55444896e-05, mean val. rec. loss:  2.63805637e-05\n",
      "Epoch: 9899 mean train loss:  2.56818915e-05, mean val. rec. loss:  2.61622072e-05\n",
      "Epoch: 9900 mean train loss:  2.54661858e-05, mean val. rec. loss:  2.70812659e-05\n",
      "Epoch: 9901 mean train loss:  2.57147927e-05, mean val. rec. loss:  2.62176777e-05\n",
      "Epoch: 9902 mean train loss:  2.57528119e-05, mean val. rec. loss:  2.70711849e-05\n",
      "Epoch: 9903 mean train loss:  2.57635599e-05, mean val. rec. loss:  2.69306733e-05\n",
      "Epoch: 9904 mean train loss:  2.60465036e-05, mean val. rec. loss:  2.84254415e-05\n",
      "Epoch: 9905 mean train loss:  2.57740665e-05, mean val. rec. loss:  2.68438604e-05\n",
      "Epoch: 9906 mean train loss:  2.56847936e-05, mean val. rec. loss:  2.80675092e-05\n",
      "Epoch: 9907 mean train loss:  2.65403951e-05, mean val. rec. loss:  2.68937384e-05\n",
      "Epoch: 9908 mean train loss:  2.59440774e-05, mean val. rec. loss:  2.63598589e-05\n",
      "Epoch: 9909 mean train loss:  2.53978923e-05, mean val. rec. loss:  2.68075252e-05\n",
      "Epoch: 9910 mean train loss:  2.55736238e-05, mean val. rec. loss:  2.67340620e-05\n",
      "Epoch: 9911 mean train loss:  2.54470271e-05, mean val. rec. loss:  2.59812353e-05\n",
      "Epoch: 9912 mean train loss:  2.51896412e-05, mean val. rec. loss:  2.66097986e-05\n",
      "Epoch: 9913 mean train loss:  2.52929816e-05, mean val. rec. loss:  2.60634189e-05\n",
      "Epoch: 9914 mean train loss:  2.51423711e-05, mean val. rec. loss:  2.57433551e-05\n",
      "Epoch: 9915 mean train loss:  2.55451295e-05, mean val. rec. loss:  2.67757875e-05\n",
      "Epoch: 9916 mean train loss:  2.55939705e-05, mean val. rec. loss:  2.68005153e-05\n",
      "Epoch: 9917 mean train loss:  2.54341710e-05, mean val. rec. loss:  2.63745419e-05\n",
      "Epoch: 9918 mean train loss:  2.52069693e-05, mean val. rec. loss:  2.62639258e-05\n",
      "Epoch: 9919 mean train loss:  2.54590504e-05, mean val. rec. loss:  2.67498422e-05\n",
      "Epoch: 9920 mean train loss:  2.57864158e-05, mean val. rec. loss:  2.76643374e-05\n",
      "Epoch: 9921 mean train loss:  2.59394633e-05, mean val. rec. loss:  2.69128827e-05\n",
      "Epoch: 9922 mean train loss:  2.57207601e-05, mean val. rec. loss:  2.58847435e-05\n",
      "Epoch: 9923 mean train loss:  2.56653309e-05, mean val. rec. loss:  2.60642003e-05\n",
      "Epoch: 9924 mean train loss:  2.54396664e-05, mean val. rec. loss:  2.69461605e-05\n",
      "Epoch: 9925 mean train loss:  2.52367787e-05, mean val. rec. loss:  2.60205803e-05\n",
      "Epoch: 9926 mean train loss:  2.55496809e-05, mean val. rec. loss:  2.72298050e-05\n",
      "Epoch: 9927 mean train loss:  2.52511775e-05, mean val. rec. loss:  2.60891302e-05\n",
      "Epoch: 9928 mean train loss:  2.58222284e-05, mean val. rec. loss:  2.63722704e-05\n",
      "Epoch: 9929 mean train loss:  2.56593238e-05, mean val. rec. loss:  2.63003814e-05\n",
      "Epoch: 9930 mean train loss:  2.56013811e-05, mean val. rec. loss:  2.65323262e-05\n",
      "Epoch: 9931 mean train loss:  2.51586761e-05, mean val. rec. loss:  2.62899779e-05\n",
      "Epoch: 9932 mean train loss:  2.53698863e-05, mean val. rec. loss:  2.65051724e-05\n",
      "Epoch: 9933 mean train loss:  2.54928222e-05, mean val. rec. loss:  2.63602723e-05\n",
      "Epoch: 9934 mean train loss:  2.54579428e-05, mean val. rec. loss:  2.75754892e-05\n",
      "Epoch: 9935 mean train loss:  2.56543660e-05, mean val. rec. loss:  2.59172058e-05\n",
      "Epoch: 9936 mean train loss:  2.51536806e-05, mean val. rec. loss:  2.68304448e-05\n",
      "Epoch: 9937 mean train loss:  2.58714873e-05, mean val. rec. loss:  2.72032828e-05\n",
      "Epoch: 9938 mean train loss:  2.56319068e-05, mean val. rec. loss:  2.66526712e-05\n",
      "Epoch: 9939 mean train loss:  2.53921484e-05, mean val. rec. loss:  2.65164324e-05\n",
      "Epoch: 9940 mean train loss:  2.52758033e-05, mean val. rec. loss:  2.68339248e-05\n",
      "Epoch: 9941 mean train loss:  2.54833242e-05, mean val. rec. loss:  2.58726181e-05\n",
      "Epoch: 9942 mean train loss:  2.53067878e-05, mean val. rec. loss:  2.70552911e-05\n",
      "Epoch: 9943 mean train loss:  2.53830189e-05, mean val. rec. loss:  2.56766542e-05\n",
      "Epoch: 9944 mean train loss:  2.54184810e-05, mean val. rec. loss:  2.72583216e-05\n",
      "Epoch: 9945 mean train loss:  2.54419163e-05, mean val. rec. loss:  2.58323487e-05\n",
      "Epoch: 9946 mean train loss:  2.50991862e-05, mean val. rec. loss:  2.60083754e-05\n",
      "Epoch: 9947 mean train loss:  2.49658702e-05, mean val. rec. loss:  2.60756306e-05\n",
      "Epoch: 9948 mean train loss:  2.53011126e-05, mean val. rec. loss:  2.63146988e-05\n",
      "Epoch: 9949 mean train loss:  2.54161385e-05, mean val. rec. loss:  2.66170720e-05\n",
      "Epoch: 9950 mean train loss:  2.53195491e-05, mean val. rec. loss:  2.63800299e-05\n",
      "Epoch: 9951 mean train loss:  2.54597435e-05, mean val. rec. loss:  2.71946601e-05\n",
      "Epoch: 9952 mean train loss:  2.56095681e-05, mean val. rec. loss:  2.66466653e-05\n",
      "Epoch: 9953 mean train loss:  2.57056250e-05, mean val. rec. loss:  2.64664794e-05\n",
      "Epoch: 9954 mean train loss:  2.52996880e-05, mean val. rec. loss:  2.65677096e-05\n",
      "Epoch: 9955 mean train loss:  2.55297064e-05, mean val. rec. loss:  2.67137841e-05\n",
      "Epoch: 9956 mean train loss:  2.54688952e-05, mean val. rec. loss:  2.72907408e-05\n",
      "Epoch: 9957 mean train loss:  2.51911041e-05, mean val. rec. loss:  2.59785345e-05\n",
      "Epoch: 9958 mean train loss:  2.53376345e-05, mean val. rec. loss:  2.62848261e-05\n",
      "Epoch: 9959 mean train loss:  2.53235026e-05, mean val. rec. loss:  2.58963850e-05\n",
      "Epoch: 9960 mean train loss:  2.54357218e-05, mean val. rec. loss:  2.59900806e-05\n",
      "Epoch: 9961 mean train loss:  2.54755145e-05, mean val. rec. loss:  2.84089957e-05\n",
      "Epoch: 9962 mean train loss:  2.61452987e-05, mean val. rec. loss:  2.76705273e-05\n",
      "Epoch: 9963 mean train loss:  2.57039871e-05, mean val. rec. loss:  2.71219351e-05\n",
      "Epoch: 9964 mean train loss:  2.55149951e-05, mean val. rec. loss:  2.60876128e-05\n",
      "Epoch: 9965 mean train loss:  2.52401764e-05, mean val. rec. loss:  2.60704220e-05\n",
      "Epoch: 9966 mean train loss:  2.52470269e-05, mean val. rec. loss:  2.71443050e-05\n",
      "Epoch: 9967 mean train loss:  2.53984743e-05, mean val. rec. loss:  2.69691210e-05\n",
      "Epoch: 9968 mean train loss:  2.56109151e-05, mean val. rec. loss:  2.71352212e-05\n",
      "Epoch: 9969 mean train loss:  2.52304185e-05, mean val. rec. loss:  2.69350641e-05\n",
      "Epoch: 9970 mean train loss:  2.54842717e-05, mean val. rec. loss:  2.64777098e-05\n",
      "Epoch: 9971 mean train loss:  2.53264984e-05, mean val. rec. loss:  2.57116220e-05\n",
      "Epoch: 9972 mean train loss:  2.47598352e-05, mean val. rec. loss:  2.54562579e-05\n",
      "Epoch: 9973 mean train loss:  2.50373921e-05, mean val. rec. loss:  2.62483318e-05\n",
      "Epoch: 9974 mean train loss:  2.49653049e-05, mean val. rec. loss:  2.57419786e-05\n",
      "Epoch: 9975 mean train loss:  2.47190310e-05, mean val. rec. loss:  2.59689487e-05\n",
      "Epoch: 9976 mean train loss:  2.49567959e-05, mean val. rec. loss:  2.60740519e-05\n",
      "Epoch: 9977 mean train loss:  2.48146010e-05, mean val. rec. loss:  2.57446385e-05\n",
      "Epoch: 9978 mean train loss:  2.48712380e-05, mean val. rec. loss:  2.62567455e-05\n",
      "Epoch: 9979 mean train loss:  2.50733251e-05, mean val. rec. loss:  2.63391517e-05\n",
      "Epoch: 9980 mean train loss:  2.55452341e-05, mean val. rec. loss:  2.65383593e-05\n",
      "Epoch: 9981 mean train loss:  2.52082710e-05, mean val. rec. loss:  2.59527255e-05\n",
      "Epoch: 9982 mean train loss:  2.55195468e-05, mean val. rec. loss:  2.72298005e-05\n",
      "Epoch: 9983 mean train loss:  2.52305673e-05, mean val. rec. loss:  2.56461318e-05\n",
      "Epoch: 9984 mean train loss:  2.52935166e-05, mean val. rec. loss:  2.77112624e-05\n",
      "Epoch: 9985 mean train loss:  2.53403427e-05, mean val. rec. loss:  2.52760992e-05\n",
      "Epoch: 9986 mean train loss:  2.50751667e-05, mean val. rec. loss:  2.62587786e-05\n",
      "Epoch: 9987 mean train loss:  2.48453984e-05, mean val. rec. loss:  2.52313503e-05\n",
      "Epoch: 9988 mean train loss:  2.49515194e-05, mean val. rec. loss:  2.60937346e-05\n",
      "Epoch: 9989 mean train loss:  2.49918333e-05, mean val. rec. loss:  2.55769709e-05\n",
      "Epoch: 9990 mean train loss:  2.48843200e-05, mean val. rec. loss:  2.56243003e-05\n",
      "Epoch: 9991 mean train loss:  2.48108598e-05, mean val. rec. loss:  2.57601553e-05\n",
      "Epoch: 9992 mean train loss:  2.48106057e-05, mean val. rec. loss:  2.56392582e-05\n",
      "Epoch: 9993 mean train loss:  2.47461933e-05, mean val. rec. loss:  2.63504729e-05\n",
      "Epoch: 9994 mean train loss:  2.47071112e-05, mean val. rec. loss:  2.54357415e-05\n",
      "Epoch: 9995 mean train loss:  2.49227900e-05, mean val. rec. loss:  2.59881930e-05\n",
      "Epoch: 9996 mean train loss:  2.45933616e-05, mean val. rec. loss:  2.54726037e-05\n",
      "Epoch: 9997 mean train loss:  2.48121759e-05, mean val. rec. loss:  2.56984472e-05\n",
      "Epoch: 9998 mean train loss:  2.51322139e-05, mean val. rec. loss:  2.62667539e-05\n",
      "Epoch: 9999 mean train loss:  2.54323503e-05, mean val. rec. loss:  2.57107361e-05\n"
     ]
    }
   ],
   "source": [
    "losses_train, losses_val = train(net_H2, loaders, args, A, H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train)\n",
    "plt.plot(losses_val)\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1 = net_H1(grid_data)\n",
    "U2 = net_H2(grid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1d48a7fbf50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(13,5))\n",
    "pos1 = axs[0].pcolormesh(XY[0], XY[1], U1.reshape(N, N).detach().cpu(), cmap='bwr')\n",
    "fig.colorbar(pos1, ax=axs[0])\n",
    "pos2 = axs[1].pcolormesh(XY[0], XY[1], U2.reshape(N, N).detach().cpu(), cmap='bwr')\n",
    "fig.colorbar(pos2, ax=axs[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
